**Speaker 1**: No way, David. All right. Who do we have on a call? Kamal. Lior, is that... Am I saying your name correctly, Lior? Yes, you're saying it correctly. It's Lior. By the way, I was born in Hanukkah. So I have my birthday this Thursday. And Lior is my light. It's my mom. It's, you know, she was a very... Hanukkah, you're lighting lights every day. Candles every day. So that's why many Lior's were born in Hanukkah.
*00:01*

**Jason Vallery**: No way, Stephen. All right, who we have on call? Kamal. Lior, is that, am I saying your name correctly, Lior? Yes, you're saying it correctly. It's Lior. I was born in Hanukkah, so I have my birthday this Thursday, and Lior is my light. It's my mom, it's, you know, she was a very, Hanukkah, you're lighting lights every day, candles every day. So that's why many Lior's were born in Hanukkah.
*00:01*

**Jason Vallery**: So now you know something, right? About the name. Yeah. Yeah. No, thank you for sharing. Hey, Chris, my name is Akarjan Sehzal. Jason Valerie, I don't think we've met before. Hey, David. Yep, Jason Valerie. I'm VP of Cloud Product Management for VAST. Got it. I'm just taking a note. Okay. And Jeff's on the call. Okay. From our side, David Pollack.
*00:30*

**Lior Genzel**: So now you know something, right? About the name. Yeah. Yeah. No, thank you for sharing. Hey, here's my name, Alakarjan. Jason, Valerie, I don't think we've met before. Got it. I'm just taking a note. Okay. And Jeff's on the call. Okay. From our side, David Pollack,
*00:30*

**Lior Genzel**: The Partnerships Lead, I also have Seiza, Gersman and Gopal is on the call but they're part of the procurement team that helped administer a lot of this bid and they'll be overseeing this through the finish. We also have Kamal and Malikarjan both with product management and Jeff part of the storage eng team. So that's who we have on a call. What I want to do is now pull up your proposal and walk through it at a high level.
*01:00*

**Jason Vallery**: The Partnerships Lead, I also have Seiza, Gersman, and Gopal is on the call, but they're part of the procurement team that helped administer a lot of this bid, and they'll be overseeing this through the finish. We also have Kamal and Malikarjan, both with product management, and Jeff, part of the storage ench team. So that's who we have on the call. What I want to do is now pull up your proposal and walk through it at a high level.
*01:00*

**Jason Vallery**: So bear with me while I organize that and share my screen. I'm doing that right now. Okay, there it is. And it looks like people were making copies of documents. Give me just one second so I open up the right one. All right, there it is.
*01:29*

**Speaker 3**: So bear with me while I organize that and share my screen. I'm doing that right now. Okay, there it is. And it looks like people were making copies of documents. Give me just one second. So I open up the right one. All right, there it is.
*01:30*

**Lior Genzel**: Okay, it's loading. I'm going to share my screen now. Let me know when you can see it. Not yet. We can see it. Okay, very good. So I want to start on the technical requirements tab. And for everything that's on here, were any of these answers no from your side?
*02:01*

**Jason Vallery**: Okay, it's floating. I'm going to share my screen now. Let me know when you can see it. Not yet. We can see it. Okay, very good. So I want to start on the technical requirements tab. And for everything that's on here, were any of these answers no from your side?
*02:01*

**Jason Vallery**: I actually don't remember if we marked anything as no. I can check on my side as well.
*02:29*

**Speaker 1**: I actually don't remember if we marked anything as no. I can check on my side as well. I'm looking through it right now. There's B8 is marked as no, B16. We'll talk about these in just one second. C2, C3, C8, C18, D7. Okay, so there is a few of them. Yeah. Jeff,
*02:30*

**Jason Vallery**: I'm looking through it right now. There's B8 is marked as no, B16. We'll talk about these in just one second. C2, C3, C8, C18, D7. Okay, so there is a few of them.
*02:35*

**Jason Vallery**: Jeff, Kamal, and Malik Karjan, I'm not sure if you had a chance to look through those, but if you want, I can just start at the top and work our way down. How does that sound to you? I can't see, so if you want to speak up, go ahead, please. All good. I'm just saying yes. Let's go. Okay, cool.
*03:00*

**Lior Genzel**: I'm not sure if you had a chance to look through those, but if you want, I can just start at the top and work our way down. How does that sound to you? I can't see, so if you want to speak up, go ahead, please. All good. I'm just saying yes. Let's go. Okay, cool.
*03:03*

**Lior Genzel**: So Lior, Jason, can you walk us through the highlighted row B8 and what was the showstopper here, if you will?
*03:29*

**Jason Vallery**: So Lior, Jason, can you walk us through the highlighted row B8 and what was the showstopper here, if you will? Sure. So, you know, we do data reduction and encryption and erasure coding such that a given block of data will be distributed across the cluster and it's not possible then to isolate it to a specific share or volume the way you might traditionally think of that.
*03:29*

**Jason Vallery**: in order to achieve those benefits. So we have multi-tenancy, we have key management to ensure separation of data concerns, but ultimately the way the data gets persisted on to the media is managed at an abstracted layer because of those capabilities. Okay, and then Jeff, for you, is that acceptable or what are your thoughts?
*03:58*

**Lior Genzel**: Okay. And then, Jeff, for you, is that acceptable or what are your thoughts?
*04:15*

**Jason Vallery**: I can't see from my side so I'm I'm on I that's as expected based on what we know about your a choppy Jeff yeah see your architecture I mean it's something we'll take the quality solution we'll have to address it
*04:36*

**Speaker 4**: I can't see from my side so that's I'm I'm on that's as expected based on what we know about you're a choppy Jeff the architecture I mean it's something we'll take the call isolation we'll have to address it sorry you're you're coming
*04:36*

**Jason Vallery**: Sorry, you're coming through very choppy, Jeff. Maybe it's just for me or for others. Okay. For me too. Okay. Let me put on a headset. I'll be back with you. Okay. Yeah. So while Jeff is doing that, Jason, quick question for you. So I guess the answer is the same, even if you're talking about not an individual block granularity, but the volume granularity.
*04:58*

**Speaker 5**: or it happens for others?
*05:00*

**Speaker 5**: Okay. Let me put on a headset. I'll be back with you. Okay. Yeah. So while Jeff is doing that, Jason, quick question for you. So I guess the answer is the same, even if you're talking about not at an individual block granularity, but the volume granularity, the answer is the same, or does that change if you're thinking of a screen?
*05:05*

**Jason Vallery**: Is that the answer the same or does that change? If you're thinking what's green? You should think of the underlying capacity being a physical pool that will stripe across and then we can logically have isolation and separation above it but the capacity is managed independently and all of the data is sharded across all of the capacity. Okay, so even if it's coarse green volume level granularity, I think the answer is basically the same. Okay.
*05:27*

**Speaker 5**: Okay.
*05:34*

**Speaker 5**: Okay, so even if it's a coarse-grained volume-level granularity, I think the answer is basically the same, okay?
*05:47*

**Jason Vallery**: I don't know if there was an underlying customer scenario you were trying to solve for here.
*05:57*

**Jason Vallery**: Obviously, one solution to this problem is if you carve up the cluster such that they're tenanted by customer.
*06:01*

**Jason Vallery**: If that was your end state or goal behind that, you could kind of achieve this, but it would be a suboptimal way to get there.
*06:07*

**Jason Vallery**: So I don't know if there was an underlying scenario behind this you were trying to achieve.
*06:13*

**Jason Vallery**: My experience is this is kind of how most distributed storage platforms operate.
*06:17*

**Speaker 5**: Well, the scenario is one of regulatory compliance adjacent. So basically there were some scenarios for customers where they wanted all the
*06:20*

**Jason Vallery**: Well, the scenario is one of regulatory compliance, Jason. So basically there were some scenarios for customers where they wanted all the media associated with a particular data set residing in a particular volume to be confiscated by regulatory agencies, right? That implicitly requires that we be able to map the data set to a set of disks or something in the back end.
*06:21*

**Speaker 5**: associated with a particular data set residing in a particular volume to be confiscated by regulatory agencies. Right? That implicitly requires that we be able to map the data set to a set of disks or something in the back end. Like you say, the sort of the default fallback option would be to scope the data set to the entire cluster and keep them disjoint clusters. But the problem of the optimality that you pointed out
*06:35*

**Jason Vallery**: Like you say, the sort of the default fallback option would be to scope the data set to the entire cluster and keep them disjoint clusters. But the problem of the optimality that you pointed out comes in. Yeah, that's exactly it. Okay. All right. Let's keep going. Yeah, in the interest of time. Jeff, sorry, I didn't know if I saw your headset on. Do you want to make a comment or anything on that? Really good. Okay. He's nodding.
*06:51*

**Speaker 5**: Okay. All right. Let's keep going. Yeah. In the interest of time. Yeah. Jeff, sorry. I didn't know if I saw your headset on or you want to make a comment, anything on that? Really good. Okay. He's nodding. Okay. Let's keep going. B16. Yeah. Jason, do you want to comment on B16?
*07:08*

**Jason Vallery**: Okay, let's keep going. B16. Yeah, Jason, do you want to comment on B16? Yeah, so we're prioritizing performance, and when you start thinking about trying to do sync replication or zero RTO, zero RPO across zones, you incur right penalties on latency, obviously. We have the ability to deliver very low RTO, RPO, you know, 10-second kind of numbers today across multiple zones.
*07:21*

**Jason Vallery**: to get down to zero, you're going to impact a performance penalty. It's something we've got on our roadmap and we have considered implementing, but it has not become a priority for us. Most of our customers are more looking at, they're not looking at those kinds of topologies. So I guess, you know, how important is this? What's the scenario behind it? What are you solving for? Is it more of an availability or a durability consideration behind this requirement?
*07:52*

**Speaker 4**: Yeah, so there are specific workload use cases that wanted to have synchronous replication, zero PO for files. I thought that the earlier
*08:20*

**Jason Vallery**: Yeah, so there are specific workload use cases that wanted to have synchronous replication, zero PO for files. I thought that the earlier discussion that we've had with VAST about the architecture, I thought that with the replication implemented primarily at the element level, that this was something that was either already present or soon to be
*08:20*

**Speaker 4**: I thought that with the replication implemented primarily at the element level, that this was something that was either already present or soon to be delivered. But I'll accept that that's where you're...
*08:39*

**Jason Vallery**: We have snapshot based today. So we have sync replication for cluster to cluster sync block level replication. Files to do like a three zone where you can have some kind of policy driven replication over top of it in a synchronous way is not present. So like you could mirror two clusters together effectively. But having the files being a
*08:55*

**Speaker 3**: Okay.
*09:13*

**Jason Vallery**: just replication policy in a synch way is not configurable.
*09:20*

**Jason Vallery**: Okay, well, the requirement would be, you know, an entire portion of a namespace would need to be replicated, including all metadata changes and directory changes, etc.
*09:26*

**Speaker 4**: Okay, well
*09:27*

**Speaker 4**: changes and directory changes, etc. Yeah, that's something that we'll have to consider and perhaps explore what the implications are if we have to get this functionality from you.
*09:36*

**Jason Vallery**: Yeah, that's something that we'll have to consider and perhaps explore what the implications are if we have to get this functionality from you. I think ultimately we're committed to working with you on this one, getting a little bit deeper into the specific scenario we're solving for and then kind of working with you on what tools and capabilities we have today to solve the problem and if there are roadmap items behind it we need to put in place, we're committed to making that happen as well.
*09:40*

**Lior Genzel**: Okay. Sounds good. Thank you. Let's keep it moving. This has been the theme. Jeff or Malakarjan, do you want to address this one so it's super clear to the VAS team? B24? Yeah, I don't think there's really an issue here. I suppose one of the questions about
*10:09*

**Jason Vallery**: Okay. Sounds good. Thank you. Let's keep it moving. This has been the theme. Jeff from Malakarjan, do you want to address this one so it's super clear to the VAS team, B24? Yeah, I don't think there's really an issue here. I suppose one of the questions about scaling to this large A performance
*10:09*

**Speaker 4**: scaling to this large performance level is at what size pools or how much can you get of this within a single pool and therefore how many pools of your storage would be required in order to reach these aggregate levels. But to be clear, the statement of aggregate level requirements are per zone here, not per pool.
*10:36*

**Jason Vallery**: performance level is at what size pools or how much can you get of this within a single pool and therefore how many pools of your storage would be required in order to reach these aggregate levels. But to be clear, the statement of aggregate level requirements are per zone here, not per pool, in your case per cluster.
*10:39*

**Speaker 4**: in your case per cluster.
*11:06*

**Jason Vallery**: Yeah, so I mean, we have single cluster deployments that push exabyte scale with hundreds of terabytes per second. So we're, you know, we're frontier model builders that are powering, you know, 100,000 plus GPUs running on a single cluster for training kind of scenarios are in production for us today. These are not concerning numbers at all in a single cluster. Now, we obviously have to size it from a hardware perspective.
*11:08*

**Jason Vallery**: Okay.
*11:33*

**Jason Vallery**: And we can size, if you're familiar with the C node, D node architecture, we size for IOPS and throughput by adding more C nodes. We size for capacity and bytes by adding more D nodes. We would do the math based on upper bounds of what you want a single cluster to deliver to determine how many C nodes are needed to hit that volume. Yep. Thank you. Okay, Jason, C2 and C3.
*11:36*

**Lior Genzel**: Yep, thank you.
*11:54*

**Lior Genzel**: Okay, Jason, C2 and C3.
*11:58*

**Jason Vallery**: Yeah, I mean, so, you know, one of the interesting things about our platform is the unique data reduction capabilities we have, you know, real world, we do differencing based D-dupe beyond just bite wise D-dupe. You know, of course, the type of data you're storing and the compression ratios and what kind of D-dupes in there and versioning and all of those things come into there to determine what the effective D-dupe ratio is. But our experience is many customers get two to one kind of reduction. When you layer that in with what we're seeing,
*12:05*

**Jason Vallery**: in terms of the trend line behind the flash vendors and the increase in density. As it calls out here, we're already in production 122 terabyte QLC and we have road mapped from our vendors into the 240 terabyte kind of QLC size and then even further out they get into crazy petabyte kind of QLC drives, which is nuts. Like our priority hasn't really been to invest in hard drives as a result because we see that trend line eventually being irrelevant and then, you know, delivering performance and consistency of experience on QLC is a better outcome for us and that's where we can
*12:35*

**Speaker 4**: Okay. Yeah, well, I mean, we'll just have to look at the solutions in terms of what it implies for the total cost of ownership. Correct. So the simple answer is we don't vote HDD. That's why it's now. Oh, sure. No, I know you're not going to HDD. I'm not expecting that. Yes.
*13:08*

**Jason Vallery**: Okay, yeah, well, I mean, we'll just have to look at the solutions in terms of what it implies for total cost of ownership. Correct. So the simple answer is we don't vote HDD. That's why it's not. Oh, sure. Yeah. No, I know you're not going to HDD. I'm not expecting that. Yes. The question is...
*13:09*

**Speaker 4**: One note that you should know is that the data is likely to be encrypted before it ever hits your storage. So it hurts every kind of data reduction. Yes. Yeah, I was going to ask the QLC question. What is your largest FIPS certified QLC?
*13:32*

**Jason Vallery**: know is that the data is likely to be encrypted before it ever hits your storage.
*13:36*

**Jason Vallery**: Yeah, that hurts DDo.
*13:42*

**Jason Vallery**: It hurts reduction.
*13:44*

**Jason Vallery**: It hurts every kind of data reduction, yes.
*13:46*

**Jason Vallery**: Kamal?
*13:50*

**Jason Vallery**: Yeah, I was going to ask the QLC question.
*13:50*

**Jason Vallery**: What is your largest FIPS certified QLC?
*13:54*

**Jason Vallery**: I don't know, top of mind.
*13:59*

**Jason Vallery**: I'd have to talk to our hard routine.
*14:00*

**Jason Vallery**: Okay, that's the other challenge we run into if we go into the bigger numbers.
*14:03*

**Speaker 7**: That's the other challenge we run into if we go into the bigger numbers. A lot of those are... It'll be good to know. Maybe we should get that information in terms of what are the FIPS compliant options we have for different hard drive shapes and sizes you are proposing. Same thing with 60 terabytes or 15 terabytes. Those are what customers use. Kamal, the model
*14:04*

**Jason Vallery**: A lot of those are...
*14:09*

**Jason Vallery**: It'll be good to know.
*14:12*

**Jason Vallery**: Maybe we should get that information in terms of
*14:13*

**Jason Vallery**: what are the FIPS compliant options we have
*14:16*

**Jason Vallery**: for different hard drive shapes and sizes you are proposing.
*14:19*

**Jason Vallery**: Same thing with 60 terabytes or 15 terabytes.
*14:22*

**Jason Vallery**: Those are the best of our seats.
*14:26*

**Jason Vallery**: The model that they describe is one where, and I don't know how, I know that the proposal actually includes hardware, but of course your business model allows for the hardware to come from other sources. Ultimately, it's important to stress we're a software solution. We work with a variety of ODM partners to deliver hardware to our customers, but we're also comfortable with you bringing your own suppliers and your own hardware. We'll go through validation.
*14:29*

**Speaker 4**: that they describe as one where the and I don't know how I know that the proposal actually includes hardware but of course your business model allows for the hardware to come from other sources so.
*14:32*

**Jason Vallery**: process, but otherwise we're providing software.
*14:59*

**Speaker 4**: Yeah, so I am curious though whether the pricing proposal that was included here assumed anything about self-encrypting drives and or whether your software supports self-encrypting drives. Yeah, I was going to get to that. So bear with me a little bit, Jeff, and we'll touch on that topic. Okay. Yeah, no, I was just going on a tangent off of Camel's questions.
*15:02*

**Jason Vallery**: Yeah, so I am curious though whether the pricing proposal that was included here assumed anything about self-encrypting drives and/or whether your software supports self-encrypting drives.
*15:03*

**Jason Vallery**: Yeah, I was going to get to that, so bear with me a little bit, Jeff, and we'll touch on that topic again.
*15:21*

**Jason Vallery**: Okay, yeah, no, I was just going on a tangent on Kamal's questions. Yeah, the tail numbers, what we're thinking in the D-loop would be different if we have a limited set of drives we can go with. That was probably the tangent, yeah. Yeah, Kamal, I took note of your question, and I'll bring it up here in a little bit. Let's just zip through some of these more technical focus ones. C8. Jason, can you speak to this one? Yeah, I mean, so we have a variety of ways we can drive.
*15:26*

**Speaker 7**: Yeah, scale numbers, what we're thinking in the DDoP would be different if we have a limited set of drives we can go with. That was probably the tangent, yeah. Yeah, Kamal, I took note of your question, and I'll bring it up here in a little bit. Let's just zip through some of these more technical-focused ones. C8. Jason, can you speak to this one?
*15:30*

**Jason Vallery**: for data replication and consistency across multiple vast clusters. This almost aligns back to the early sync conversation where we can work together with you to architect what the right solution is here. I think our global data namespace is ultimately the way most of our customers end up trying to achieve this, where they can have a primary and then where a satellite mirror kind of callback. The architecture ends up being you have a hub and spoke kind of model for this.
*15:56*

**Speaker 7**: Yeah.
*16:00*

**Jason Vallery**: where one side is primary and then you can have cache secondaries in satellite mode. We also do have a full async replication engine for block level replication. Multi-region consistency though I think is where you're saying you're active, active, active across multiple regions and that's not a mode we support so there's always one authoritative side and then consumers of that or write backs that come across the WAN. We do have in our roadmap a global
*16:26*

**Jason Vallery**: Right lease capability that will ship kind of late in this coming year, maybe late summer, which kind of also solves this problem space. So I think again, this is the underlying customer challenge you're trying to solve and if we have the right set of tools to architect for the business problem versus strict eventual multi-region consistency. Yeah. So the hub and spoke model is the async replication. Is that right? So the hub and spoke model is kind of a
*16:56*

**Speaker 5**: So the hub and spoke model is the async replication, is that right?
*17:17*

**Jason Vallery**: There's a central one site that we define as master for a given volume or share. And then we can connect other vast clusters back to that one such that if reads come through that, it will cache the reads within the remote site and then writes go back to the primary. So that is one way to accelerate performance across multiple sites. Those writes are async replicated to the secondaries.
*17:26*

**Lior Genzel**: And there it is.
*17:49*

**Lior Genzel**: Those writes are async replicated to the secondaries.
*17:51*

**Jason Vallery**: So today, they're actually strongly consistent, where the writes aren't apt to the satellite until the central site has it. So we're guaranteeing global consistency that way. What we're bringing is a global write lease capability where a given satellite can take a lease on a central share for some object or file or prefix, and then writes are local and act back to the client once they're done locally, and then async moved back and then they expire based on
*17:56*

**Jason Vallery**: a lease policy that would then allow for data loss based on your preference and that'd be a tunable consistency sort of model. That's what's coming in sort of summertime frame. Today they're actually strongly consistent back to the primary. So is the granularity of this ownership and these leases a sounds like it's a sub-bucket prefix kind of granularity? Well it works on both file and object so we're you know we're kind of supporting file shares as well for this model.
*18:26*

**Speaker 4**: So is the granularity of this ownership and of these leases a, it sounds like it's a sub-bucket prefix kind of granularity? Okay.
*18:39*

**Jason Vallery**: Actually, that's probably the more common use cases are customers that are exposing NFS points across the globe into multiple different cloud regions, for example. Sure. Yeah. What I was wondering, how is three buckets map onto the placement in terms of where the primaries are? Is it to, if you were doing, if you were looking at an object store? You can set up an object endpoint in each one of the satellites, if you will, and access
*18:55*

**Speaker 4**: Sure. But I was wondering how is three buckets map onto the placement in terms of where the primaries are? Is it if you were doing, if you were looking at an object store?
*19:05*

**Jason Vallery**: that endpoint if you like and then for behind the scenes the leases work the same way
*19:25*

**Lior Genzel**: Okay.
*19:29*

**Jason Vallery**: just to say that slightly differently it's transparent to the client the client isn't aware
*19:31*

**Lior Genzel**: On the point. Sorry.
*19:31*

**Jason Vallery**: that it's being managed in this way the internal system is managing read leases and write leases
*19:35*

**Jason Vallery**: to hide that complexity sure my question is really aimed at in the case of availability loss what the
*19:40*

**Speaker 4**: Sure. My question is really aimed at, in the case of availability loss, what the consistency
*19:43*

**Jason Vallery**: What the consistency expectation should be. And along those lines, I was also going to ask, with this global namespace technology, do you automatically fail over to an asynchronous copy of data in this global namespace if you lose a zone? An availability zone?
*19:52*

**Speaker 4**: the expectation should be. And along those lines, I was also going to ask, with this global namespace technology, do you automatically fail over to an asynchronous copy of data in this global namespace if you lose a zone, an availability zone?
*19:53*

**Jason Vallery**: that is also in our roadmap. We have not delivered that yet. So we do envision having a global sort of traffic load balancer that would allow failover. But today, the client has to fail the traffic to another region if that local satellite goes offline. Okay. Thank you. Okay. I'll keep moving on. And I would say there are, like, for the S3 endpoint for HTTP, there are, like, open source solutions that can handle that.
*20:23*

**Jason Vallery**: Okay, thank you.
*20:37*

**Lior Genzel**: Okay, I'll keep moving on.
*20:42*

**Jason Vallery**: that proxy over top of that from a low-balancing and DNS perspective. So it's not like we can't architect a solution for S3. NFS is where that becomes a bit trickier because it's all IP-based. Yeah. Jason, I'm looking at C18. Yeah, so I mean, there's a number of different S3 capabilities around much more granular encryption.
*20:52*

**Lior Genzel**: Yeah. Jason, I'm looking at C18.
*21:03*

**Jason Vallery**: that we haven't implemented into the API surface today. Again, these would be things, if they're critical blockers for you, we can consider for the roadmap. For example, one of them would be you can pass the key as part of the put, and then we don't store the key, and then you pass the key as part of the get. Those kinds of semantics are possible in the S3 API. If you have specifics around those, we can consider them.
*21:22*

**Lior Genzel**: Jeff, Molly Karajan, any comments?
*21:50*

**Speaker 5**: Yeah, no, no, I think, Leah, I think just in the leading to what I was thinking, there's a client library where it put calls where you can actually specify the client, the encryption key in line as part of the API call itself. And Kamal, I don't know if you remember which specific variation of encryption is being used for some of our canonical customers today. Yeah, I think depending on the
*21:54*

**Jason Vallery**: No, I think, yeah, I think just in the leading to what I was thinking, there's a client library where it put calls where you can actually specify the client, the encryption key in line as part of the API call itself. And Kamal, I don't know if you remember which specific variation of encryption is being used for some of our canonical customers today. Yeah, depending on the
*21:55*

**Speaker 7**: customer levels, information classification level, they have tendency to use both bucket level and object level encryption, right? Like I want to have in the same bucket, ability to encrypt certain group of objects with a dedicated key. Or I could have the default encryption, which applies to the bucket itself. So the question is, does the product natively provide that capability where customers can choose
*22:21*

**Jason Vallery**: customer levels, information classification level, they have tendency to use both bucket level and object level encryption. Like I want to have in the same bucket, ability to encrypt certain group of objects with a dedicated key. Or I could have the default encryption which applies to the bucket itself. So the question is, does the product natively provide that capability where customers can choose
*22:22*

**Speaker 7**: what keys to be used for either at the bucket level or object level encryption.
*22:52*

**Jason Vallery**: What keys to be used for either at the bucket level or object level encryption? So we set the key at the tenant level. So it depends on how you configure granularities of buckets and tenants and so forth. And then, you know, we don't have the granular scoped encryption capabilities today because we are multi-protocol. And so if you started implementing that for S3, then it wouldn't work with NFS. And then that sort of breaks the whole story. So we haven't implemented anything more granular than that.
*22:52*

**Jason Vallery**: So when you say keys are at the tenant level, it's not the node of the hardware, it's logical tenants which you create on top of your hardware. Yeah. You could implement it for all the buckets which go in the tenant would have the same key. Yeah, and you can figure that out to your KMS, whatever that is, and then it's storing the keys and then they're aligned there for what we do for encryption at rest. So today you're saying you are getting from the KMS, you're managing a single DEK
*23:20*

**Speaker 7**: So when you say keys are at the tenant level, it's not the node of the hardware, it's logical tenants which you create on top of your hardware. And you could implement it for all the buckets which go in the tenant would have the same key. So today you're saying you are getting from the KMS, you're managing a single DEK for the
*23:20*

**Jason Vallery**: for everything under a single tenant, including block and file data as well. There's no par per volume. I believe that's right. I will double check on that. That may be something that is there that I'm not aware of, but I'm pretty sure that's how that's done. Okay. Yeah, the granularity of the encryption domains is of interest to us, for sure. At least it's granular at the tenant, but I'm not sure if we have anything more granular for bucket level at that side.
*23:50*

**Speaker 4**: for everything under a single tenant, including block and file data as well. There's no par per volume. Okay. Yeah, the granularity of the encryption domains is of interest to us, for sure.
*23:50*

**Speaker 7**: Okay. Like the use case would be customer managed encryption key. Like they want to share the volume. Now, do I have to do at the tenant level or can I do at a single volume level? Shred the whole tenant? Probably not that often. Yeah. So that's why granularity is important. Like, yes. So if I understood directly, Jeff, today we don't have not just an object. The question was very specific to
*24:20*

**Jason Vallery**: The use case would be customer managed encryption key. They want to crypto share their volume. Now do I have to do it at the tenant level or can I do it at a single volume level? Shred the whole tenant probably not that often. Yeah, so that's why granularity is important. Yes. So if I understood correctly, Jeff, today we don't have not just an object. The question was very specific.
*24:22*

**Jason Vallery**: specific to object, but we don't have that at a block file level as well. We have finer granularity at a block file level as well. I'm not sure how that showed through in the, right off the top of my head, I don't remember how it showed through in the RFP, but it's definitely something we're interested in for sure for the very reasons we mentioned. Yeah, my question was more for VASC, like in the implementation part, do we have more granular keys or volume and block, or it's
*24:50*

**Speaker 7**: object, but we don't have that at a block file level as well. We have finer granularity to block and final level as well. I'm not sure how that showed through in the top of my head. I don't remember how it showed through in the RFP, but it's definitely something we're interested in for sure for those very reasons we mentioned. Yeah, my question was more for vast, like in the implementation part, do we have more granular keys for volume and block or it's
*24:50*

**Jason Vallery**: Same for across all three protocols and just in the 10th level today.
*25:20*

**Speaker 4**: So same for across all three protocols and just at the tenant level today. Yeah, I thought in an earlier discussion, I thought it actually was, you know, you have all these subtrees in your organization. I thought you could assign different keys. You could have keys at a subtree kind of granularity,
*25:20*

**Jason Vallery**: Yeah, I think it may be at the view level, but I need to double check exactly how that's exposed. So let me take this as a follow-up because I don't want to speak incorrectly.
*25:24*

**Jason Vallery**: Yeah, I thought in an earlier discussion, I thought it actually was, you know, you have all these sub-trees in your organization. I thought you could assign different keys.
*25:34*

**Jason Vallery**: You could have keys at a subtree kind of granularity, which would be useful. That's possible. I just don't know how it ties back to bucket policies, so I need to see how this will work. Okay. Yeah, so if you could provide more information on that as it pertains to all of block file and object, that would be helpful. Yep. All right, moving on to the next one.
*25:47*

**Speaker 4**: which would be useful. Okay. Yeah. So we, yes, if you could provide more information on that as it pertains to all of block file and object, that would be helpful. All right. Moving on to the next one.
*25:50*

**Lior Genzel**: Can you speak to D7, Jason?
*26:20*

**Jason Vallery**: Can you speak to D7, Jason? Yeah, I mean, we have a management API for being able to configure and update the cluster and so forth. So maybe in some ways you could think of it as a fault injection API because there are destructive things you could do in there, but it's not intentionally designed to be that. So I don't know what exact scenario you were trying to solve for with fault injection APIs and if this is about, you know, just rollout testing.
*26:22*

**Jason Vallery**: update testing like what the scenarios are you're trying to hit with it.
*26:47*

**Speaker 4**: Yeah, well, the scenarios are just making sure that we are able to meet our SLOs in the face of the kinds of faults that we expect to have to be able to manage, and then to have regression tests against those as well. So ideally, we'd be able to automate testing with fault engine.
*26:54*

**Jason Vallery**: Yeah well the scenarios are just making sure that we are able to meet our SLOs with in the face of the kinds of faults that we expect to have to be able to manage and then to have regression tests against those as well. So ideally we'd be able to automate
*26:54*

**Jason Vallery**: testing with fault injection to confirm that we're still able to meet our SLOs under fault conditions.
*27:17*

**Speaker 4**: to confirm that we're still able to meet our SLOs under fall conditions. Yeah, that's, you know, it's a step, a small step in that direction, sure. No, I mean, if you simulate a hardware failure, you know, they take, you know, depending on what you
*27:21*

**Jason Vallery**: I mean, the management ABI would, for example, allow you to take a node out of surface, but I don't know if that's really a fault injection.
*27:28*

**Jason Vallery**: Yeah, that's, you know, it's a step, a small step in that direction, sure.
*27:36*

**Jason Vallery**: No, I mean, if you simulate a hardware failure, you know, they take a, you know, depending on what you consider your legitimate fault domains are, you know, simulate a drive failure or simulate a controller failure, one of the controllers failing in one of your shelves, those kinds of things are what we have in mind when we ask that kind of question. Gotcha. Okay.
*27:43*

**Lior Genzel**: to consider your legitimate fault domains are to have a simulated drive failure or simulate a controller failure, one of the controllers failing in one of your shelves. Those kinds of things are what we have in mind when we ask that kind of question. Okay. Jason, all of the feedback you provided on this tab that we have displayed right now was based on the
*27:50*

**Jason Vallery**: Jason, all of the feedback you provided on this tab that we have displayed right now was based on the pricing for the products on this tab that I'm displaying right now, is that correct? I'll defer to lure, but I mean those are all software level conversations, the hardware is sort of independent of it. No, there is also hardware here. So the hardware is, again, are we moving to pricing now? Yes, yes. Okay, so
*28:13*

**Speaker 1**: the pricing for the products on this tab that I'm displaying right now, is that correct? No, there is also hardware here. So the hardware is, again, are we moving to pricing now? Yes, yes. Okay, so the way this is structured is that, the way we structure it is, we looked at the configuration you guys have asked us to build,
*28:20*

**Jason Vallery**: The way this is structured is that the way we structure is we looked at the configuration you guys have asked us to build clusters for us and these are much smaller than or smaller than what we usually sell in the field because it started with 150 TIB to 250 if I remember correctly then 300 and then 500. So what we've used is the basic building block of entry-level system for VAST based on the ODM
*28:43*

**Speaker 1**: and these are much smaller than what we usually sell in the field. Because it started with 150 TIB to 250, if I remember correctly, then 300 and then 500. So what we've used is the basic building block of entry-level system for VAST based on the ODM artwork that our customers usually buy, what we call two-by-one. Two-by-one is two C nodes,
*28:50*

**Jason Vallery**: that our customers usually buy.
*29:13*

**Jason Vallery**: What we call 2x1.
*29:17*

**Jason Vallery**: 2x1 is 2 C nodes and 1 D box.
*29:18*

**Speaker 1**: and one debug. And this pricing tab here also calls out the outdoor components, not just the software. So the pricing you're seeing here do include, based on the number of months and the support levels, the software, the support, and the needed outdoor as one. But it's also splitted in the numbers between software and outdoor. So clearly we do not want to sell the outdoor. So we're just sharing the transfer prices, which is exactly the same as we have in
*29:20*

**Jason Vallery**: And this pricing tab here also calls out the algorithm components, not just the software.
*29:22*

**Jason Vallery**: So the pricing you're seeing here do include, based on the number of months and the support levels,
*29:28*

**Jason Vallery**: the software, the support, and the needed algorithm as one.
*29:35*

**Jason Vallery**: But it's also split in the numbers between software and output. So it was clearly we do not want to send the output. So we're just sharing the transfer prices, which is exactly the same as we have in the field. We didn't try to pre-negotiate with our ODM suppliers a deal for Google. We just use the same transfer prices as we're using for normal business. And because fast their policy is not to take any margin on output. We don't transact that deal. But I'm sure that there is more room for space and negotiation with our providers.
*29:40*

**Speaker 1**: We didn't try to pre-negotiate with our ODM suppliers a deal for Google. We just used the same transfer prices as we're using for normal business. Because vast policy is not to take any margin on hardware. We don't transact that deal. But I'm sure that there is more room for space in negotiation with the hardware providers. We also did not look at OEM providers, meaning likes of Dell and HP and Cisco and others. Because when we use their hardware, usually the
*29:50*

**Jason Vallery**: did not look at OEM providers, meaning likes of Dell and HP and Cisco and others, because when we use their artwork, usually the, not usually, the entry level system needs to be an eight node cluster. That's what we're calling e-boxes. And an eight node cluster usually starts from a petabyte scale. So all of the configurations you guys suggested were just smaller than that. So it didn't make any sense to build that with e-boxes, with OEM partners.
*30:10*

**Speaker 1**: not usually the entry level system needs to be an eight node cluster that's what we're calling eboxes and an eight node cluster usually starts from a petabyte scale so all of the configurations you guys suggested were just smaller than that so it didn't make any sense to build that with eboxes with oem partners okay thank you for that uh the jeff go ahead i have some questions too yeah so if we were buying is there i'm trying to understand
*30:20*

**Jason Vallery**: Thank you for that, Luar. Did you have to go ahead? I have some questions too.
*30:40*

**Jason Vallery**: Yeah, so if we were buying... I'm trying to understand why you're thinking that... why the implication of an ODM is that it ends up being e-boxes. If we were able to find an ODM supplier for d-boxes and then just use ODM vendors for those...
*30:45*

**Speaker 4**: why you're thinking that why the implication of an ODM is it ends up being eboxes. If we were able to find an ODM supplier for dboxes and then just use ODM vendors for the C boxes, is that a valid configuration that can...
*30:50*

**Jason Vallery**: Cboxes can be any, Cboxes are, you know, standard servers, right? Okay. You choose your poison. That's an easy one. And what we use there, and there is the price of hardware, I think it's $1500 for the box, is just the ODM servers that, you know, we're sourcing. That's simple. That's Cboxes. This is the servers you had for adding performance, you know, IOPS or throughput. Dboxes, we have
*31:10*

**Speaker 1**: Cboxes can be any. Cboxes are standard servers, right? You choose your poison. That's an easy one. What we use there, and there is the price of hardware, I think it's $14,500 for the box, is just the ODM servers that we're sourcing. That's simple. That's Cboxes. This is the servers you add for adding performance, IOPS of throughput. Dboxes, we have designed Dboxes with our ODM suppliers, which are very
*31:15*

**Jason Vallery**: We have designed Dboxes with our OEM suppliers, which are very dense JBoss with Bluefield interface. And we have shared two examples here, two sizes of Dboxes that we ship today. One is 320 and some terabytes, and the other one is costing a petabyte in scale, just the size of the drives. That's really the difference between the two configurations. So that's one option. What I was saying about the OEMs is that
*31:40*

**Speaker 1**: with Bluefield interface. And we have shared two examples here, two sizes of D-Boxes that we shipped today. One is 320 and some terabytes, and the other one is crossing a petabyte in scale, just the size of the drives. That's really the difference between the two configurations. So that's one option. What I was saying about the OEMs is that if you want to use any OEM for C-Boxes, then it's easy.
*31:45*

**Jason Vallery**: If you want to use any OEM for C boxes, then it's easy. You just take the spec of the server and you choose your OEM. But we also have another deployment model, which is not C and D. It's what we're calling E boxes. And in that deployment model, these are really only standard servers. And the C container and D container functionality runs on the same server. So it's an hyper-converged configuration. And that's how CIS and others will take it to market. Okay, but that also implies that you're
*32:10*

**Speaker 1**: You just take the spec of the server and you choose your OEM. But we also have another deployment model, which is not C and D. It's what we're calling e-boxes. And in that deployment model, these are really only standard servers. And the C container and D container functionality runs on the same server. So it's an hyper-converged configuration. And that's how Cisco and others will take it to market. Okay, but that also implies that your... is a node.
*32:13*

**Jason Vallery**: is a node, meaning all the storage behind that node could fail together when a node reboots or goes down.
*32:41*

**Speaker 4**: Meaning all the storage behind that node could fail together when you lose it when a node reboots or goes down this. Sure, it reduces your erasure coding for the time that that node is missing at least.
*32:43*

**Jason Vallery**: It doesn't impact the availability story. It's just around eliminating... You're effectively... The only real trade-off is that you no longer have the ability to size throughput in transactions independent of capacity. That becomes a fixed ratio based on the node scale, the node design.
*32:52*

**Jason Vallery**: Sure, it reduces your erasure coding for the time that that node is missing at least. It reduces your erasure coding factor. It causes you to do a lot more erasure coding overhead associated with reads at least. Yeah, and we'll start reconstruction immediately. You'll have a slight degradation in performance during that window, but it doesn't cause an availability impact. You'll see some subtle impact in performance depending
*33:06*

**Speaker 4**: It reduces your erasure coding factor. It causes you to do a lot more erasure coding overhead associated with reads at least, I would guess.
*33:13*

**Jason Vallery**: on how many nodes in the cluster and how many fail, but the erasure codes are wide stripes. Yeah, I can imagine there might be some use case where we're interested in the E node based architecture, but for the most part, you're looking at C and D. Yeah, look, the biggest disadvantage with Evox is actually the east-west traffic, right? With C and D, you're effectively eliminating the east-west traffic. Each C node can see all of the D nodes.
*33:36*

**Speaker 4**: There might be some use case where we're interested in the e-node based architecture, but for the most part, you'd be looking at C and D.
*33:43*

**Speaker 1**: Yeah, look, the biggest disadvantage with e-box is actually the east-west traffic, right? With C and D, you're effectively eliminating the east-west traffic. Each C node can see all of the D nodes and that's it. And, you know, they are aware of each other.
*33:53*

**Jason Vallery**: and that's it. And you know they are aware of each other. Ebooks, any CNode runs needs to access the other CNode servers to get access to the container. So 50% of the traffic is lost with that. So that's the biggest disadvantage. But the reason why it's so successful in the field, as you just mentioned, is that some customers are very opinionated about the hardware. And some customers have a lot of hardware they want to reuse. Again, we're a software company.
*34:06*

**Speaker 1**: ebooks, any Cnode runs, you know, needs to access the other Cnode servers to get access to their, you know, D container. So 50% of the traffic is lost with that. So that's the biggest disadvantage. But the reason why it's so successful in the field, as you just mentioned, is that some customers are very opinionated about the hardware. And some customers have a lot of hardware they want to reuse. Again, we're a software company. So we will do the best with the deployment model and the hardware that we'll get.
*34:10*

**Jason Vallery**: and do the best with the deployment model and the hardware that we get. As long as it's flash and fast network, then it works. So what you see here is the upper part and maybe we start with the lower part, the mid part. The mid part is the cluster configurations based on what you have asked for. And that's lines 4, 15, 16, and 17. I've shared the numbers here of kind of what was the request in TAB. And then I also showed an under number
*34:36*

**Speaker 1**: As long as it's flesh and fast network, then it works. So what you see here is the upper part, and maybe we start with the lower part, the mid part. The mid part is the cluster configurations based on what you have asked for. And that's lines 14, 15, 16, and 17. I've shared the numbers here of kind of what was the request in TAB. And then I also shared an under number of what, again, assuming that we can get our
*34:38*

**Jason Vallery**: of what, again, assuming that we can get our DRR at kind of market standards, and we just talked about it, that might not be the case, but what's the potential of getting from that system with the 2x1 DRR? But even without the 2x1, as you can see, we're still getting more than the required minimum, because that's the entry-level system. So line 14, as an example, 150 TAB requests, we're looking at 223 TAB, even without taking DRR.
*35:06*

**Speaker 1**: So DRR, it's kind of market standards. And we just talked about it. That might not be the case. But what's the potential of getting from that system with two by one DRR? But even without the two by one, as you can see, we're still getting more than the required minimum because that's the entry level system. So line 14, as an example, 150 TAB requests, we're looking at 223 TAB, even without taking DRR into account.
*35:08*

**Jason Vallery**: So that's the first four lines are exactly the cluster you have asked for. And then proactively I've created two other configurations with slightly bigger scale clusters with full pricing. And the main reason though is to show that the bigger the clusters gets, the more efficient our erasure coding becomes. We can go into sessions to explain kind of the thresholds and all that. But more than that on bigger capacity clusters, we can also use bigger drives.
*35:37*

**Speaker 1**: So these four lines are exactly the cluster you have asked for. And then proactively, I've created two other configurations with slightly bigger scale clusters with full pricing. And the main reason there is to show that the bigger the clusters gets, the more efficient our erasure coding becomes. We can go into sessions to explain kind of the thresholds and all that. But more than that, on bigger capacity clusters, we can also use bigger drives. And that drives the cost of the hardware dramatically.
*35:38*

**Jason Vallery**: and that drives the cost of the hardware dramatically. So the erasure coding, the efficiency, and the cost of hardware. So that's why I've included those two configurations. In the supporting document, we also created some higher scale clusters because you also wanted to see how does a 100-terabyte looks like in the configuration, but I did not go into the exercise of pricing the bigger clusters. Okay. So let me ask this then.
*36:06*

**Speaker 1**: the erasure coding, the efficiency and the cost of hardware. So that's why I've included those two configurations. In the supporting document, we also created some higher scale clusters, because you also wanted to see how does a 100 petabyte looks like and bigger configuration, but I did not go into the exercise of pricing the bigger clusters. Okay, so let me ask, let me ask this then the software and support, well, software costs and
*36:08*

**Jason Vallery**: The software and support, well, software classes, since you include bundling the supporting, how do those scale with capacity? I mean, as you're saying, there are various ways of getting efficiency at the hardware level with capacity, but what I'm unsure of is how the
*36:35*

**Speaker 4**: you include bundling the supporting. How do those scale with capacity? I mean, as you're saying, there are various ways of getting efficiency from at the hardware level with capacity. But what I'm unsure of is how the if we turn that dial somehow, does it really how big a difference does that make to
*36:38*

**Jason Vallery**: If we turn that dial somehow, how big a difference does that make to the software costs? It does. So first of all, on the right side, there are notes that I think I've shared some more information there, not only in pricing assumptions, but also on the right side, I just kind of have to call out the size. Let's see. No, so it's in Section 7.
*37:02*

**Speaker 1**: It does. So first of the way, on the right side, there are notes that I think I've shared some more information there, not only in pricing assumptions, but also on the right side, that's kind of to call out the size. Yeah, let's see. No, so it's in the other section seven, but pricing. The way it works is that, you know, the bigger the capacity is, so we have, we are,
*37:08*

**Jason Vallery**: But pricing, the way it works is that the bigger the capacity is, so we usually offer, and we did the same here, deeper discounts for scale-out clusters. So the bigger the cluster becomes, the more aggressive cost becomes on pricing. So when we're selling two exabytes to XAI, it's not the same price per terabyte as we're selling 200 terabytes. So that's one factor.
*37:31*

**Speaker 1**: We usually offer, and we did the same here, deeper discounts for scale-out clusters. So the bigger the cluster becomes, the more aggressive VAST becomes on pricing. So when we're selling two exabytes to XAI, it's not the same price per terabyte as we're selling some 200 terabytes. So that's one factor. As you guys probably saw, there is also a difference in price between four hours and next business day, which is roughly 10%, right? Within those two, we do.
*37:38*

**Jason Vallery**: There's also a difference in price between four hours and next business day, which is roughly 10%. Within those two, we don't break it up because for us, the way we support customers, it's bundled into the subscription. So I think that's maybe the three parameters. And I can follow up with a detailed email that explains the way that the structure is so you can understand the notes. But there is roughly 10%.
*38:01*

**Speaker 1**: So don't break it up because for us it's the way we support customers. It's just it's bundled into the subscription. And so I think that's maybe the three parameters and I can follow up with a detailed email that explains the way that the kind of the structure is so you can understand the knobs. But there is roughly 10% higher prices for dark sites. It's less than 10%. There is roughly 10% difference between four hours and next business
*38:08*

**Jason Vallery**: 10% higher prices for dark sites. It's less than 10%. There is roughly 10% difference between four hours and next business day. I'm rounding it through 10% because it's easy to explain. And there is at least 10% discount when you scale up clusters to bigger sizes. We couldn't really get to the scale over that because the size that you guys have asked for are really entry-level sizes. If the actual requirement is for 100 petabyte scale, then it would be a deeper discount. But we didn't quote that.
*38:31*

**Speaker 1**: I'm rounding it to 10% because it's easy to explain. And there is at least 10% discount when you scale up clusters to bigger sizes. And we couldn't really get to the scale over that because the sizing you guys have asked for are really entry level sizes. If the actual requirement is for 100 petabyte scale, then it will be a deeper discount. But we didn't quote that stuff of configurations. And that discount is based on the capacity per cluster?
*38:38*

**Jason Vallery**: of configurations. And that discount is based on the capacity per cluster? The pricing is, so pricing as of right now, the way it works for us is that the SKU, I think it's showing here, yes, line number 10, right? So the SKU is 100 terabytes, that's the software SKU. 100 terabytes usable capacity license. So when I had to quote the 150 TAB, I had to quote two of
*39:01*

**Speaker 1**: The pricing as of right now, the way it works for us is that the SKU, and I think it's showing here, line number 10. So the SKU is 100 terabyte, that's a software SKU. 100 terabyte usable capacity license. So when I had to quote the 150 TIB, I had to quote two of those licenses, even though you only asked for 150. So I delivered hardware that is, again, not aligned with the request, 223.
*39:08*

**Jason Vallery**: those licenses even though you only ask for 150. So I will deliver the hardware that is again not aligned with the request 223 TAB and the software license was 200 terabytes not TAB because we're right now priced on usable terabytes not TAB. Comes with February 1st we're going to change pricing and it's going to be based on per terabyte scale. So it's going to resolve itself and pricing will adjust itself to be even more accurate to your request.
*39:31*

**Speaker 1**: and the software license was 200 terabytes, not the AB. Because we're right now priced on usable terabytes, not the AB. It comes with February 1st, we're going to change pricing and it's going to be based on per terabyte scale. So it's going to be solved itself and pricing will adjust itself to be even more accurate to your request. So please don't place any POs before February 1st. Wait, wait with the POs. Thank you.
*39:39*

**Jason Vallery**: Please don't place any POs before February 1st. Wait, which will be the POs? Thank you. The licensing cost that's shown is per core, that's strictly C-blade. Yeah, so the licensing per core is just for the C-boxes. There are no cores in the D-boxes. It's not the same with E-box configurations because E-box do F-cores, so it's a different math. And we also give, in our
*40:01*

**Speaker 1**: The licensing cost that's shown as per core, that's strictly C-Blade. Yeah, so the licensing per core is just for the C-Boxes. There are no cores in the D-Boxes. It's not the same with E-Box configurations because E-Boxes do F-Cores, so it's a different math. And we also give, in our pricing structure today, there is kind of a discount based on the actual capacity of the cluster.
*40:08*

**Jason Vallery**: Pricing structure today, there is a discount based on the actual capacity of the cluster. So again, the bigger capacity you buy, the more discounts you get. And that drives down the price of the cores. So capacity base is just based on the capacity based on 100 terabytes increment. We change that to terabyte increments next year. Compute is the C nodes compute. And the bigger the capacity is,
*40:31*

**Speaker 1**: So again, the bigger capacity you buy, the more discounts you get. And that drives down the price of the cores. So capacity base is just based on the capacity based on 100 terabytes increment. We change that to terabyte increments next year. Compute is the CNotes compute. And the bigger the capacity is, the more discounts and incentive you'll get on the compute side. I can also share that math, which is behind the numbers you're seeing here.
*40:38*

**Jason Vallery**: the more discounts and incentives you'll get on the compute side. I can also share that math, which is behind the numbers you're seeing here. So you can get the formula to understand how it really works, but it was calculated that way. Okay, and if we wanted to translate this number of terabytes column into usable terabytes, would that just be dividing by the, your assumed data reduction ratio here of two?
*41:01*

**Speaker 1**: So you can get the formula to understand how it really works, but it was calculated that way. Okay. And if we wanted to translate this number of terabytes column into usable terabytes, would that just be dividing by the, your assumed data reduction ratio here of two? Is that- So you want to calculate, so this number is no DRR assumption at all, the column D right now.
*41:08*

**Jason Vallery**: So this number is no DRR assumption at all. The column D right now. So the two by one configuration that represents what we offer in line 14 will actually-- Ah, okay. Okay, so how-- This column is the real TID. Gotcha. Not taking account any DRR. What you see on the left side of the description is, I said, assuming DRR, because we're really good in that.
*41:31*

**Speaker 1**: So the 2x1 configuration that represents what we offer in line 14 will actually... Ah, okay. Okay, so how... This column is the real TIB. Gotcha. Not taking in account any DRR. What you see on the left side with description is, I said, assuming DRR, because we're really good in that. Encryption aside, 2x1, you get much higher capacity. You see the difference? It's exactly a double up. I do. Yeah, no, I think I was confused by saying...
*41:39*

**Jason Vallery**: I do, yeah, no, I think it's confusing. Unclear on what the cluster size is here. I'm trying to follow along. So it's okay, like Putin, the cluster size in TIB is 223 terabytes, right? That's the cluster size. It's basically... Usable capacity, usable capacity in TIB, not terabyte.
*42:03*

**Speaker 1**: - Unclear. And what the cluster size is here? I'm trying to follow along. So it's okay, line 14, the cluster size in TAB is 223 terabytes, right? That's the cluster size. It's based on a single capacity or is that the rocket capacity? Usable capacity in TAB, not terabyte. Usable capacity after erasure coding, after everything. Usable capacity, that's TAB, 223.
*42:09*

**Jason Vallery**: after erasure coding, after everything. Usable capacity, that's TAB, 223. Sorry? Yes, the 150 is our number. And you're saying that you would use a 223 usable. From a hardware farm factor perspective, does this translate to how many nodes? Leo, you said that's like... One D-books. This is one D-books and two C-notes. Two C-notes and one D-box.
*42:31*

**Speaker 4**: Yes, the 150 is our number.
*42:41*

**Speaker 4**: Yeah.
*42:44*

**Speaker 4**: 150 was our number.
*42:45*

**Speaker 4**: That is why that you would use a 223 usable.
*42:46*

**Speaker 5**: From a hardware farm factor perspective, does this translate to how many nodes, Leo?
*42:49*

**Speaker 5**: You said that's like two?
*42:54*

**Speaker 5**: One D-box.
*42:55*

**Speaker 5**: This is one D-box and two C-nodes.
*42:56*

**Speaker 5**: Two C-nodes and one D-box.
*42:59*

**Speaker 5**: Got it.
*43:01*

**Jason Vallery**: Got it. Okay. And that includes the storage class memory drives within the D-Box, whatever you see? Exactly. It includes the storage class memory, it includes the QLC drives. The overall price of that line includes two switches, includes the cables which are required to build the configuration, includes the installation cost, includes everything. It's the price for a cluster. Got it. And is that the same thing from ROSE 14 through, I don't know,
*43:01*

**Speaker 5**: Okay.
*43:01*

**Speaker 5**: And that includes the storage class memory drives within the D-box, whatever it is.
*43:01*

**Speaker 1**: Exactly.
*43:07*

**Speaker 1**: It includes the storage plus memory, it includes the QLC drives. The overall price of that line includes two switches, includes the cables which are required to build the configuration, includes the installation cost, includes everything. It's the price for a cluster. Got it. And is that the same thing from ROSE 14 through, I don't know, 19? Is that the same answer? So let's take that one by one. So not to be confused. 15, you guys have asked for 250 TIB.
*43:08*

**Jason Vallery**: So, let's take that one by one. So not to be confused. 15, you guys have asked for 250 TIV. Right. So what I did here, I've rounded it back to the same size, 223. So if we get no DRR, I'm actually not in compliance with your request. But as you can see, it's 223. And the assumption was that it would be over that with some level of DRR achievement, right? So line 15, it was either doubling up or just going
*43:33*

**Speaker 1**: So what I did here, I've rounded it back to the same size, 223. So if we get no DRR, I'm actually not in compliance with your request. But as you can see, it's 223. And the assumption was that it would be over that with some level of DRR achievement, right? So line 15, it was either doubling up or just going with 223. Line 16, the request was 300 TAB. So what we did here is...
*43:39*

**Jason Vallery**: 223. Line 16, the request was 300 TIB. So what we did here is... One question, if the rows 14 and 15 are identical, why do they have different pricing for software? They have different pricing because... Good question. Good catch. Thank you for that. Line 14, I did partial pricing for software. So since the request was only 150 TIB, I should translate to roughly 180 terabytes.
*44:01*

**Speaker 1**: If the rows 14 and 15 are identical, why do they have different pricing for software? They have different pricing because good question, good catch. Thank you for that. Line 14, I did partial pricing for software. So since the request was only 150 TIB, I translate to roughly 180 terabyte based on our pricing today. I use two SKUs of line 10 and that's it.
*44:08*

**Jason Vallery**: based on pricing today, I used two SKUs for line 10 and that's it. With line 15, the request was over 200, so I had to use three SKUs. That's the difference in... This is the spot for SKUs and the... Yes, I got an approval here for something that we usually don't do, which is partial software licensing for the configuration. Got it. That's it. And then it means that if you want to upgrade later on, you already got the hardware,
*44:31*

**Speaker 1**: With line 15, the request was over 200, so I had to use free SKUs. That's the difference in... This is the software SKUs in support. Yes, I got an approval here for something that we usually don't do, which is partial software licensing for the configuration. Got it. And then it means that if you want to upgrade later on, you already got the hardware, then we only need to save the software license as an increment. That's the difference between them.
*44:37*

**Jason Vallery**: Then we only need to serve a software license as an implement. That's the difference between them. Line 16, just to follow them to be clear, is a 300 TAB request. So here I took two D-boxes. So you can see that the scale from 223 to 503, right, the next scale up. And here I also used four licenses. So again, it's 300 TAB, which translates to more than 300 in terabytes.
*45:00*

**Speaker 1**: All of them to be clear is 300 TAB request. So here I took 2D boxes. So you can see that the scale from 223 to 503, right? The next scale up. And here I also used four licenses. So again, it's 300 TAB, which translates to more than 300 in terabytes. And our pricing works on terabytes, not TAB. So there is about 10% difference. That's why I use four licenses here.
*45:07*

**Jason Vallery**: and our pricing works on terabytes, not TAB. So there is about 10% difference. That's why I use four licenses here. So it's licensed for 400 terabytes usable. And line 17... Sorry, Tendor, this is still two C nodes plus one debug. Is that correct? Yes, it's two by two. No, no, it's two by two now. It's two C nodes, right? Because there wasn't any specific performance requirements, so I didn't, you know, create bigger clusters. So it's two C nodes, but it's two D nodes now.
*45:30*

**Speaker 1**: So it's licensed for 400 terabyte usable. And line 17... Sorry to interrupt, this is still two C nodes plus one D box, is that correct? Yes, it's two by two. No, no, it's two by two now. It's two C nodes, right? Because there wasn't any specific performance requirements, so I didn't, you know, create bigger clusters. So it's two C nodes, but it's two D nodes now. Because now I've doubled up the capacity, right? So now you need to have two D boxes to...
*45:36*

**Jason Vallery**: because now I've doubled up the capacity, right? So now you need to have two D boxes to... By the way, there is in the supporting document, you have... We can go there. There is drawings of all the configurations. Oh, I see. Okay. Okay, but it's really line 16 D, you can see it's two D boxes because it's doubled up the capacity. Now, you might ask, why isn't it 223 by 2? If I doubled up, there is another D box and I get
*46:00*

**Speaker 1**: There is in the supporting document, we can go there. There is drawings of all the configurations. I see. Oh, I see. Okay. Line 16 D, you can see it's two D boxes because it's double up the capacity. Now you might ask why isn't it 223 by two? If I double up, there is another D box and I get 223 from a D box. Why isn't it 223 by two? The reason is when I installed two D box,
*46:06*

**Jason Vallery**: 2.2. The reason is when I install two big boxes, automatically I get better erasure coding. So I get more efficiency. So I get more usable capacity. That's why. So the bigger the cluster is, the less overhead I have for erasure coding. We'll end with the last one that you have asked for because 18 and 19 are just my imagination of creating numbers. But the last one was 500 terabytes.
*46:30*

**Speaker 1**: automatically I get better erasure coding. So I get more efficiency. So I get more usable capacity. That's why. So the bigger the cluster is, the less overhead I have for erasure coding. We'll end with the last one that you have asked for because 18 and 19 are just my imagination of creating numbers, but the last one was 500 terabytes. So again, I've used the same capacity. It's two by two, two C nodes, two D boxes.
*46:37*

**Jason Vallery**: capacity, it's two by two, two C nodes, two D boxes. And this comes with six software licenses because now it's more than 500 terabytes usable. So it requires six software licenses. Again, comes the 1st of February, we're going to change pricing. It will roughly end up with the same. So we're, of course, committed to that pricing here, but we'll be more flexible on the actual usable capacity because it won't be 100 terabyte chunks anymore.
*47:00*

**Speaker 1**: with six software licenses, because now it's more than 500 terabytes usable. So it requires six software licenses. Again, comes the 1st of February, we're going to change pricing. It will roughly end up with the same. So we're, of course, committed to that pricing here, but we'll be more flexible on the actual usable capacity because it won't be 100 terabyte chunks anymore. Kamal, you've been waiting patiently. Sorry, go ahead.
*47:06*

**Jason Vallery**: Kamoam, you've been waiting patiently. Sorry, go ahead. Yeah, I think you answered already the part of my question like C node, D node configuration. I also had a quick question about what is the disk size we're using? They're different for each line or it's QLC, 60 terabyte or which one are we using? So then if you look at, I think I wrote it down here. No, I didn't. So probably I wrote it in the next one.
*47:32*

**Speaker 7**: You answered already the part of my question like c node denode configuration. Also the question about what is the disk size is we are using they're different for each line or it's QLC is 60 terabyte or which one are we using for these? So the if you look at I think I wrote it down here. No I didn't so probably I wrote it in the next tab but these are two different configurations of of dboxes. One is
*47:36*

**Speaker 1**: is the 223 terabytes and the other one is over a petabyte and the difference between them is the size of the drive. Right? So these are two different D-Box configuration that have two different sizes of drives. They both have the SCM drives. They both have QLC. But the bigger configuration is bigger drives. That's the difference here. So I saw you had 15 and 60. So one are 15 and one is 60? Exactly. That's why it's four times bigger. Okay. Both are QLC but for all your...
*48:06*

**Jason Vallery**: So I saw you at 15 and 60.
*48:28*

**Jason Vallery**: So in line 18 and 19 I have used configurations which are based on the 60 terabyte drives. And does this cover your units or block units better? Like let's say if somebody has to go one petabyte, will they just buy two or five or threes? If somebody wants to go with a petabyte of usable capacity,
*48:30*

**Speaker 7**: So in line 18 and 19, I've used configurations which are based on the 60 terabyte drives. And does this cover your units or block units better? Like, let's say if somebody has to go one petabyte, will they just write you like two or five or threes? If somebody wants to go with a petabyte of usable capacity, then yes, he can, assuming that the performance is size right, he just needs to have a single Dbox and that's it.
*48:38*

**Jason Vallery**: - Yeah. - Assuming that the performance is size right, he just needs to have a single D-box and that's it. - With 60s again. - Yeah. - Yeah. Petabyte is still a single D-box with a 60 terabyte drives. Now again, in the RFP we answer that for scale configuration, we're all using QHC which are 122 terabytes. Normally looking for some of our exabyte customers for even double that. But again, the sizing you guys have suggested here, under the assumption that
*49:00*

**Speaker 1**: With 60s, okay. Yeah. Petabyte is still a single D-box with a 60 terabyte drive. Now again, in the RFP, we answer that for scale configuration, we're all using QLC, which are 122 terabytes. And we're going to be looking for some of our exabyte customers for even double that. But again, the sizing you guys have suggested here, under the assumption that you're building clusters for specific customers, a single tenant, we tried to fit it the best possible way.
*49:07*

**Jason Vallery**: You're building clusters for specific customers, a single tenant. We try to fit it the best possible way. Is there an option to mix and match drives in your Dnode or they have to be exact same as they're all? To the best of my knowledge, it's the same size of drives per Dbox. So you can mix different Dboxes in the same configuration. And we have talked about different generations of Dboxes and Cboxes in the same cluster. That's easy. That's the way it's designed to work.
*49:30*

**Speaker 1**: Is there an option to mix and match drives in your Dnode or they have to be exact similar all of them? To the best of my knowledge, it's the same size of drives per Dbox. So you can mix different Dboxes in the same configuration and we have different generations of Dboxes and Cboxes in the same cluster. That's easy. That's the way it's designed to work. But in a single Dbox, it needs to be the same size of drives. Okay.
*49:36*

**Jason Vallery**: It's probably the same size of drives.
*50:00*

**Jason Vallery**: Ideally everything is the same at a time of deployment, but you can expand a cluster later with newer generation of hardware and that's fully supported, which then brings that asymmetry in.
*50:03*

**Speaker 7**: Yeah, I think the way we carved the sizes were more for block storage, but in your case, they applied to both object and block. So your size doesn't have to be added for both use cases and come to the numbers where we need to land, I guess. Okay. Okay. We're almost up on time. Malikarjan, Kamal, Jeff. Ideally, the way I would be able to interpret this is just hardware plus software.
*50:12*

**Jason Vallery**: Yeah, I think that we have a car that sizes were more for block storage, but in your case they apply to both object and block. So your size doesn't have to be added for both use cases and come to the numbers where we need to land, I guess.
*50:12*

**Jason Vallery**: Okay. We're almost up on time. Malik, Arjun, Kamal, Jeff. Ideally, the way I would be able to interpret this is just hardware plus software and services for a certain SKU to satisfy a requirement for us. But I don't feel like I know enough yet in terms of how the logic works here. Are you each comfortable with how we can define that and derive that internally on our side without asking additional questions and we can just have that conversation offline?
*50:26*

**Lior Genzel**: and services for a certain SKU to satisfy a requirement for us. But I don't feel like I know enough yet in terms of how the logic works here. Are you each comfortable with how we can define that and derive that internally on our side without asking additional questions and we can just have that conversation offline? Well, my main question on this pricing is, you know, it's unlikely, I'm guessing it's unlikely we'd find QLC drives that have FIPS capability.
*50:36*

**Jason Vallery**: Well, my main question on this pricing is, you know, it's unlikely, I'm guessing it's unlikely we'd find QLC drives that have FIPS capability. Assuming we can get the platform to support FIPS drives, and therefore we end up with less density, higher cost per gigabyte media, does that affect the software costs associated with those terabytes at all? Looks like the SKUs are structured so that it wouldn't be a lot.
*50:55*

**Speaker 4**: Assuming we can get the platform to support FIPS drives and therefore we end up with less density higher cost per gigabyte media does that affect the software cost associated with those terabytes at all it looks like the excuse or structured so that would not so it would be an increase in the cardboard cost associated with that media but not software cost correct correct yeah are there is the hardware quoted for self-interest
*51:06*

**Jason Vallery**: So it would be an increase in the hardware cost associated with that media, but not software cost. Correct. Correct. Okay. Are the hardware coded for self-encrypting drives or is it unencrypted? So it didn't take that into account. Just standard hardware. Yeah. I see. I think Mark, we did call for a SED, I believe, Jeff, right? It did. However, the questions in the spreadsheet did not
*51:25*

**Speaker 4**: So I didn't take that into account. Just standard hardware. Yeah. I see. Yeah. I think it did call for a SED, I believe, Jeff, right? It did. However, the questions in the spreadsheet did not mention SED drives. Exactly. Exactly. Yeah. Yeah. We looked at this exercise and we were under assumption it's just for you guys to get a feel on our pricing.
*51:38*

**Jason Vallery**: We looked at this exercise and we were under the assumption it's just for you guys to get a feel on our pricing, but because there isn't also no performance expectations per cluster, it's just capacity. So is it two by one good enough? Maybe it needs to be three by one, more performance. So we went with the minimum set just to... That's a different set of questions there. Yes, we would love to know more about the performance levels.
*51:59*

**Speaker 4**: there isn't also no performance expectations per clusters, it's just capacity. So is it two by one good enough? Maybe, you know, it needs to be three by one more performance. So we really went with the minimal set of just to... That's a different set of questions. Yes, we would love to know more about the performance levels of a CBOX per core or whatever. I believe that in the support document per configuration, we added more information about each of these configurations
*52:06*

**Jason Vallery**: I believe that in the support documents per configuration we added more information about each of these configurations not on usable capacity but power consumption, performance and so on and so forth so check it out. We'll look at that if we have more questions yes thank you. Okay really quick just so unclear the support services are bundled as part of the support cost is that right? Correct.
*52:28*

**Lior Genzel**: not only usable capacity, but power consumption, performance, and so on and so forth. So check it out. We'll look at that and see if we have more questions. Yes, thank you. Okay. Really quick, just so I'm clear, the support services are bundled as part of the support cost. Is that right? Correct. Okay. And that question around whether or not
*52:37*

**Jason Vallery**: Okay. And that question around whether or not these are self-encrypting drives, the answer was no. These were based on generic SSDs. So, sorry, sorry for a pedantic question. A quick, really quick follow-up. So, do you actually have a different price for SCDs or are SCDs not supported at all? So, let's start with the self-requicing. Are self-requicing these
*53:02*

**Speaker 5**: These are self encrypting drives. The answer was no. These were based on generic SSDs. Sorry, sorry for a pedantic question. A quick, really quick follow up. So do you actually have a different price for a city or a city is not supported at all? So let's start with the software pricing. Our software pricing is the same, right? Doesn't change. So understood. Now hardware, Jason, I think, and then follow up action item to take and check it out internally, right? On that specific point.
*53:06*

**Jason Vallery**: - It doesn't change.
*53:26*

**Jason Vallery**: - Understood.
*53:27*

**Jason Vallery**: - Now hardware, JSON I think added a follow-up action item to take and check it out internally, right? On that specific point.
*53:29*

**Jason Vallery**: - Yeah, I mean, we don't often deploy on self-encrypting drives, so I'm not sure what our ODM partnerships look like there and if we have any hardware options for you.
*53:35*

**Lior Genzel**: We'll get the key on us. Does your software support self-encrypting drives and external key managers for them? Okay. Okay. We're at time, but I just wanted to touch on this topic here of bidder investment. Anything to share with us?
*53:43*

**Jason Vallery**: - Will you get your support on us?
*53:44*

**Jason Vallery**: - Does your software support self-encrypting drives and external key managers support them?
*53:46*

**Jason Vallery**: - Yeah, that's a follow-up.
*53:52*

**Jason Vallery**: Okay. Okay. Okay. We're at time, but I just wanted to touch on this topic here, a bigger investment. Anything to share with us at a high level, despite what's already inside of this? Yeah. So we had a similar exercise with one of the other CSPs of signing a big contract. Yeah. There was a question. People need to leave. Signing a big contract which requires this kind of API integration and collaboration.
*53:54*

**Speaker 1**: at a high level despite what's already inside of this? Yeah. So we had a similar exercise with one of the other CSPs of signing a big contract. Yeah, there was a question. People need to leave. Signing a big contract which requires this kind of API integration and collaboration. And we use the same concepts of what we have learned from the other project to size this project. So I guess that's where the assessment of the engineers that we need to put behind
*54:06*

**Jason Vallery**: We use the same concepts of what we have learned from the other project to size this project. So I guess that's where the assessment of the engineers that we need to put behind it came from. And my team had a lot of other questions. And this is, again, just to kind of set expectations. And I think you can see that it's really, it's not a huge number of employees and, you know, $10 billion investment in engineering. Vast is very nimble.
*54:24*

**Speaker 1**: it came from.
*54:36*

**Speaker 1**: My team had a lot of other questions.
*54:37*

**Speaker 1**: And this is, again, just to kind of set expectations.
*54:39*

**Speaker 1**: And I think you can see that it's really, it's not a huge number of employees
*54:43*

**Speaker 1**: and, you know, $10 billion investment in engineering.
*54:48*

**Speaker 1**: Vast is very nimble on that level.
*54:51*

**Jason Vallery**: and we have proven to ourselves that we know that's true for us there. Other commentary was a request about us for the PLCs. So we just made it clear that for PLCs we won't charge for anything, but it's our software, which means you need to negotiate with the hardware supplier about hardware. And the other topic here that was, I think the last one on the list was about lab equipment. So again,
*54:54*

**Speaker 1**: And we have proven to ourselves that, you know, that's true for us there.
*54:55*

**Speaker 1**: Other commentary was a request about, you know, us, you know, for the PLCs.
*54:59*

**Speaker 1**: So we just made it clear that for POCs, we won't charge for anything. But it's our software, which means you need to negotiate with the hardware supplier about hardware. And the other topic here that was, I think the last one on the list was about lab equipment. So again, this is kind of pre-negotiation and all that, but the concept was that, you know, go with roughly 50% off pricing for the lab configuration. And then the actual price, it depends on what type of equipment,
*55:04*

**Jason Vallery**: This is kind of pre-negotiation and all that, but the concept was that, you know, go with roughly 50% off pricing for the lab configuration and then the actual price, it depends on what type of equipment you want to buy for the lab. But it will be cheaper and that's something that we're happy to discuss. Our team also has a lot of questions about that email that came out with a $12.1 million investment in hardware and, you know, what exactly is it going to be used for? What are the expectations around the payment terms of that system?
*55:24*

**Speaker 1**: you want to buy for the lab, but it will be cheaper. And that's something that we're happy to discuss. Our team also has a lot of questions about that email that came out with a $12.1 million investment in hardware and what exactly is it going to be used for? What are the expectations around the payment terms of that system? And when will it need to be acquired and so on and so forth? That would be an interesting discussion to have on our side. You saw the way we replied to it.
*55:34*

**Jason Vallery**: and when it needs to be acquired and so on and so forth. That would be an interesting discussion to have on our side. You saw the way we replied to it. I guess that's it. It's really straightforward here, but this is a very detailed part of the Excel sheet and we'd happy to discuss it in a follow-up conversation. Okay. Yeah, we are at a time. I appreciate the high-level context. I'll dive through this with the team. If I have any follow-up questions, I'll send those to you in writing in advance of our
*55:54*

**Speaker 1**: That's it. It's really straightforward here, but this is a very detailed part of the Excel sheet and we'd happy to discuss it in a follow-up conversation.
*56:04*

**Lior Genzel**: Okay. Yeah, we are at time. I appreciate the high-level context. I'll dive through this with the team. If I have any follow-up questions, I'll send those to you in writing in advance of our next meeting. But most likely we'll be chatting again. So I'll reach out to you once the team and I have a little bit of time to digest here. I'll also drop a note with some of what we discussed so that way you and Jason can help us.
*56:15*

**Jason Vallery**: next meeting.
*56:24*

**Jason Vallery**: But most likely we'll be chatting again.
*56:25*

**Jason Vallery**: So I'll reach out to you once the team and I have a little bit of time to digest here.
*56:27*

**Jason Vallery**: I'll also drop a note with some of what we discussed so that way you and Jason can help track down some of the open items.
*56:31*

**Lior Genzel**: help track down some of the open items. Okay, when should we expect the next step? Oh, we are swamped this week. Give me towards the end of the week, follow up with me if you don't hear from me by Thursday, Friday, and then I can advise on next steps. Okay, David and Tim, thank you for your time. Thank you for everything. Bye-bye. Yeah, thank you. Take care now. Bye.
*56:34*

**Jason Vallery**: Okay.
*56:36*

**Jason Vallery**: When should we expect the next step?
*56:37*

**Jason Vallery**: Oh, we are swamped this week.
*56:39*

**Jason Vallery**: Give me towards the end of the week.
*56:41*

**Jason Vallery**: Follow up with me if you don't hear from me by Thursday, Friday, and then I can advise on next steps.
*56:43*

**Jason Vallery**: Okay.
*56:47*

**Jason Vallery**: David and Tim, thank you for your time.
*56:48*

**Jason Vallery**: Thank you for everything.
*56:50*

**Jason Vallery**: Bye bye.
*56:51*

**Jason Vallery**: Yeah.
*56:52*

**Jason Vallery**: Thank you.
*56:52*

**Jason Vallery**: Thank you.
*57:23*

