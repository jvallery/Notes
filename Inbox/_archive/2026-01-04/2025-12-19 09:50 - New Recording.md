Okay. So let's walk through maybe the top table real quick and we'll just show what it is. So yeah, so essentially we are looking at some sort of 80/20 read/write ratio, uniform random reads. The latency we're trying to keep is under two milliseconds, so all your testing and performance make sure the latency numbers are lower than two milliseconds for performance. So everybody has the same level field because sometimes people can that number and your other numbers can go up and down accordingly so that's essentially the idea. We usually do our IOPS benchmarking at 4k. Of course customers can use this IO size at different workload types or whether there's sequential read, sequential write, it could be different but for IO numbers we are saying okay what if it was just a 4k how many IO's can you support. Protocol for block was NVMe or TCP which you support. File protocol, V4, Object S3. Drive types. These are the drive types we require for a lot of our customers. Not all of our customers, but some of our customers have a requirement of those. So those are the ones we are choosing for benchmarking. If there are issues or concerns, we can probably talk about those, but that's essentially our standard config. Yeah, let me jump in on that one. That is something of concern. I did talk to the team about this. For a variety of reasons, we've chosen not to implement self-encrypting drive support. So we make our FIPS check marks, we hit the criteria based on dual software layer encryption. You know, it's proven problematic to fully support self-encrypting drives given our architecture. And so that's not currently something we do support. Okay. Yeah. So we can make a note of that to say this is not option or not enabled in our environment and we have encryption at other layers. We probably have a similar, this is more of performance commercial type of discussion, but we definitely would have one more way we go deep dive into the security aspects of it. And if we have time today, we can jump into it. I just want to make sure we cover others, but. Okay. So, Kamal, one second. Sure. So, Jason, so just to confirm, right? So, you're saying there are still double data encryption going on in the storage system. That's right. It's just that it's not at the drive level. Both re-encryptions are outside the drive. Right, it's happening in software on fast versus happening by the drive on the drive. So it still achieves the FIPS requirements, but because of various issues with managing that hardware, particularly like the way we have dual controllers. So if you think about one of our D-Boxes, it has high availability controllers in it that can fail over to each other that are both managing the same drive. Supporting self-encrypting drives in that world has proven very challenging and problematic. Okay, thanks. Yeah, I would also put a detail of what your encryption type is so we have good understanding of what is the ultimate you are using in the configurations. Okay. Okay. Yeah. So let's go encryption in transit enable. So we use IPsec for file and block and we use TLS for S3. So that essentially is must requirement. So a lot of time these are are the type of things which will take away IOPS. So we really want to make sure we are benchmarking with these enabled in your testing. Encryption at rest, yeah. So at tiny granularity or final, like we, this is primarily for both for our, like as we already talked about SCDs. Malikar, did we add this line after? What was the thought here? Sorry. Sorry, which line? Line 10. No, this is the double encryption we just talked about, right? Okay, alright, so that's covered already. Yeah, yeah, yeah. Okay, so we already have... The second layer of encryption. Ah, okay, so volume level, okay, yeah, sorry. Yeah, yeah. Saying, okay, so you have hard drive level, hardware level and software level. So you already have software level. The first one was a drive level. Okay, we have two breakups. D-dupe and compression, usually because our data is encrypted at source, there's not much we get, so we kind of keep it disabled so we really don't we have a column where it would be good for us to know what your capabilities are but ideally in lot of scenarios we don't get benefit of that and durability is like what you are the durability you support for each of these configurations in terms of 5 9s 10 9s depending on the protocol or system got it and when you the configurations we've got here 16 through 19 are tied back to the designs we put in the proposal that you're specifically asking us to benchmark - Sorry, I... - Well, the capacity numbers here, which are the numbers we actually use, were the same thing we asked in the RFP? - We provided ranges in the PDF RFP document, and Jason and team came back with similar offerings to meet those ranges. But what we're asking for here is more specific to our actual... and use cases. So that's what we're looking to see is to get pricing for our actual use cases instead of the ranges. I see. It's almost the same, but it's not. The 112 is smaller than the 150 before. But it's just almost the same. I think that's fine, Lior. I think part of the focus, as Kamal was talking about earlier, is for us to really understand the IAPS per GB in this exercise and the pricing that goes for that performance, right? Yeah. So that's really the focus. That's why you were saying approximately, you see that there. So I think, yeah, what you have, please do share with us the small, medium, large, extra large configurations. They may not be precisely the numbers as shown. Yeah. Okay. Yeah. So what I would do in this case, let's say you have small, right? 112, but your number is actually 150. So I would add another line under it to say, hey, my number is 150, but do also fill for 112. Say, if I have to sell that config, I still have to buy a 150 box. I need to understand what price point it comes to me. But then you can put a line under it to say, by the way, at 150, it's better because I can do all of this and also give you the capacity. So just make sure you fill these, but feel free to add more color with adding lines to it. Yeah. I guess the higher level message I'm trying to share is that, you know, most of these numbers are going to be dependent upon the underlying hardware and the sizing of that hardware. And as I think we've talked about in previous calls, we can size throughput independently from capacity. and size performance overall independent of capacity. So we can give you sort of a reference design, but we also have more flexibility than necessarily this benchmarking would expose when we kind of try to drive higher performance numbers. So what ultimately the way this would play out if we were having a workload conversation with a customer is we would take their workload characteristics and their capacity demand and then size a system targeted at that design. to one workload or one customer. This is the cloud we are building for multiple use cases. So we have some patterns we'll show about it in the second tab over here. But idea is to try to see what our product lines are and what we will need to buy for those. That's the first order understanding. And the dimension we are pivoting here on is build it to the capacity and IOPS we want. Like if you have 112 terabyte of capacity and 92 IOPS per gig, for each of those capacity, what would your, what the configuration would I have to buy for that product? Once I buy the product, what price it would be in, and what is the total IOPS, like if you go more to column F, G, H, what is the total IOPS throughput and max throughput it will support? So that gives me some idea of, okay, for this price point, I'm getting this much performance and this much box. I totally understand. in your case you have more flexibility. If you have a way to show by adding more lines like hey per controller I can give you A, B, and C and per drive size I can give B and B. Like what is your building blocks? Feel free to add those. But this is actually where we have use case where we really want to for our product use cases where we really would like to use the configuration what it would cost and perform for us. Makes sense. And so specifically the target number that we're driving is based on IOPS and we'll will target 2, 5, etc. in these tests. That's the ratio. That is your starting point. You're saying, "Hey, I want to take capacity in IOPS, build a configuration around it." And then, like if in that configuration throughput is coming out really, really low and you think that's not sustainable, you might have to tweak your configuration. My idea is that's the starting point, yes. Ultimately, we're looking at column O to R to compare against all the vendors, what IOPS per gig they provide. in this case you will be 2 or 3 depending on what your controllers can do. Ask is 2 but maybe your controllers can go more. What is the read throughput per gig I get? What is the right throughput I get? And what is the price per GB for each of those configurations you're providing? If I may Kamal, one more thing. Just to add to what Kamal just said, Jason and Lior, what we're really trying to do is, this is what we do today in GDC. The storage service queues are based on, their pricing is based on IAPS per GBbyte. And we of course also want to provide the highest throughput that goes with that at that IAPS per GBbyte. But the primary pivot is IAPS per GBbyte. So that's why you see all of them call out in rows 22, 30 and 38. Basically all those things are all calling out IAPS per GBbyte as the starting And you would want to optimize it through put it at that IAPS, but that's not the starting point. It's the IAPS that's the starting point. Yeah, got it. I have a question which is related and unrelated. So we are very good with enabling multi-tenancies and quality of service on the same cluster for multiple users. Is that something that might be a fit here or it really needs to be separated clusters and separated hardware? No, no, no. Multi-tenancy is very much a part of GDC architecture and substrate capabilities in regards to multi-tenancy layer will be leveraged, will be relied on by our solution stack. This is complementary to that. This is not detracting from that multi-tenancy. I know. The reason why I ask is because right now our smallest configuration is 223 TIB, right? And your smallest request is 1.112. So clearly if we can build bigger systems that will be used to multiple users that will address these clusters with virtual clusters, then we have a big advantage. If we need to build big clusters for small configurations, then we have a disadvantage in price. But again, we can keep that for later. The request so far is very clear. Yeah. And I guess ultimately we don't have lab environments that replicate each one of these potential hardware scenarios. So what we'll end providing you is the scaling dimensions and an approximation of what we expect performance for a configuration of that size to be. Yeah, what would be the your nearest setup which will give us this number and what would the price and performance for that is? Yeah. So you could say my standard is to fit to 300 I will provide 300 for you for line 70 or 223 is what you said for 16 that's what we will have to provide because these are different use cases, some for management, there are different use cases for this. So if they were all one system and it would be easy to carve, we could just get one big and everybody could carve. But these are for different reasons. These are either fault domains or different deployments of the same solution and the sizes are carved according to those. Okay, and the SKUs we have is two IOPS per game. which is line 40. Line 22 is 5 IOPS per gig. Actually, yeah, line 30 is 10. I don't think we removed the throughput part, right? Like we just said, just two IOPS pivoting. Line 30. I think this has some... David? Yeah, yeah. I think those things need to be removed, David. I thought we remembered them, but maybe not. Yeah, okay. Yeah, so everything outside the 4K we can go. Same thing for a file, we have two IOPS per gig. We can remove it. Okay, so... And so columns, can you scroll to the right again? I just want to see what G was. the rest over there what you're looking for. Well, scroll to the top. Let's go to the block ones and then we can go in object. They're different. Okay. Okay. What's our use? Rack unit. Oh, yeah. I have to put now for this. Like I'm giving you three terabytes now your half rack is gone. I probably wouldn't be able to buy that in my data. Yeah, yeah, yeah. Those are some understand God power. I guess in my mind, those are like columns A and B because it's really about the hardware configuration, not about the performance. But yeah, I gotcha. Yeah, just trying to understand like what all this comes as. So yeah. Okay. I think that's explaining that this is kind of the normalization across multiple solutions to say what range is there in like, we're not going to say exactly like, but we at least build a graph where what fits in this. Yeah. Yeah, I think one of the key messages you'll find when we respond here is that the performance is decoupled from capacity. So effectively what we're going to end up likely providing is that these numbers should roughly be the same across all of the configurations and it'll be much more of the IO characteristics that drive the throughput and results. and then that would be consistent because we're normalizing on capacity across all of them based on the configuration we designed for that given profile. Yeah. Yeah. For your case, you would have C node and D nodes, right? You will respond with? Yeah. So we'll size it based on the C nodes. And then effectively what you'll see is that, you know, rows 16 through 19 on read throughput per gigabyte, write throughput, like those all be the same because we've sized it based on the performance requirement of two IOPS per gig. Yes. Yeah. And D nodes have no performance characters like A1, D nodes? You can hit it like, not in this range, but you can hit a point where the D node then becomes the bottleneck and you'd need to have more D nodes to over to, like, if you get to like, say, 244 terabyte drives and your D nodes, now we're talking about, you know, tens of petabytes of capacity, then the drive capacity itself, the throughput of the DPU on the D node can end up being the bottleneck. But we're nowhere near that with these configurations. So, the D nodes will be able to handle anything the C nodes can put at them. Okay. All right. Sounds good. So that's essentially not a consideration at least in these areas then. Yeah. Okay. And how do you have different, like do you have anything different for object or? I mean the way you use this. Go ahead. Because we use the same environment or architecture. We, in our benchmarking, do object UPS primarily for small and large object sizes. So that's essentially 256 KB and 256 MB. Scroll a little bit more to the right, David, please. Yeah. So what's your read-throughput, your support, what's write-throughput, your support, intentionally left blank because we didn't have anything off from that So we just left it blank for this one and the same configurations. I mean, I would still like in an object world, I would be benchmarking time to first byte latency, but yeah, it's essentially the same story. You find that, you know, latency are slightly higher on object than file protocols, but it's a protocol definition issue. I mean, how object obviously has the HTTP overhead layer, but otherwise you'll see very similar performance characteristics on throughput. Okay. All right. So yeah, those would be something when we get to the bucket level and right now we're at a unit or system level. So we're trying to get some of the higher numbers at the spec level for the system. So there is a little bit of additional commentary in rows 46 and 54. David, if you could please scroll to the left. Oh yeah, we put it there, yes. Yeah, maybe do you want to briefly touch it? Go ahead, go ahead. Yeah, yeah. So basically what we're really saying here is 5050 get for the standard storage class and in the so-called nearline or archive class of storage, we envision that as only 10% read and 90% write for the obvious reason of this scenario. And the other thing we obviously would want to do, of course, is make sure that the workload exercises the real end-to-end system capabilities, not necessarily limited to the the cache, that's why you see that working size greater than or equal to 20% of usable capacity suggestion, right? That makes sure that we're actually measuring the end-to-end system performance for that specific scenario. Let's see, anything else? No, I think. Yeah, and disclosing the throughput would be helpful. I think we're calling that out. Throughput and latency, I guess. Yeah. And feel free to ping us, Jason, if you have follow-up questions. But feel free to, of course, also ask now while Kamal and I are here to answer any questions. Yeah, I mean, one comment, obviously, that we're all Flash. So there is no standard in Archive Nearline. So there's one number for object. There's not different media for the different tiers there. Sounds good. Yeah. Sorry, remind me again. Maybe my memory is a little fuzzy. is there no QLC versus TLC differentiation here, right? Yeah, you'll see the same performance across it. Yeah, there's the staging on SLC, but everything is flash beyond that. Okay, sounds good. So maybe one of these would apply to you guys. That's cool, right? Yeah, but still the workload difference is one is two, there were two workloads as Kamal was calling out in columns F and G, which is 256 kilobytes and 256 Yeah, sequential versus random IO basically. Yeah. Yeah. Thanks. And all our units and IE, they're like DB bytes versus terabytes. So we standardize on those. So in your numbers, make sure we are talking about those days. And then in terms of like clients that you typically use when you're doing your own performance testing and benchmarking, do you have a standard playbook that you use like Elbencho, FIO? What are you using? So FIO is very common. We use it pretty extensively in our use, but there's a bunch of other solution and workload level solutions as well. We are doing the performance testing. Depends on which dimension you're going after, right? So through code versus... But FIO is a starting point. So if that's what you're going to use for this, that's perfect. Yeah, I mean, we obviously already have a bunch of articles published on how we think about benchmarking and performance analysis testing. So I mean, the underlying question is really how we align methodologies so that we're comparing apples to apples with each other. And if you have any guidance on how you'll do the analysis or how you'd want us to do it to make sure that we're validating things in the way you'd want to see it. I think I would start with the FIO test unless Malika Jinthi and otherwise. A lot of these to compare. Yeah, sorry, I was a little distracted. What was the question? Is there any particular performance benchmarking methodology you would say they should follow? I think we chatted about FIO, right, Kama? Yeah. You remember, right? Yeah, I think that ought to work, Jason, unless you have a different option in mind. Yeah, I mean, obviously, file versus block versus object have different methodologies and you know we have kind of our standard way that we do it also you know what I'll take is an action after this is also do our benchmarking process that we use yeah please share with us which tool you use for which benchmarking while you share your results characterization that would be great yeah and if you see column b and c we like we probably want to look at both numbers like I know some of the We want to look at both usable capacity. So we do like to like comparison. What is the usable without any D2 compression and what is the usable, expected usable with D2 and compression? And some places, if I remember you had two to one or three to one, you could go in usable capacity, right? If you have any efficiency enable. Yeah. I mean, obviously that depends on the characteristics of the data as we previously discussed. So encrypted data versus text files, you're going to get wildly different numbers. But yeah, so is there like a data set kind you're wanting a number against? No, this is more like Golem C is I would have put estimated or expected in if these were available in the in the general scenario because again, as he said, there's no particular use case we're building this for. So this could be your generic to say we have in our configuration 2 is 2 1 or 3 is 2 1 is standard for 80/20 read write, random read write, most most use cases across different types of work. because there's not going to be one workload. There's going to be VM based, Kubernetes based, databases, all of those mixed running. Think of this as a cloud solution, right? So this is our average. Let me make a clarification. So when we would do a performance test, we would use completely random bytes so that no dedupe applies. So that we're ensuring that we're not getting fake results because of deduplication in the system. So that's how we would approach performance testing methodology. That's unrelated to usable capacity, because usable capacity would come from real-world data analysis. So what are you looking for as a number in this column? So usable capacity without enabling any deduped compression. Is there a way to disable deduped compression on your site? There are some configurations you can disable it. Like, I don't want to go over to for it. If you have the data encrypted, it automatically does that work for you. But if not, ideally you should disable it in your system. system to say I don't want any. So that would be a column B. Column C would be either you do the test twice with enable and disable or you say estimated in all our cases is usually in this range. Okay, so just your what you're asking for then is the performance overhead of enabling it versus disabling it in a workload-apply file where even if it's enabled we know the result is it's not going to have any impact because it's all random bytes. It's both impact as well as the capacity gain, right? So both So we would like to understand what does it does to usable capacity in column C and that doesn't have to measure it can be your average in average scenarios column C would be you would be able to get 400 terabytes out of the system. Column B is the one measure where we say okay usable is this number for this configuration without any efficiency. Yeah okay again though to come up with a ratio we would want to know. - That's why I'm saying it's an average. I don't want to say it won't be an exact number for column C. - Like across all of our customers who are storing unencrypted data or something like that? - Yes. - Yeah. I mean, okay. We can maybe call out an example workload scenario and give you like a rough number of some enterprise customer. - Mixed workload. It won't be one specific workload, but we mix. Think about somebody running a private cloud, right? Like what would be their use case for you? - Okay. Okay. And raw capacity is the actual amount of disks you have in there. So it also gives you idea of how much overhead you have in the system. Like you might put grades or other things. Your raw capacity might be much higher than what you're using. Pre-erasure coded. Yeah. Okay. I think it's clear. And then we have, what was the tab two for? that part of the discussion with Jeff and you guys are having it. Do we need to go into that now? I don't know where we landed by the end. Can you actually go to that tab David? I don't remember what Jeff finally left in there. Okay let's look at this. Oh yeah this is really about additional characterization that Jeff really wanted to see for each of the configurations in the cited, in the previous step, right? For example, config details to small, medium, large, and large column C is really about that is key to the same configuration in the previous step. And what is the latency versus IAPS curves for each of these workloads, right? These are the specific workloads that Jeff wanted for those small, medium, large configurations. Yeah. So let me just read this. Yeah. Okay. Yeah. This makes sense. Okay. Thank you. Let's see. Is there anything else? We're going to share a copy of this with the last, right? David you're gonna send it after the meeting okay thank you anything else David that you wanted to cover today I haven't digested the latest note that you're sent to us so I'll be reviewing that today if I have any questions comments I'll respond back in writing but before we drop from the call Malikarjan and Kamal are you clear on the whole topic of whether or not the self encrypting drives are part of the and if not, then how is that topic addressed and whether or not that's acceptable for us? Yeah, I think we are clear in terms of what VAST is doing. I'll play it back. Jason, please correct me if I'm wrong. I think right now self-encrypting drives, either FIPSC enabled or otherwise, are not used in the VAST system, number one. But that said, you guys do have of double encryption of data possible in the overall solution that you're proposing. But both encryptions are being done in the software stack and together, or at least one of them, this is part of our FIBS compatible, FIBS 140-2 compliant. That's right. That's right. And one of the other kind of related topics from our last call is where we can set keys and at what hierarchy within the file system. So what I can also share there is you can set the key at a path. And so a path can translate to a folder or a bucket. And then you can set a key at the tenant level. And those two layers allow that sort of hierarchical configuration. And then the last thing is with our most recent release in 5.4, we support per object keys on the S3 API. - It's the client-side API. - Yeah. - Okay. Sorry, remind me, pardon me if I'm repeating the same question from the last meeting, Jason. Regardless of the granularity the encryption is used at, those are comparable, they can integrate with an external key manager, right? - That's right. And there's a set of ones that we've validated and tested with. We can get you the list of all the ones that are already certified. - Okay. Kamal, they're not to work for the end-to-end scenario, right? The ability to set the encryption at different granularities, different scopes is just called out. In conjunction with the key manager. The key CUJ is like if I am a customer, I create a disk or a PV, I should be able to do cryptoshred of that disk. Let's say I want to go ahead and say I have a key in my key management system. Once I want to delete this key and now this data is unusable by GDC or any of the systems where it's hosted right because we need to provide that level of customer managing encryption key and crypto sharing use cases to our customers so I probably don't know understand the path like your terminal data big path although maybe those the same thing but we good to understand we can provide you these are the CUJs we want our customers to be able to do and are they possible in your environment or not is probably the Yeah, I mean, obviously the, the statement is slightly different between block file and object. Um, object, it's at a bucket level effectively or object. We have also sort of had that object capability we talked about, uh, on file and it's, it's a path and then at the tenant and then a block it's at, you know, the volume or the tenant. Perfect. So, and then at object level, the two scenarios you provide is I have a bucket level key. So I can decide as a customer to say all my objects in this bucket, I can use this key to encrypt all my objects and if I delete that key that bucket is unusable or within the bucket I can choose let's say project A is using key A for their objects project B they're sharing a bucket but I can do and this is not client side encryption which this is not no action required for the client here other than just choosing the key they want to but as a bucket we provide customers and keys like hey this bucket comes with n number of keys. One is the default and the others you can use for different use cases. So if that project goes away, they can delete that key and anywhere their object lives, it's unusable. So the way you should think about, because it's multi-protocol, right? I can access the same data from S3 or from NFS, is that a bucket is analogous to a folder at the root level of the file share. And so we're setting a path-based key at that folder aka bucket has a customer managed key in the key management system and if that key is revoked from the KMS then you know you can't read that data anymore that's effectively made that data not viewable by the vast system or exposable back to the customer so that's how key revocation works is at that folder slash bucket level yeah so you have bucket level but when you said object So object level is back to this notion of in the S3 API there's a per object key that you set at put time. So when the object is put to the system. Customer action where they have to configure this in there in the customer client side application. Well yeah I mean the S3 SDK can manage this for you right. So if you're using Amazon's SDK and all that stuff. We started with that option and some customers like we really like if you go high level of classifications. You really need that even like some customers okay, some customers like it system has to provide object level. So without them sitting on their side, they should be able to choose from the bucket key. Let's say in your path, if you were to give them 50 keys and they will be able to see, okay, instead of having a default for the bucket, I should be able to use A, B and C for my use case. So there's, are you talking about like expressing a policy at the bucket level that says this sub path has a specific set of keys. So actually, I don't even think the S3 supports that, where you can say this set of objects within this bucket have a specific server side key. It's the same thing you're doing, right? Like you just have logic on your side. Like today, let's say you have a default key for the bucket. Customer doesn't need to encrypt anything. That key is used to encrypt all your stuff. Now you have 10 keys and customer has a way to say that, hey, for my object one, use key one, my object two, you use key two. It's not one default bucket key, but you have multiple keys. To clarify, you're saying that the client would pass the key identifier when it's writing or reading the object, not the key itself. Exactly, exactly. So you would have certain of those created for upfront and they would say for this one, I want to use instead of default, I want to use key A, key B, key C or whatever. And that should be something. Let me follow up on that. I'm not sure. I actually am not sure. I'm not as familiar with how S3 does this from a service perspective either, so let me look at the API on that. Okay. So, Kamil, how critical is the scenario of two projects sharing the same bucket with different encryption keys and only one of the projects deciding to cryptoshred the data while the other project is still retaining its data, all shared I think that sounds like the gap that we're beginning to identify here. So the project has a very specific terminal on GDC, not confusing with the R-Namesense project, you're just saying use cases, like different use cases sharing the same bucket and you remove one use case and the other use case still there. So the challenge is it's a small subset of folks which require that, but the problem is you don't want two different solutions for two different use cases. I have a customer here, I have a customer here, I'm not even wanting to have two solutions for two customers. Unless there is the price variances and there's other variances which are too high to say okay, knock out a special solution just for these use cases. No, clear enough. I'm trying to understand, right? So you're thinking, Kamal, that one of the projects would want a crypto shared in this scenario, meaning only some objects need to be crypto shared while the others still stay around, correct? That is one example of multiple keys but there are other examples. I cannot share all the details of how they use it. That's one example where they have to say I need to have a way to deterministically define that certain objects in a bucket are using key A versus key B versus key C and multiple keys there is. Okay makes sense. Cool. All right, anything else? So I think we should do a security deep dive because... So Jason on the SEDs, are you not finding this as a requirement from your public sector customers where they require hardware level encryption? Yeah, that's right. I talked to our federal team about it, our SME on encryption, and she shared that they've been able to resolve this by doing dual-ear encryption within software and have met all of the FIPS requirements that our customers have asked for using that approach. We had done it like I said we did the analysis on trying to support it at a hardware level but just because of the way we do HA it is very difficult to do the key management on the SEDs. Okay is she okay to... Is FIPS 140-2 certified? Yeah N3 yeah. Is there a way for your SME to... Yeah I don't know if we have a very common term you say one pager like define a one pager like this is how how we do it and this is the level of validation or certification we'll be able to get with this configuration. Yeah and if we do a follow-up deep dive on encryption I'll bring her and we can just have direct conversation. Yeah which anyway was the plan like once we get some sort of normalizing and we said okay here's the top two top three what to go deep dive into that would be the second step that we go into capabilities features and other things. Security being one of them. Sorry David go ahead. That's okay. Jason I don't know I know this is a question for you or for Lior, but I wanted to bring it up just for my own clarification. Ideally, if we were to engage with you and move forward, VAST would be the only party that would be responsible for the entire solution, so hardware, software, and the associated support services in the field, even prior to being deployed in the field. How will it work if we want to move forward with you in terms of you being a third party hardware provider independently of you because you're only providing your portion of the solution or can we have you provide the full turnkey solution? Both are options. So we do effectively resell hardware when customers ask us to or obviously we have a set of vendors where customers go direct on the hardware so we can consider both models depending on your preference. One call out is just if you're worried about it from like a legal entity perspective. Our federal business is a separate legal entity. So there's vast data which is the parent corporation and then there's a vast federal entity which is where all of the folks that work with our federal government sit. So there's potential nuance around that but they're obviously you know sister companies from how we manage things perspective. Right. Yeah our system integrator would most likely be procuring from you and most likely They're probably procuring from the pairing company versus the federal entity that you mentioned. So I don't think that's going to be problematic. Is there any like obvious call outs of why customers would want you to manage all versus them managing it independently? Like if you were going to sell me on one way or the other, do you have any insights to that right now? I mean, there's a variety of reasons why you might choose hardware from one of our partners like Cisco or Dell or HPE that may come into play. from an enterprise customer's lens in terms of existing relationships for the rest of their infrastructure. Obviously, they each have their own hardware profiles in terms of capabilities, performance, and cost that may factor into your decision matrix. But for other customers who are just looking for an optimized SKU, then we have pre-qualified hardware that we can bring. Okay. And then in the matrix that we were showing earlier, in terms of the small, medium, large, and extra are you using a mix of partners to potentially satisfy those requirements or do you use one partner and mindset? Yeah so the proposal we put together is like a vast designed SKU so that's what we'll respond with from the performance perspective because that gives us what we think is the best performance for capacity performance per dollar ratio and so that's kind of our like defined hardware versus the Cisco HPE kind of scenarios so we'll come with our for our best hardware. Your best hardware meaning you're selecting... The stuff that we white label. Like the front of it has a VAS logo on it. It's our white labeled hardware. Okay, got it. Okay. When you say it's your VAS labeled hardware, you're using your own ODM-CMs to do that? Or you're actually partnering with one of the major suspects you mentioned? Actually, a lot of it comes from Supermicro. Okay. Supermicro. Got it. And then in that matrix that we were showing, there was a column there for pricing and it was supposed to be all inclusive of hardware, software, and support. It won't be difficult for you to provide that blend, right? Hardware plus software plus support. Typically software and support are bundled together, so we'll have to determine how we would want to position that if you want them isolated. Well, I mean, for this immediate exercise, it can be all bundled. I just want to make sure that all three components are included. Yes, all three included and we easily differentiate hardware and software. Yeah, yeah, understood. Okay. And then in the notes anywhere there, whether it's your specific hardware, your labeled hardware or some third party hardware, you can just clarify that too. That would be helpful. Okay. Okay. Yeah. And if I have any other follow up questions based on the feedback that you and Laura already provided in writing, then I'll respond back to that same email thread. Okay. And it looks like you have a question. No, no, I didn't have a question. I just wanted to say, I need to talk for another meeting, but if you have any If you think anything, reach out for the sheet and we will definitely love to answer and get your feedback too. Thanks. All right. Thanks everyone. Same here. Thank you, Jason. Yep. Thanks folks. Talk soon. All right. Bye. Bye.