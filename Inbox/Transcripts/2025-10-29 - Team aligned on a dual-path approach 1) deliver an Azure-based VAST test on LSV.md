---
entities:
  customers:
  - '[[Microsoft]]'
type: transcript
source_type: unknown
date: '2025-10-29'
---

# 1:1 â€” Unknown â€” 2025-10-29

> [!info] Purpose
> Build trust, surface and remove blockers, align on priorities, coach & grow.

## Summary

Team aligned on a dual-path approach: 1) deliver an Azure-based VAST test on LSV4 by the first week of December; 2) pursue a bare-metal/dev system in a Microsoft data center via Anandâ€™s org. VM availability and regions were discussed (LS96 vs LS192), RDMA timelines need confirmation, and Slingshot integration is a separate track. Procurement paths for OEM hardware and software (non-marketplace for internal Microsoft) must be defined. Lifter/Azure Native integration will expose full VAST features via APIs/Portal/CLI, with an engineering workshop to kick off.

## Key facts learned

- UK Met Gen1: HPE Cray + GPFS/ClusterStore in dedicated DCs; several Azure services already in use.
- Gen2 must be UK-based; likely dedicated compute and storage, possibly L-series or bare metal.
- LS96 has 40 Gbps; LS192 offers higher bandwidth but limited regional availability.
- VAST Azure test system commitment moved to first week of December; demo planned around 2025-11-17 (SC/Ignite).
- VM-to-VM RDMA for LSV4/LSV5 targeted; timelines need verification; GPU-to-VAST RDMA also desired.
- EGAL/LSV5: AMP VM Q1 test/Q2 prod; potential 800 Gbps, ~300 TB NVMe SKU; mid-Nov decision expected.
- Marketplace cannot be used for internal Microsoft subscriptions; alternate software procurement needed.
- Slingshot: no kernel RDMA path for VAST; TCP/IP/Ethernet viable; direct connect to Slingshot requires HPE approval.
- Performance targets include up to 1 TB/s symmetric IO and small-file all-flash targets (e.g., 25 GB/s bidirectional on ~1 PB).
- Current GPFS issues include lockups/outages; metadata create rate may be tight versus capacity.

## Outcomes

- Parallel plan confirmed: LSV4 Azure test and Anand-led bare-metal track proceed in tandem.
- Testing may begin on LS96 (40 Gbps) if LS192 is not available in required regions.
- Primary network focus is Azure Ethernet; Slingshot integration treated as a separate exercise.
- VAST/Lifter plan to expose full functionality through Azure Native APIs/Portal/CLI.
- Engineering and business tracks will split once testing starts; initial workshop to be scheduled.

## Decisions

- Proceed with Azure LSV4-based VAST test in early December and a bare-metal/dev system with Anandâ€™s org.
- Accept LS96 for initial benchmarking if LS192 is unavailable in target regions.
- Defer Slingshot specifics from the main development path; handle as a separate track.
- Microsoft to prefer sourcing OEM hardware directly; HPE likely path of least resistance (final shortlist pending).
- Internal Microsoft cannot use Marketplace for software; pursue non-marketplace procurement route.

## Action items (for Unknown)

- [x] Review and approve the shared VAST configuration; expedite shipment/installation for Anand DC path. @Leo â« âœ… 2025-11-08
- [x] Introduce Nico to Roni (data plane) to align VM, region, and availability for the December test. @Leo â« âœ… 2025-11-08
- [x] Select starting VM for testing (LS96 vs LS192) based on regional availability and coordinate specifics. @Nico â« âœ… 2025-11-08
- [x] Deliver an 8-node LSV4 VAST test system in Azure for UK Met by the first week of December. @VAST Engineering ðŸ”º ðŸ“… 2025-12-05 âœ… 2025-11-08
- [x] Prepare Azure demo for Supercomputing/Ignite. @VAST Engineering â« ðŸ“… 2025-11-17 âœ… 2025-11-08
- [x] Ping Sarah to confirm mid-November commitment on EGAL platform update. @Tiff â« ðŸ“… 2025-11-15 âœ… 2025-11-08
- [x] Verify RDMA availability and timelines (VM-to-VM and GPU-to-VAST), including MTU and VNet constraints, with PM. @Nico ðŸ”¼ ðŸ“… 2025-11-15 âœ… 2025-11-08
- [x] Schedule engineering workshop on Lifter/Polaris integration with Nico, Trevor, and Travis. @Jonsi Stemmelsson ðŸ”¼ âœ… 2025-11-08
- [x] Set up a separate engineering track for OEM/bare-metal automation (hardware/switch APIs). @James ðŸ”¼ âœ… 2025-11-08
- [x] Discuss with Anand who will purchase/own the bare-metal POC system; target an answer around Supercomputing. @Mike â« ðŸ“… 2025-11-20 âœ… 2025-11-08
- [x] Draft bill of materials for the bare-metal dev/POC system and facility requirements. @James ðŸ”¼ âœ… 2025-11-08
- [x] Coordinate with Ed Kim on OEM sourcing and transacting route; align OEM shortlist. @Jason Vallery ðŸ”¼ âœ… 2025-11-08
- [x] Provide top 2â€“3 preferred OEMs for the bare-metal path (e.g., HPE first; others TBD). @VAST ðŸ”¼ âœ… 2025-11-08
- [x] Define a non-marketplace procurement path for VAST software for internal Microsoft subscriptions. @Microsoft Procurement ðŸ”¼ âœ… 2025-11-08
- [x] Explore compute options to mitigate NVMe ephemerality (dedicated hosts or special SKU) for UK Met. @Azure Compute team ðŸ”½ âœ… 2025-11-08
- [x] Draft a GPFS swap-out design using VAST (TCP/IP/Ethernet) with a business case tied to outage penalties. @Trevor ðŸ”½ âœ… 2025-11-08

## Follow-ups

- [x] Confirm LSV4 LS192 availability in target regions (e.g., East US 2) and plan fallback to LS96 as needed. @Nico â« ðŸ“… 2025-11-05 âœ… 2025-11-08
- [x] Confirm EGAL platform specs (e.g., 800 Gbps, ~300 TB NVMe) and timeline; ensure placement in UK Met test regions. @Eagle â« ðŸ“… 2025-11-15 âœ… 2025-11-08
- [x] Verify if Ignite announcements include east-west VM-to-VM RDMA for LSV4 and capture preview/GA dates. @Nico ðŸ”¼ ðŸ“… 2025-11-18 âœ… 2025-11-08
- [x] Clarify where the bare-metal POC will reside (PG vs customer environment) and the access model. @Mike ðŸ”¼ âœ… 2025-11-08
- [x] Align with customer on symmetric IO requirements and potential adjustments. @Mike ðŸ”½ âœ… 2025-11-08
- [x] Confirm space and power availability for potential POC systems if Microsoft purchases them. @Anand ðŸ”¼ âœ… 2025-11-08

## Risks

- Schedule slip on LSV4 full-node availability and regional constraints could delay testing.
- RDMA (east-west VM-to-VM, GPU-to-VAST) timelines uncertain; performance may be impacted.
- Marketplace restriction for internal Microsoft complicates software procurement.
- Slingshot direct connect requires approvals; kernel-space RDMA not available for VAST.
- GPFS instability at customer increases urgency; symmetric IO and metadata targets are challenging.
- Power/space constraints for potential POC systems; procurement lead times could impact schedule.
- Ephemeral NVMe behavior on L-series may limit advanced options unless dedicated hosts/special SKUs are used.

## Open questions

- Which regions will have sufficient LS192 capacity for benchmarking, or should LS96 be used initially?
- What are the definitive timelines for VM-to-VM and GPU-to-VAST RDMA on LSV4/LSV5?
- Will Azure Compute deliver a special SKU (e.g., persistent NVMe) or dedicated hosts for UK Met?
- Who will purchase and own the bare-metal POC system, and when will that be decided?
- Which OEMs will be used for the bare-metal path (final shortlist and sourcing route)?
- What is the approved non-marketplace mechanism to procure VAST software for internal Microsoft use?
- Will customer symmetric IO and metadata requirements be relaxed or updated?
- Is Slingshot direct connect approvable for this deployment if pursued, and under what constraints?

> Next meeting (if any): (none)

---

<!-- ai:transcript:start -->

## Transcript (auto)

```text
[00:00:00.04]   RemoteWe're just doing introductions and we said well, Jonsi is accepted and you've just joined. Do you want to say hello? Yeah, hey, everybody. I'm Jonsi Stemmelsson. I'm the GM of cloud for Vast. In my previous life, I was the global CTO of UP at NetApp and GM of Cloud. cloud storage to begin with and built ANF as an example with of course my co-founder at Green Cloud, Eiki, who is the VP of engineering - cloud engineering, sorry. Excellent. Okay, all right, well let's let's crack on with Leo's update, and if anybody else joins in, we can do that. I can share the notes. If Nico comes in, I can share the notes with him after that. Leo, over to you. - Okay, so where we stand right now is, I've reached out to Anand, and Mike, you saw that, because you're on the thread, and that was for me to prepare a plan B that might be plan A. trying to figure out a way to give this project the best success of getting to production, and after we met with Anand in Redmond a few, maybe six weeks ago, he suggested a path of building vast hardware in his data center and enabling that access to UKMET, and it's not only UKMET. I think you see your... it in the email. We have a few other projects and some of them are actually bigger than UKMet, but priority and timeline, you guys are the first priority on timeline. So it seems that there was a conversation earlier this week and there is a configuration that James, you're supposed to be sharing with me today or tomorrow? That's in your inbox. Okay, already in my inbox. I will up on that spread with the configuration and internally I will follow up with the right people to to get it approved and to get the most expedited timeline of us shipping it, installing it, deploying it and making it ready for my QA team. So that's one path and again I'm the biggest cheerleader for Eagle and Compute to provide the right VMs. I just I don't believe in taking risks. Any questions and comment on that process? Where is this going to go? Anand will want to get the requirements, I think it's a more Microsoft internal, so Mike can ask. I don't know which data center he has and we need to figure it out. But at least now we have a configuration that will enable me to close the loop on that external email and also follow up internally and ask for this algo to be available

[00:02:49.07]  Jason ValleryASAP. Can I ask a bit of a naive question of the the UKMAT team? You know, in this phase two plan, is this an extension of an existing Azure region? or will this be an isolated environment for you came in? Meaning, do we have all the Azure primitives in this environment, all this Azure services in this environment? Is that the plan?

[00:03:09.58]   Remote- That's a complicated question. So currently we're in dedicated data centers for some components of what we call. generation one. Generation one is what is mostly in production today. So the first phase is live running. It's replaced the on-prem. There's a second phase gonna go live in March next year. So that's the already I viewed as the legacy environment, which is HP, Cray, GPFS, cluster store, everything slingshot. But around there's lots of other Azure services like the archive that Nico built is in other UK SAS data centers, there's a post-processing cluster, there's a VDI environment, those all running on normal Azure services at the moment. Now for the Gen 2, we probably will need to look at swapping some parts of that existing we go. I've had some questions with James around potentially replacing some of the problematic pieces of that puzzle, like GPFS with VAST potentially, if we can inject it into Slingshot. The other, so for the newer environment, that's to be determined. It has to be in the UK. We're probably, you know, we have reservations. specific data centers, which we're busy working with the customer on sizing the amount of compute that they're going to need. We know how big the storage is that goes along with that, but there will more than likely be dedicated facilities anyway. So whether we do it with L-series or we do it with bare metal, it'll end up on a dedicated customer network. Possibly because of the scale, it'll end up as dedicated stamps of compute and storage just for this customer.

[00:04:59.59]  Jason ValleryYeah, I guess the reason why I was asking is that they're leading around the idea that, you know, Vaston optimized hardware versus the L-Series is a power savings for you, data center rack space.

[00:05:10.59]   RemoteIt's power and we also have a fixed budget to deliver the whole program. I'm also just curious as to whether we can get to the price targets with our own SKU versus procuring something third-party and doing it bare metal. Also, just the technical design, I think we can just get a lot closer to the metal with the bare metal solution. So like the preferred option would be just consume somebody else's SKU. But if it's not hitting the performance at the price point, then building it bare metal is tried and proven path that we've used before.

[00:05:50.85]  Jason ValleryAnd I guess, just to layer in my motivations here, is to leverage this as an opportunity combined with some of the other ones that Leora alluded to that we're hearing about. to build this as a repeatable motion that we can do with Azure. So, you know, we're looking at this transparently with MAI folks and others, and I think that's ultimately what our joint customer is the best price for performance versus trying to run this on general purpose compute or L-series compute.

[00:06:17.79]   Remote- Yeah, sounds good. - I mean, ultimately, I mean, the sort of API spec for the bare metal and being able to automate deployment that we would have a similar process as we would do something with a hypervisor layer on top of it. I mean that's something that we can investigate to your point Jason instead of bringing in our own ODM hardware. if that's the case where we can have access to sort of build our own SKU on top of your infrastructure. I mean, ultimately, BEST is a pure software play. So, I mean, if that is something that actually would align better with the strategy going forward. We are happy to jump and throw engineers at that. So I think we've got kind of both options. That's going to be up to Anand's org, I think. But basically, we have our own supply chain to go out and procure through OEMs that are already supported by VaST. Or we could have you guys source and integrate that hardware with the software and then ship it as a pre-configured solution. We've done that with other vendors before. So I think both options are there. Strategically, it's probably better for Microsoft to source its own hardware. But ultimately, we want to make sure that you're super comfortable. in control of that configuration to meet your standard as well. Yeah so even today our on-prem businesses customers are sourcing hardware from our hardware suppliers so we can do the same here or we will do the same here, BAST only sells software and we integrate it at the customer so that really works well with that. That takes us to the other side of plan A over Not sure what's plan A and plan B anymore, but we have a clear plan now for starting this engagement with vast hardware just to start testing and we have a bill of material, and the other side is, so two level of updates, one is Nico shared earlier with me that the full VM for LSV4 isn't available yet. and it will only be available in about two weeks. So just a reminder for the team. You want to say something about that, Nico? >> Yeah. I just want to say to confirm, it's slipped for two weeks at least. Yeah. Seems that they have some issues. So I suggest that we start with anything that we can currently use. >> It's 96. >> Yeah. Well, whatever it is. Yes, I understand that you cannot do the tuning, but at least. we can understand the dependencies and and and start working on it so that we shorten it as much as possible. Okay so I want to share an update on our side as well so so first off we are committed to giving UKMet and you guys are again first priority on the Microsoft Azure side a system for testing. say system is, of course, the ability to install in your tenant on an 8-node LSV4 cluster fast and start doing your benchmarks, and that's in parallel to what we're doing with Anand, so we're committed to that. I was hoping that we get that system to be ready for the week of November 17, last time we spoke. The reality is that that system will be ready for the first week of December. So, our engineering pushed me to that date. They know that you guys are going to look for performance, at least a certain level of performance, and they want a few more weeks to get there. So, that's the committed date. On November 17, we will have a demo. the demo at supercomputing and maybe at Ignite, we will have a demo. So, the software will run on Azure, but setting expectations with your extended team, Mike, the first week of December, I apologize, but this is a date coming down from Shahar, the head of engineering. So, I think it's... Is that targeted to run only on the LS192, so the... one Nico was talking about. So it will so I was since we're delayed here I was hoping that we get access to the full VM which means 200 gig network access and it seems that now it's going to be delayed by two weeks we're going to aim on the 100 gig VM the 96 you just mentioned if and when we get access within the next two weeks to the new VM we'll figure out if we can know, switch and enable the 1st of December on the full VM. But for that, we need to get the full VM access first. So that's-- - Yeah, what would just one, I don't know if this is an issue or not, but when we were looking where the LS is more for NICO, where we're looking for the LS 192, it wasn't in regions where we have the HCI. capacity where we're running the benchmarks so I don't know what the deployment plan is. I checked on that they told me that there is no reason for them to basically change the regions because he just deploying our configuration and the software so I didn't tell them in which regions we want them. Sara said that she thinks it will be okay so I think you said or in Korea or something. - Yeah, East, we've actually moved, yeah, just based on stuff with Rob Walsh, we've moved one of the clusters over to East US-2 now, just 'cause there's more H-series capacity there. So let's have a sync after this, and- - Yeah, I do know that. - And guys, so we do, we do. I'm going to connect Niko to Roni, the guy who's doing the engineering for the data plane. So they talk about the VM, and they talk about the region, and they talk about the availability, and we don't get everything ready for the first week of December in the wrong region with the wrong VM. So Niko, expect an email. I just want you to cover that with him. That's not the channel, but I just want to see that you guys are on the same page Sorry James just just to close on the previous topic one thing that I wanted to cover the LS 96 Doesn't have 100 gigabit me. It has 40 gigabit just that you are aware because you mentioned a hundred But it's not it's just 40 gigabits And that's the reason why we are pushing for the full node, because the full node is 200. - But so what I wanted to get to is if, worst case, LS192 is not in the right region right now, LS96, even if it is just 40 gig, is probably enough for what we're going to be doing for this benchmarking, as long as we've got like six or eight of them. Yeah, so I just want to make sure that. but that is included. We're not going to just skip that one and go to the 192. I think, I mean, that's going to be the best one. But if you can support the other one, it gives us more flexibility to test in different regions 'cause there's more of it. - So to be clear, you guys right now are in a position to determine which VM we're going to start with 'cause you're the first project we're aiming for. general availability, you don't see is still February for Azure, right? We still have time for that. So, you know, you're getting early access, you're going to be I'm seeing it as a design partner, and that's why I said Nico talk to to Ronnie on emails, and if it's the 96, let's go with the 96. If it's the 192, the 192, let's go with the the right VM that works best for you guys. Okay? - Okay. - Now, expanding that topic to LSV5 for a second, and I'm just sharing feedback. This is really Microsoft to Microsoft communication, so I'm trying not to be noisy, but I'm sharing the conversation I had with Eagle. So, we met with Eagle for this. and I had a few interactions with him this week. So he's extremely committed, right? Not saying that that's the right path to go, I'm just saying extremely committed, and he did share the same news that we got about the AMP version of LSV5 that will come Q1 of next year for testing and Q2 for production, which is 200 gig and. 138 terabyte NVMe, and I told him that Nico told me that you're expecting an answer by mid-November if he's going to do something better for UKMet. So his feedback was, I already told him, again, that's his words, you follow up internally, that I will build an 800 gig VM with 300 terabyte of capacity. So, again, and I think Eagle is really trying to figure out the path to be the best partner he can, and you know, I will let you guys close the loop with him because right now it's not on the critical path of our work with you. The critical path is the LSV4s and it's an end project, right? It's not LSV5, at least not right now. So are we still on for a middle of November confirmation whether or not we're going to get an EGAL platform that's suitable? This is what they committed to. I don't know, but they committed it by middle of November. they will share, first if they're able to deliver that based on where they are, but they are also supposed to deliver the timeline so that we can actually match that to the MAT office timeline, and therefore, within the right region where you are doing the UK MAT office testing? Yeah, that part we will fix because that skill still doesn't exist. So that will be part of our general capacity planning. But I just want to make sure that we understand the timeline, which will be crucial for us to understand if we can use it or not. Another interesting data point that you shared, that was kind of a surprise to me, is going to announce, so you guys are going to announce at Ignite. but all of those VMs are also RDMA ready, hardware RDMA ready. So it seems that the implementation of RDMA within the FPGA in those VMs is done, and that will apply also to LSV4, not only LSV5. - You mean VM to VM RDMA, right? Yeah, that was supposed to happen in November. but I think there was some issue with it, but yeah. - He said it's going to be announced. Look, we are client to client, because at the end of the day, we're building our storage cluster. If we're doing it with LSV4 or LSV5, we need RDMA between different LSV4s to establish the cluster and to have lower latency and better performance. So for us, it's interesting news. Assaf, our chief architect, was at that meeting. EGLE introduced him to someone in the team that has more data, and we're now analyzing what does it mean for us, but again, RDMA for VAST is good news, client to client. So... Yeah, I'll follow up with the PM later. I know that the last date that I knew was November 2025 for a private review. I'll check if that's... you guys still are the same, right? - Okay, sounds good.

[00:17:44.20]  Jason Vallery- And just to clarify, I think there's actually two RDMA topics we should investigate. There's peer-to-peer within the VAST cluster, so L-series to L-series VM and then the same deployment, and then, you know, GPU to VAST. That's a different RDMA flow that we would also like to support. So it would be good to understand all of the permutations of what Azure is going to offer us here.

[00:18:04.91]   RemoteYeah, this is mostly like CPUs, right? What's the GPU consumption of this project? Relatively, it's small right now, but it's going to be coming in an increasing part of it. Even the customer themselves can't really project that right now. they just don't know. They've just handed us a project which is going to be the first AI workflow going into production, so they do have some small development GPU environments, but yeah, I think we should assume it's going to be a mixed CPU, GPU capability going forward. Jason, a generic comment is that the clouds... provided a lot of feedback about north-south RDMA being ready much before the east-west, so kind of the challenge of again with Microsoft and the other clouds wasn't about connecting having an RDMA connection to the GPU servers, was having RDMA connection between our nodes. Right. But I agree with you, we need to get the full picture on all for sure. It's still a VM talking to a VM, but it is a good point, I'll verify that, but it seems that it's the same case, right?

[00:19:20.31]  Jason ValleryWell, just, okay, now, I think, just to be clear, if Compute, be it GPU or CPU, is running bare metal for UKMAT versus running virtualized, that might change the... the path. I mean, I have some background knowledge on that that I'm bringing.

[00:19:33.54]   RemoteWell, if it runs bare metal, then it's our choice what are we going to run on it, right? This is basically if you are using the Azure hypervisor, can do a VM to VM RDMA. Now, is that VM to VM east-west or south-north? It shouldn't matter. But again, I will verify just It is possible that there will be some limitations around like V-net crossing and things like that, but I'll check just to cover all the corner cases. Okay, second topic for me. I'd empty you to your list there because that's another one. I am, yeah. So I've asked. Yonsi and or us that will invite Yonsi and Iki because at least last week when we spoke about it, the second topic for this call was to introduce you to what we're doing with Ignite, not with Ignite, with Lifter and our overall marketplace operation because you guys are some like we've been talking only about the data plan and vast clusters as a cluster, but not about how it integrates gates with Azure, and you guys are going to be the front runner in, you know, being the consumer of this technology. So I would love a kind of, if you have, you can start questions or maybe the guys can explain what we're building or whatever you want to do, but to cover that topic as well. I can, I can do a real quick run through of what we're building on it. on the Azure side. So there is, we're working with a LFTR team or the ISV native provider team, as we call it. We happen to have a pretty good resource provider team now that we have lots of experienced folks. that have built these research providers for Azure. But we're working with a little team right now defining some of the. Resources as an Azure speak and working on the first implementation. Anne and in parallel we are working on the marketplace. offering side of it, too. So we'll see this as kind of a staggered approach, starting with the marketplace offering, because Lifter usually takes, I think, at least six months from the start to private preview. That has been the case with the Lifter team. our end goal is to have, you know, a full SaaS. So the offerings will start out as, you know, build your cluster kind of thing through the marketplace with limited pay-to-operations. Then once with the Lifter integration, have an Azure Native API and then we'll have an Azure Native UI to do the same thing so you can you can build and create vast clusters of any size through the APIs or through the UI, the native Azure UI in Litter, and the last piece of the puzzle, like the phase... 4 is where we want to have Vaast hosted on behalf of clusters so you know similar idea of a full-blown SaaS service as you have in Azure Files and other other offerings like that like like ANF. So we supporting essentially both types of deployments, or kind of like three types of deployments, where customer account clusters are completely managed on behalf of, vast owned clusters, and that means hosted on behalf of, and then And individual sort of feature selling, just like you have with some other native first-party services, essentially like full-blown SaaS where you could just create a database or just create, you know, a global namespace, for example, without thinking about clustering. I mean, just to emphasize a little bit, like, even though it's deployed within the customer tenant, it is still fully managed, you know, the automatic deployment, the lifecycle management, the automatic upgrades, I mean, one thing that we are doing. In the clouds or in Azure is, for example, we are not giving the end customer access to the actual infrastructure. He has all access to all the bells and whistles, all the features and capabilities of VAST. But ultimately, we are taking care of all the infrastructure management for them. This has been a very clear. demand for our customers that are going into the cloud. They expect a fully managed offering where they don't have to have storage admins or somebody taking care of the actual solution. So I think that's one of the biggest difference that existing customers of AST will see in the public cloud, and in many cases, they are even asking us to bring the service delivery. platform or Project Colares as we call it into the Neo clouds as well in order to be able to have that free-flowing global namespace utilizing the data engine to migrate, replicate or cache different data sets, you know, from the Neo clouds to Azure as an example. Yes, I mean from the Met Office perspective, hosted on behalf of doesn't play because of all the security constraints, right? So basically mobile is the only option. However, the thing that I see as a challenging part is actually exposing all the features and functions that you support through the Lifter API, because basically that will depend on can that service be managed through through all the regular channels that we do, or you're not exposing anything. It has been a problem with other partners in the past before. It happened, I take ANF for example, right? You have so many things that that service can actually do, but it's not exposed, which means that customers cannot actually use it. So a lot of that will depend of, in short, everything that you build for the marketing engagement smart as mandatory, need to be exposed to the API, otherwise we have a problem. How are we going to actually manage it? So it will depend on how much you would like to bring of that to the-- - Absolutely, absolutely, yeah. - I was gonna say, I was gonna be the bad guy, but Niko did a pretty good job there. 'Cause that's what we've seen on A and F. So I know exactly what the NetApp can do and then what we see exposed was a fraction of the functionality and it was quite frustrating for us and for customers. We happen to know a little bit about that too. Very different reasons. Let's just say that we've architected things to avoid this problem. problem. But I mean ultimately the big difference is one thing is a very large monolith and the other thing is pure software defined where we are not restricted by cluster wide variables that are unable to expose due to multi-tenancy isolation and security. So that's a problem. we are not faced in vast. I can be I can be honest I'm not I'm not concerned about your side I'm concerned because basically everything that you do also needs to be exposed through like portal CLI SDK and so on that includes our work as well not just not just yours says that I'm much more concerned about that side than your side to be honest, and I'd love to continue that conversation so we can tackle those things ahead of time. Mostly what you're concerned about. Awesome. You know, when we started this process, I was kind of thinking, how are we going to do this? do this and then do you guys, you guys. get hired and come along and it's just well that's the answer okay the initial proof of concept or testing of the first of december is wasn't aimed on on this conversation now right on polaris and the project so it's uh nico mike up to your appetite to decide how much and how fast you want to engage we're looking for design partners you want to be as a engage as possible on all the details sooner than later. - Yeah, and we would absolutely love to work with you guys, Nico and Mike, we would love to-- - The sooner we engage, I think-- - Get to do like an engineering workshop and build the best potential product that we can. - Yeah, I mean, I agree. The sooner we get engaged, less risk for us. I think any any of those lower level conversations pull both Nico and Trevor and Travis can have a lot to say about this as well. He's been testing some of the other solutions. So yeah, Trevor on the call or. Yeah, he's on the phone. Okay. I only see people's enabled camera I don't know why that's. Ah, there he is. I was a little late. I was talking to our legal people, which I don't enjoy so much. Once you're on the previous call, we also discussed the fact that we want to have a party to celebrate production in next year in Iceland, right? Because Iceland in August of next year is going to have a full eclipse, right? Yeah. So we have the date. need to do is now to be ready for production before that date that's it absolutely we will gladly host you in snowy Iceland at the moment Leo you had a second thing to raise no that was the second thing that's why we don't see anything so I really again we were talking data data data and the actual performance and which is important, everything starts and ends with integration, ease of management, and this is going to be a big project. We are opening that door effectively now, and we want to collaborate and we want to make IKI and team a part of this program moving forward, not just to talk about performance and, you know, throughput and IOPS. Excellent. Glad to hear it. Okay, so that's pretty much the bulk of the actions and conversations up from last week. I was just wondering, Tiff, would it be appropriate for you to ping Sarah and just ask if she's still on for that mid-November? Just to keep her honest? That would be fantastic, thank you, and I've reached out to Claire to open up the conversation around the T's and C's, the commercials, the KPIs that I've got to hit, and John, it'd be great to have you involved in that conversation as well, to figure out how we take the flow downs back from the Met Office contract. So just speaking on that, Mike, would it be all right to give us an overview where you're able to with the end customer, you know, the kinds of capabilities that the system needs to have, and perhaps we can also touch on some of the conversations that we had with Jan at the back end of last call around some of the performance metrics that they're asking for. So nothing new to share other than what we shared already in the, the pseudo RFI that we shared. So it's a, it's a dual, the way it's designed currently it's a dual site solution so two, two data centers with four supercomputers, two in each data center for redundancy, and then we have a. a shared, or a set of shared file systems in each data center. Currently, it's all, you know, ancient sort of technology, so it's cluster store luster file systems rated up to 20, 30 petabyte kind of range doing up to about one terabyte per second. One of the most challenging requirements at the moment that we're probably going to go back and challenge is symmetric I/O, so that when they say a terabyte per second, what they actually demand is an I/O benchmark doing simultaneous 1 terabyte read and 1 terabyte write, which is the challenging part. system has a capacity and a performance metric, which is defined in the same way. So going down to an all flash file system, small files and Python and that kind of stuff, which is one petabyte and 25 gigabytes per second bidirectional. There is on that file system some extra metadata. So MD test. performance requirement, and then we're projecting out what that would look like if the amount of compute for the second generation system is bigger than generation one, so that may increase, but we've given the worst case capacity and performance requirement in that initial just to make sure we could deliver that if we have to, but I actually think it's going to be a lot smaller than that. as we go back and challenge the customer requirements. The number of supercomputers may reduce as we go forward and we may not provide force. It may actually become a bit of a simpler architecture. What we've also been discussing with Jan is trying to consolidate the requirements. We're not specking out individual file systems on individual files. you know, groups of servers or hardware clusters. We actually aggregate everything together and then use the vast virtualization to under-provision the amount of capacity and performance, but still meet all of the customer requirements. So there's some things we've already been working out with Yann. We worked out how that would look with a vast architecture running on L-series, roughly, and yeah, I can't remember how many servers, but hundreds of L-series servers, which is where the 200 gig versus 800 gig, and then trying to get the density of the NVMe drives up. So Microsoft tends to use very small NVMe drives in our SKUs currently. up to something that's more economically feasible, but we still, Nico and I still suspect that it's going to be a challenge even with the denser LSV5. Hence we started talking about the dedicated hardware, what that might look like, so we worked with James and Jan this morning just to boil that down to what would a good sort of development system. look like that we could drop into one of the Azure, yeah, data centers, probably canary regions where we can attach that to some compute, do the bare metal integration and then start, yeah, testing that against some real compute. So I think it's going to be that, I mean, we've done this before around all the things I talked about, the Cray supercomputers. GPFS file system plus the store. So we've done that with Anand's team before. They've done lots of other solutions like that for things like VMware, SAP and so on mainframes. So they have an established methodology. Their network technology evolves quite rapidly. So it may be that you're working with different network technologies So we've got to work through all of that with that team and understand how far we want to take it with the resource provider for that as well. So, for example, in Gen 1, we don't have a very rich resource provider around the supercomputer. So we just kept it very, very simple in terms of how it integrates into Azure. We're just injecting BNIC. into the customer vNets. We're not actually exposing all of the management plane and everything else from the supercomputer into the customer sub because it's just, it's too much old school stuff in there that just isn't really, there's no value in passing that through to the customer. We just manage it on their behalf. I think VAST is going to be very different. to have a much richer front end passed all the way through to the customer. Sounds like that's what you're planning anyway.

[00:36:30.46]  Jason ValleryYeah, on the networking side, is the vision when we do this testing with bare metal slingshot to proxy between the Azure Virtual Network and our bare metal or do we have to go and do any work on our side to support network offload?

[00:36:45.83]   RemoteIt's a good question. I think the slingshot thing is a bit of a sideshow that we want to look at. I don't think that should influence the main development track right now. I think the goal should be to connect it into Azure Networking, Azure Ethernet, to connect up to Azure Compute. If we want to connect it into Slingshot Compute, I think we've I think we should view that as a sort of a different exercise, because I don't think we're going to be using all the same type of integration, we would just take that as a vanilla vast metal solution and connect that into our slingshot edge network. It's just, I don't think we should get distracted. system? It's not for the whole generation to work. So it's... What I'm looking for are opportunities to inject VAST into the current solution wherever I can, so we can start to just get some experience and build some momentum around it. They may come with an opportunity cost and we mutually decide, look, it's too complicated, it's a distraction. we'll do something else there and look forward. So this is for the GPFS potential swap out that we touched on a couple of weeks back. Have you got anything to comment on that? I've been asking you when you've been on PTO about slingshot integration. I've kind of like garbled some of your words around RDMA. support. What's your take on that if we were to replace the GPFS system that's already in Gen 1? So the long story short is there is no RDMA on Slingshot for us and that has to do with how Slingshot does RDMA and it's true, it must be true for GPFS as well because basically Slingshot's using a user level library for RMA and file systems sit in the kernel space and HPE went a long way to get Lustre to work so I don't think they have done this for GPFS so probably they sit on the TCP/IP layer which we would sit on as well so we have two ways into Slingshot one is Ethernet groups and gateways or routers. switches or whatever they call them but it's basically Ethernet, a group of Ethernet switches we connect with Ethernet into or we do something called direct connect and we have done that with a couple of customers but it's a bit it's a bit gray zone because officially HPE product group needs to approve everything that goes direct connect into a Slingshot fabric so the customer has to push for this we have done this in San Diego Supercomputing Center and one or two other sites. But for a home file system, I wouldn't be too concerned. TCP/IP is a good way of doing that. We have done that with CSCS, and we have done that with Bristol. So no concern. The only performance concern for this relatively small config that you gave us was the metadata create rate. Not that we can't hit it, but we need to-- amount of hardware, just because how our systems work, and that didn't quite match with the capacity, but other than that, perfectly fine. Yeah, okay, great. Sorry, lengthy answer, but... Oh, yeah, thanks. Yeah, that's helpful. Should we pick this up on another conversation, Mike and Nico and Trevor, and see if we can come up with something? It's not funded, as you say, but if we can put a design in your pocket, that'll give you some ammunition come the day, perhaps. It'll probably be a business case based on the penalties that it's causing us currently. Yeah. Can you elaborate a bit on what is causing the outages? Like, what is the problem right now? It's just that I don't know the detail, but it looks to me like all the classic issues you get with one flaky client bringing the whole thing down, hanging the file system. The ripple it crashes off. Yeah, so your node health checks fail on all your compute nodes, and suddenly you've got a massive outage. Getting spells off of the GPFS client. Locking issues as well. On the compute side, would we have the opportunity to influence or change the ephemeral state of the NVMe drives? Would we be able to, for example, to suspend? I think it's an eager question if it's their compute. You mean on the L-series or? Yeah, because right now it's not allowed or the NBME is ephemeral, that obviously causes some challenges, but it would also open up more opportunities for some more advanced if it wasn't, or if it was at least controllable. - Trevor, Nico, you have to come across that. - Oh, I think that will depend on what the compute team will end up with, because if they will build a special SKU just for Met Office, then it will not have that issue, because that would be the hardware that is purely dedicated to that customer. Even if it will be a general purpose, it should still be a dedicated cluster. are taking the security constraints and taking it will probably host it in a in a in a separate disease. So I don't think that would be an issue for Met Office. You might have that issue for other customers, but for Met Office I don't see that as an issue. We do have technical ways how to do it even today with something which is called dedicated hosts. So it's shouldn't be that big of an issue from from the Met Office perspective. I'll share the status with Compute. We've been asking for that for the past nine months since we started talking like in parallel to discussing Ignite before the contract was signed and they understand what we need and they understand why and that wasn't clearly something they wanted to provide with LSV4. I think that now there is a more momentum shift with the way they're seeing us as a partner, and it's time to revisit and to ask for it, not just for UKMF, just to ask for that to be available with LSV-5. But again, it's a computing decision if it's standard VMs. We'll take it with engineering, with the data plane engineering again. I had this this um bare metal POC system going to mention the Canaries region is that as in Canary Wharf or Canaries as in the island? It's just our term for like a canary in the coal mine. Right I see okay okay. It'd be nice if it goes to the Canaries. Yes I was gonna I was gonna say um is is this a system that uh the Project Malone titled? Is this a POC system, an NFR system? Will you own it? - Can I, is the regions where we basically host non-production, non-production stuff, right? So yeah, we have three of them or something like that, two or three of them. - Typically it would be the product group that are building the product will own that system. So it'll be their development. platform and we will be, we'll have access to it if they grant us access to go into their environment and test things, so it probably won't work that way. We could do it the other way around, if we're on a super tight timeline, could deploy it in our environment and give them access to it, that's how we did the HP supercomputers. I think in this case, it's probably better to give Anand's team full control over that and get it done the way they want to do it. OK. So who-- is this something that we'll sell to Anand's team? Will they buy it? OK, I mean, that's going to be the determining factor then, I think, then. If that's not clear yet, then, yeah, we'll have to figure that out internally. We would love to sell you the system instead of giving you the system, clearly, right? So if we can do that, that would be great. Much easier on operation. It's healthier. Yeah. Yeah, it's healthier. The system isn't going to go out, right? So it'll be much easier. We're going to run with the process to expedite everything unrelated to the answer, but that would make it so much easier. Okay. How do we figure that one out? Is that a conversation, Mike, you have with Anand? I'll bring it up with Anand, and then maybe we aim to have an answer around supercomputing. Would that work? Okay. Yeah. Yeah, okay. I mean, the alternative is, I can... work with Trevor to put a PAC together like we would do for any ordinary customer. The number of hoops that we've got to jump through there are quite severe. We've got to talk around exactly what we're testing. So if there's an opportunity for you to just buy and do what you want with it at leisure, then... When Karl had talked about this earlier on, Nico, we definitely were planning on... on buying the system? Well, I think it was planned to buy two systems for this. - Okay. - Yeah, that was the plan. But at the end, it's an unscaled because we also need to find space for that and space and power and all of that, all those things. So he controls that, right? So let's close on that internally, Mike, with him. first and then we come back to the last team. Okay, so what I'll do in the meantime then is I'll work with Buzz, we'll put together a bill of materials which will define what it looks like in the data center, how I call it. By the way I suspect, I don't know how your conversation went, but I suspect he might take the position that he wants to wait for the answer from Igal's team. to sort of rule that out before he moves ahead with the bare metal or do you think he was irrespective of that is he committed to the bare metal track because of these other larger accounts you mentioned? I think it's either or we'll see we'll see how it applies. Another logistical question since Microsoft will be buying one Microsoft to the other. then how do we transact it's hard work that's something that if we need to on board as a vendor in the system because I guess it's not or maybe it can be done for the marketplace you want to see because you know just again this is like to step forward if you come to a place that we need to buy I think the easiest way is that we use all of the approved vendors, whoever you have a deal with, but we don't buy from you directly, but from HP, Dell, or whoever the preferred vendor is, because we already have those relationships, so that will be much faster and easier than-- - I'll give you a name, but don't reach out to him yet. It's Ed Kim, so he's our procurement contact. for OEM hardware. So I'll talk with him and see, you know, what would it go through that route. I mean, ultimately, once the hardware is in place, and let's say the hardware is in place, I mean, we basically just can point the marketplace offer or give you our deployment, Polaris Deployment Toolkit that actually automatically deploys BAST on that hardware, on that dedicated hardware. I think if you can, probably just thinking a few steps ahead, if you can give us your, the configuration Jan is building and your preference as to which OEM, so which OEMs you're most comfortable with. maybe give me a one, two, and a three, and we can take that to sourcing and see. Yeah, so I think if we're looking to do the bare metal stuff, path of least resistance would be to either work with HPE, because they can transact through their 3PO process hour OEM stuff. So that would be Simon Appleby and Collette Keelty, who would organize that. any of the sort of traditional integrators from WWT computer centers, CBW, those types of things? I'm thinking one layer up, so we're dealing directly, you know, it's all secure supply chain stuff, so we're dealing directly with Dell, Lenovo, HP, those sort of guys. Okay. if you can sort of think on that level, what would you-- - Yeah, okay, so in terms of hardware provenance and source of supply, yeah, I would say, yeah, we would push that through HPE, that's the path of loose resistance there, to do vast OEM kit, so that would not be-- - Who would your number two be? Let us get back to you and check. It really depends on the last massive deals that we've signed with those guys to see who is the easiest to work with. It's the same on our side. They may have a preference in terms of where they are on the buying cycles. It may be more favorable to pick another one. I don't know yet. I would probably say Eviden as the second chance if we're looking for secure supplies. with potentially batched staff working into organizations. - Okay. - But we can spin it anyway. We've got a very good partner network. So we can go with whoever, whichever. - The problem with the partner network, the question is, do we have an existing contract? - Exactly. - If you have, that doesn't mean that we do. try to basically catch HP, Dell and all of those players because I'm pretty sure that we have a big pool. When you're ready, do you want to send, do you want Ed and I to have a chat and you know I can see his top list and he can see our top list and then we can pick some winners off the back of that. I mean ultimately we're just looking for somebody to transact and not make a big zero of it. So when you're ready we can work with those guys and we can give them the support. I think that hardware is one path, then the second path of getting our software, that should be an easy one, because we are going to be on your marketplace. So once we decide about the hardware path, then the software license. Yes, yes, yes, I know Microsoft internal is not allowed to use software from marketplace. place. This is an issue, yeah. That might be another. So internally, now, no organization within Microsoft can actually use software for marketplace, right? But, I mean, ultimately, Niko, we would basically just, we could basically run this on BAN. paper but you but you would go through the same automation as the marketplace that's you know that's that's not not non-issue basically basically just handle it like a marketplace transaction in terms of the automation and everything and the lifecycle management and basically just give them the tool But we would have to transact through the last papers. Yeah, because if it's an internal subscription, that's what you will not be able to purchase anything from marketplace, even if it's free, that doesn't matter. It's just internal stuff. So good news. It's not free. But other than that, jokes aside, even if it's Lifter, it's still marketplace, right? Okay, so with your support guys, we would like to become a first party, so it's not a marketplace anymore. We accept your support. You have the wrong people on the call for that. Leo, you know that, right? So, yeah. Let's keep the discussion for another day. There will be many wrinkles like this that we'll run into, so we're just going to... That is for sure because... because I ran it in that multiple times, I couldn't find any kind of bypass mechanism for that. It's just like legal stuff, I never got into details. But if it's just a deployment part that we just need, then that's completely fine. As long as we don't need to transact with marketplace through APIs, that will not work. to figure out a separate procurement process just by the software. Yeah okay okay good well I'm glad that we've identified that we don't know how to do this but okay we'll keep talking about that. All right we're at time anything else that we need to cover off anything else that we need to put on the agenda otherwise. I'll fly through the actions, drop a note out, AI Notetaker, your mileage may vary, and we'll have a call in about a week's time or so. In the meantime... James, can we set up a separate engineering track as well? Yeah. This is usually an engineering... track to be honest like that's because we're now dealing with logistics but this is usually an engineering track so okay no i'm talking about like for example if we are going down the oem hardware route uh i then we need to know a little bit more about uh the the hardware apis the switch apis in order to be able to automate all of it just like we automate. I mean we have done this before with hardware on ANF, so hardware even though we are all software guys, but the that's not going to be an issue to automate that we just need to get to the bottom of that. The other OEM that we're discussing is actually vast C-boxes and D-boxes right so it's really how hardware so if we want to get fruit i mean so so that's kind of on us it's all our our software even if we call it odm oem from the polaris side we do not care what the instance type but it's going through a hypervisor as long as we can communicate to the vast apis and then we can actually have that translation layer that actually translate so the customer can choose whether to manage the vast with ARM APIs or vast APIs depending on what he is more familiar with or and this also goes into integrations with native services that you might want to offer to you came at open air services as your one leg or whatever it makes the integrations very very simple from our perspective and we already have the ARM APIs back in place. So I would like, we're at the end of the time, but I would like to suggest two action items. One is internally, guys, we're going to sync up and figure out again how do we approach it the best possible way. A lot of stuff was discussed today. Let's keep this call as our sync as one until we start testing and then we'll split it into a technical track and a business track and because we had similar as what we did with Lifter we had one track when we were planning stuff and once we actually started moving forward completely different tracks. So I think in a week or two we'll need to to create a different track if not sooner. I agree with Jonsi on that. Okay the bare metal thing is something separate so let's we let's in order to get Anna on board, then we'll get you introduced to that engineering team, yeah? Yeah, sounds good. Okay, good stuff. All right, thank you very much. Thanks everyone. Bye for now. (keyboard clicking)Pause for group workSilence from 1 minute to 1 minute 45 seconds
```

<!-- ai:transcript:end -->
