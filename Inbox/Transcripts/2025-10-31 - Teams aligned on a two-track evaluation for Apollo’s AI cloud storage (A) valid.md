---
entities:
  customers:
  - '[[Microsoft]]'
type: transcript
source_type: unknown
date: '2025-10-31'
---

# 20251031 1134 Parallels Transcription

**Date:** 2025-10-31  
**Participants:** Qingying Zhang, Yanzhao, Jason Wilder, Wendy, Anson, Paula, Jovane, Lior Genzel, Jason Vallery, Ray, Paul Haddo, Andy Prentice, Alon

## Summary

Teams aligned on a two-track evaluation for Apolloâ€™s AI cloud storage: (A) validate VAST software on Azure-native lab hardware, and (B) run a VAST hardware POC (loaned gear) in Microsoftâ€™s lab and/or Azure dedicated. VAST will share rack/server specs and costs, ship a minimal POC cluster, and coordinate with Anandâ€™s team. Apollo will provide proposed lab hardware SKUs/specs and precise KPIs for sizing. Timeline targets launch in Sepâ€“Nov next year; initial DC 30â€“40 MW; eventual scale to ~400k GPUs.

## Key facts learned

- Apollo is a new AI cloud, separate from Azure; Linux + Kubernetes based.
- Initial focus: AI training (single tenant initially), with inference between training windows; SDN planned from day one.
- Scale goal: ~100k nodes (~400k GPUs) at maturity; first DC target 30â€“40 MW.
- Target launch timeframe: Sepâ€“Nov next year.
- Evaluation compares VAST vs alternatives (e.g., Lustre, Azure Storage) on same hardware where possible.
- VAST architecture uses C-nodes (compute/protocol/EC) and D-nodes (flash, DPUs, no x86 CPUs).
- D-nodes use BlueField-3 DPUs; NVMe-oF target via NVIDIA DOCA; can alternatively use SPDK on ARM.
- Example rack envelope ~27 kW including switching; D-box is 1U, >1 PB per 1U possible with dense flash.
- Minimal resilient POC: ~3 C-nodes, 1 D-box (dual BlueFields), and a pair of switches; requires RoCE/RDMA and 400 GbE uplinks.
- Azure Lsv4 is insufficient for performance benchmarking; Lsv5-like shapes come later; Azure flash density may limit perf/W and perf/PB versus VAST-optimized gear.
- VAST can ship loaner hardware within days; can preconfigure; standard racks, air-cooled.
- Throughput/KPI requirements are not finalized; Apollo to follow up with precise numbers.
- Anand Ramakrishnaâ€™s Azure dedicated path may accelerate hardware POC access (similar to UK Met Office engagement).

## Outcomes

- Agreed two-track evaluation: software-on-Azure hardware and VAST hardware POC in parallel.
- VAST to provide detailed rack/server specs and cost estimates sized for target scale and for initial 30â€“40 MW site.
- Apollo to share proposed Azure lab hardware SKUs/spec for software-only POC.
- VAST to ship a minimal POC cluster to Microsoftâ€™s Stargate lab; can use Apolloâ€™s switches if RoCE/RDMA enabled.
- Both sides to engage Anandâ€™s team for Azure dedicated POC path and timeline.
- Ignite executive meeting coordination to be arranged (VAST CEO with Microsoft leadership).

## Decisions

- Proceed with two-track evaluation (software-only on Azure lab hardware and VAST loaner hardware POC).
- Initial deployment model: single tenant with SDN ready for future multi-tenancy.
- Use RoCE/RDMA networking for POC; 400 GbE uplinks required if VAST switches not used.
- Baseline POC footprint: ~3 C-nodes + 1 D-box; expandable as needed.
- Defer Azure Lsv4 for performance testing; consider VAST-optimized or Azure dedicated hardware instead.

## Action items

- [x] Share VAST rack/server specifications (C-nodes, D-box/D-node, network uplinks) and estimated costs, sized for ~400k GPUs and a 30â€“40 MW initial site. @Lior Genzel â« âœ… 2025-11-08
- [x] Send latest VAST hardware specs including AMD Turing-based updates and DPU details. @Ray â« âœ… 2025-11-08
- [x] Provide proposed Azure lab hardware SKUs/specs (e.g., Gen9 storage pod with BlueField-3 DPU) for software-only POC. @Yanzhao â« âœ… 2025-11-08
- [x] Confirm Stargate lab can host the POC (rack space, ~30 kW power availability, air cooling, standard rack size, 400 GbE uplinks, RoCE/RDMA). @Yanzhao â« âœ… 2025-11-08
- [x] Provide POC power specs, rack size, connector types, and network uplink requirements. @Ray â« âœ… 2025-11-08
- [x] Ship minimal VAST POC kit (â‰ˆ3 C-nodes, 1 D-box; optionally preconfigured) to Microsoft Stargate lab. @Ray â« âœ… 2025-11-08
- [x] Share evaluation license and installation/operations guide for deploying VAST on Azure lab hardware. @Lior Genzel â« âœ… 2025-11-08
- [x] Engage Anand Ramakrishna to scope an Azure dedicated path for earlier access to VAST hardware POC. @Qingying Zhang ðŸ”¼ âœ… 2025-11-08
- [x] Coordinate with Anandâ€™s team from VAST side to align on Azure dedicated POC logistics and timing. @Lior Genzel ðŸ”¼ âœ… 2025-11-08
- [x] Research precise KPI targets from NVIDIA to build dagger slide. Work with John Mao (per-GPU bandwidth, aggregate throughput, capacity) for sizing. @Myself â« âœ… 2025-11-08
- [x] Validate lab network configuration supports RoCE/RDMA (SP3/SP4) and VLAN/SDN requirements for single-tenant POC. @Jason Wilder ðŸ”¼ âœ… 2025-11-08
- [x] Share VAST reference design slides and materials discussed on the call. @Lior Genzel ðŸ”½ âœ… 2025-11-08
- [x] Arrange Ignite introductions (VAST CEO with Brendan Burns and Microsoft leadership) and confirm scheduling. @Qingying Zhang ðŸ”½ âœ… 2025-11-08

## Follow-ups

- [x] Clarify DPU programming interfaces: VASTâ€™s current DOCA/NVMe-oF usage and compatibility considerations with DASH API. @Alon ðŸ”¼ âœ… 2025-11-08
- [x] Confirm minimal POC BOM and any switch requirements if not using VAST-provided switching. @Ray ðŸ”¼ âœ… 2025-11-08
- [x] Confirm whether VAST hardware POC will run in Microsoft lab and/or Azure dedicated, and share expected timeline. @Lior Genzel ðŸ”¼ âœ… 2025-11-08

## Risks

- Approval and lead times to host third-party hardware in Microsoft facilities could delay POC.
- Azure-qualified flash density may limit perf/PB and perf/W compared to VAST-optimized hardware, skewing comparisons.
- GPU scarcity limits representative end-to-end testing in the near term.
- Unfinalized KPIs (per-GPU and aggregate throughput) impede accurate sizing.
- Tight launch timeline (Sepâ€“Nov next year) increases schedule risk if hardware decisions slip.
- Potential DPU API mismatch (DASH vs DOCA/NVMe-oF) may require integration work.

## Open questions

- What are the finalized throughput KPIs (per GPU and aggregate), and perf/GB or perf/PB targets for training and inference?
- What exact Azure lab hardware SKUs (CPU, DPU/NIC, SSD type/density, network) will be used for the software-only POC?
- Will the VAST hardware POC be hosted in Stargate lab, Azure dedicated, or both, and what is the approval timeline?
- Are there constraints on 400 GbE uplinks and RoCE/RDMA configuration in the lab networking?
- Will Apolloâ€™s target DPU environment require DASH API support for production, and what implications does that have for VAST integration?
- How many GPUs (if any) will be available for early POC performance testing?

---

## Transcript (auto)

```text
[00:00:00.05]   RemoteI did not attend the meeting last week, so it would be great if we maybe start with some background of your project and what you're trying to achieve, and the timeline behind it, and then the objective of today's call, and we've prepared a slide. deck that I will share after the call that is some basic information about VaST and VaST solution, but it also is a reference design to what we understood was the request from last week and the objective for today's call is to discuss how do we move forward. So you're talking about testing, what type of testing, what type of KPIs, and so on and so forth. But yeah. So, you know what, maybe it's almost time. Do we have, I see Jason has joined. Maybe we give another 30 seconds before we start, then we kick it off. - Yeah, sounds good. So in 25 minutes, we'll have our CTO and founder alone joining the call, and he will go there as well so an easy follow-up for you is to meet with Alon and when you go to the Kubernetes events there's Andy joining so I think we can start with him joining okay and Ignite we have our CEO and we have our cloud GM join attending so if there is anyone in your team that will be attending and you want to put them in front of the two top executives on our side, then we can do that. Yeah, I think someone reached out, I did share, Brendan is going to be there, Brendan Burns is going to be at Ignite, so I can connect with him, connect whoever on your side and your CEO to arrange a meeting with them and our VPM. So connect me, I'll work with someone for marketing who is controlling the agenda for CEO. Those day and a half is going to be at Ignite. Connect him to me and I will verify that he's on the schedule. Okay, so let's start. So maybe a quick round of intros since some of the guys on the call are new. So we start on our side. Jason?

[00:02:36.16]  Jason ValleryYeah, hi folks. I'm new to VASTA, a couple weeks in role, I'm looking at a VP of Product Management for cloud offerings. Most recently, I spent 13 years at Microsoft. So, I don't think I crossed paths with any of this crew, but I was a GPM for object storage, blob storage for Microsoft, focused on AI storage for quite some time. and I jumped over and joined the VAST team and I'm really excited to work with you guys on this project. I guess I should also add some context. You know, I do have some familiarity with MAI and what they're up to and all of that. So eager to learn from you guys how we can help.

[00:03:12.75]   Remote- Awesome, awesome. Okay. - Okay, and we'll move to which is joined. Ray, you want to introduce? yourself? Are you joined? Hi, my name's Ray. So I'm one of the fuel technical directors at Vaast, and I am focused on large opportunities and any designs that need to adhere to reference architectures. Okay, thank you. Hi guys, I'm Paul Haddo. I'm the Systems Engineering Manager for VAST, looking after the pre-sales team in my region, and Andy, I think you all know Andy from the last meeting, andy? Andy Prentice, Centerfield CTO of VAST, and I guess Tiffany was on the previous meeting as well. So my name is Lior, Lior Genzel. I lead the sales and business development with all of the clouds, including Microsoft, and we'd love to get a quick intro on your side. Sure, I can start. So I think I introduced to, I think we saw Andy and a few other folks last time. So I'm Qi, I'm the CVP corporate vice president. in Azure. Now, I lead the Azure Kubernetes Service Arrow, these cloud-native services, but right now, my focus is mainly on this new project that is going to build a new AI cloud ground app. I can nominate next. Yanzhao, you want to go next? - Yanzhao, I'm the Apollo Product Manager. I work with Qi, thank you. - Okay, Jason Wilder. - Hi, I'm Jason Wilder. Used to run the AKS, the infrastructure, all the underlays and Kubernetes and things, and now working with Qi and folks on this new AI-based project, kind of doing similar areas of work. - Wendy. same thing as Jason, but I used to work on the AKS managed data plane. Anson? Anson, I'm here on AKS, and I'm Paula. Okay, cool, and Jovang, I guess everybody knows you. We all know Jovang. Thank you. - Hello. - Everyone, yeah. - Okay, so, yeah. - Yeah. - I'll go, Jovane, introduce yourself. We have Jason on his new year, yeah. - Yeah, I'm Jovane, I'm part of the Corp BD team. I cover our HPC and AI infra business, so yeah. - Okay, yeah, so Lier, you earlier mentioned that you want to get a-- understanding of what the project is about, so maybe I can give a very quick introduction of the project. So this is the new AI, like I said earlier, AI cloud which we will build from ground up. That's why we are considering different type of storage choices to meet the demand of this new massive AI training and AI inferencing workloads. Okay and you know building it ground up and you're saying massive numbers behind it meaning GPUs or capacity or performance anything that you can share? Yeah yeah we shared the last time with a group uh what we are current target is the 100k GPUs, 100k node, so that is the 400k GPUs. 400k GPUs, okay. Right, 400k GPUs, so 100k node if we are talking in the context of GP200, GP300, that's the scale target we are targeting. Of course the first data center we're going to build is not going to... be that high. So, but that's why we're asking you to give us the kind of estimate, a quote, like how much storage is going to be needed, what kind of bandwidth or throughput you estimate based on how you, the experience you gained in running it on Core Wave and for MAI. Of course, we are also talking to MI to get all these requirements, but we are thinking we need to go higher, a little higher than that to just have some room for growth. But 100K is the skill limit, as I said, initially we are targeting maybe 30 to 40 megawatt kind of data center and for that we'll definitely scale down from there. So initially that's why this gave us kind of like give you a roadmap where we want to target and where we want to start. Can I ask

[00:08:08.99]  Jason ValleryHow effectively the capacity was used? Sorry on that point is this training inferencing both? Is it single customer? or per facility or is it multi-tenant?

[00:08:18.88]   Remote- Good question. Yes, it will be single tenant initially, eventually it will be multi, but the first is single tenant. It will be target the training first, mainly training first. But then again, the thinking is in between the training, we can use it as inferencing as well. - Okay, timeline for the project? Like you have asked to start testing yesterday, so I guess it's all urgent. So what's the timeline? - Yeah, a year. So a year is about September to November timeframe. Next year would be the launch date. That's why we are actively evaluating storage choice now.

[00:09:04.97]  Jason Vallery>> What kind of Azure services will be in this facility? So you say new Azure, does that mean all of the-

[00:09:11.48]   Remote>> It's not Azure, it's not Azure.

[00:09:13.81]  Jason Vallery>> Nothing to do with Azure, like the ARM isn't there?

[00:09:16.15]   Remote>> It's not Azure, right?

[00:09:17.78]  Jason Vallery>> Yeah.

[00:09:18.42]   Remote>> It's a side-by-side with another Azure data center or data center. It's a separate data center, but everything is a ground up is a fully newly built based on just Linux and Kubernetes.

[00:09:30.71]  Jason ValleryI mean, obviously I have a lot of historical context, which is this considered the extended zone offering?

[00:09:36.41]   RemoteNo, no. Oh. Okay, um. Interesting, and so you already asked for testing again, we'll soon go into what we've suggested is the design Do you have any test plan in mind or any KPIs or any initial setup that you envision for this testing? We don't have that many GPUs 200 there in the data center yet, right? Because that's very precious and coming in waves. So, right now, we can only use, we can only test the performance based on limited number of racks of storage. So, that's why I need to ask you guys how much SSDs and what kind of CPU machines you need another option is that we can have DPUs, which is like a Bluefield 3 kind of DPUs, to be fronting the SSD. So that's why I'm asking if you have any preference of recommendation for the hardware. Of course, we need to take it to compare with existing, Jason probably knows, storage team has storage pod, machines, those hardware, so, of course, we have that.

[00:10:57.10]  Jason ValleryI have to be careful about what I'm supposed to forget and what I can remember, but, yeah, exactly. I do have a lot of context there.

[00:11:04.51]   RemoteOkay, so, I'm just, okay, so recapping kind of the way I think we're starting the meeting. So last meeting, I believe, Alon and Andy talked a little bit about... and the vast technology in iLevel, and it ended up with us getting kind of a task internally of looking at how does it look like if we build a system for 160,000 GPUs, and our, again, for design references, we looked at that scale with an exabyte of data. The size is really relevant to the use case itself. but an exabyte of data, and we've also used the NVIDIA best practices in that suggested design. So for me, this call today, I'm going to share some slides, but the first bunch of slides are going to be slides about recapping what kind of was discussed last time, and I'm not sure we want to cover that. more marketing than technical stuff, and then Paul and Ray on the call, sat down and kind of designed a system that's similar to what we're deploying with other use deployments, and we can go for that to explain how stuff works, and then we can really narrow it down to how do we build something for you guys to test based on your requirements and your expectations. and your timeline. Before we get there, just asking a question. Hopefully there is an option for us to ship hardware for you to test with? I mean, it doesn't have to be software running on Microsoft VMs, right? It can be our-- No, it's not. Yeah, it's going to be bare metal, but we prefer, I think, I'm not sure. how it will work with your hardware, I think it will be harder and take much longer time to get approval. That's why we prefer this on-prem. That's why last time I was asking for this on-prem solution. We provided the hardware and just deploy VASTA onto the hardware. Or do you think there's, maybe you can, how about this? Maybe you can share the. or your hardware spec so that we can evaluate to see, okay, what's the difference? Does it really worth us to have to insist to use ours or is it worth to pursue using yours? - Okay, so I actually don't have slides for that, but let's talk about it before we jump into showing you what we designed. - Yeah. - As of today, right? The last is the software solution, and as of today, we've been working with Microsoft on certifying our solution on your hardware. When I say your hardware, it's Compute VMs, and the available VM that is out there today is a VM named LSV4, and we also worked very tightly with your team, with the leadership of Igor Figlin, who was also on some of the email threads. and that has been a great help for us on designing LSV5, the next generation that will come out mid-next year to the end of next year, that will have a better spec. Now, at this point in time, LSV4, you know, isn't going to be the right platform for you guys running a benchmark of performance. No, and we'll talk about in a second, we'll show you the hardware that we have today, but LSB4 is a server with 200 gig of network traffic, 200 gigabit, and 23 terabyte of NVMe capacity, in-server capacity, and the way that BaaS technology works, the BaaS software works, is that we install our software on commodity servers and we transferâ€¦ transform them into a vast cluster solution and the building blocks that we have today that are being used at all of these big deployments we discussed are completely different and are at a factor better on efficiency, on performance, on scale. So maybe let's start with showing you what's, how does a vast design look like, and again, the reference that we created for an exabyte of data. data and 160,000 GPUs, and then we can talk about what's the room of possibilities of doing stuff with you guys for a test. Does that make sense? Yeah, before we start there, sure. Maybe, yeah, Andy, what are the things we we have learned last time? Maybe we can skip those and directly get to the technical discussion and especially the evaluation of the hardware spec and the recommendation of the storage setup. Okay so let's let's start with let's start with the engine right let's go straight to what we have designed. So I let the technical team it's actually only two slides. it's one slide that explains the building blocks per rec and then a slide that explains how does that scale, and after we're done with that, if you have one and ask any question you want to ask, then let's talk about the room of possibilities of testing based on today at Microsoft. So, Ray, do you want to take the lead on that? Yeah, sure. So, this is just aâ€¦ representation of what we would do as a rack scale solution and you would then each one of the racks would be identical of course the the ratio of nodes inside of the solution can be different depending on what power consumption is available per rack and this design for example it's designed to fit inside of a a power envelope of about 27 kilowatts per rack including all of the switching and all of the hardware. Now in this design we are showing the current hardware that we would ship it's AMD Genoa based in our C nodes and the what referred to as 1350 terabyte series units. So the bottom units actually house all of the NBME capacity and then the c-nodes at the top houses all of the protocol access, all of the erasure coding capabilities for the for the solution. That then gets scaled out to about I believe 72 racks in total. Now, so that basically then builds up. Quick question. Can I interrupt? Quick question. For those AMD C nodes, what's their spec? Okay, so I can give you a specification of them. Your timeline would mean we can actually use a of AMD Turing-based systems? - Yeah, I was trying to use that to compare, like I said earlier, right, in Azure, if you want to order a new skill, it takes a lot of effort to get approvals. I was trying to compare with existing storage skills and see if that can meet the demand to deploy Bust. - Yep.

[00:18:00.66]  Jason ValleryWhat you're likely to give us the JBOF side where the challenge is going to be. I mean we can run on a variety of compute nodes Because these are compute nodes. There's not India VM. It's the JBOF where we're gonna run into problems with any of the existing measure schemes.

[00:18:13.01]   RemoteYeah, so Just a moment We're jumping forward straight to the design because we in a way covered all of the other slides last time around but again the design for an optimal vast setup is based on a cnode dnode design and that's what we kind of just showed right so cnodes are just compute nodes and these are you know you can find I guess the right SKUs within Microsoft for those but the dnodes are design servers just for capacity, and so you have the C nodes on the top of the rack, not top of the rack, that's a wrong name, but imagine you have the rack. In the middle are the switches connecting the compute with the data, kind of sort of like a NVIDIA's design, you have those NVLink connecting the GPUs with each other. But here you are connecting the compute with the data, is that, did I understand correctly? Correct. So the C nodes, as Jason just mentioned, are not the problem, right? We can find the right C nodes within your compute fleet for that, but the D nodes are a problem. That's not the skew that you have today, and if we go back to what we just shared here, So here we're talking about, as an example, in a REC setup, or kind of our standard offering is that you have ten times, these are one-use servers, D-nodes, with 1.35 petabytes of flash per system, right? Yeah, and you just, again, we just don't have that. Now, there is another reference design that isn't highlighted here. isn't designed for this scale of projects, which is called e-books, and in the e-books design, which isn't on one of these slides here, but if we go backwards to the slide I just shared. So an e-books is really taking the C node and the D node, which are containers. We talked about it in Germany a while back. Taking that and really mixing it. in one physical server. So instead of having physical disaggregation between the C nodes and the D nodes, now you have a standard server, we call it ebooks, and the standard server runs the compute and it runs the data. Right. Now, even with those standard servers, what we're using today are servers that are are much faster and bigger than your Compute VMs in the fleet today, and we've been working with Microsoft, and that's why I shared at the beginning with the Compute team to get something that would be a much better fit for our technology, and right now, Compute team didn't commit, so I'm not committing on their behalf, right? but the compute team are talking about having shapes of VMs for those e-boxes or standard VMs that will be a better fit for our technology at the end of next year. Give or take when you wish to move to production. But if you wish to start testing next week, then I think the right approach is really to use our ODM hardware, and we've been working with Anand from the specialized team on another project which is also at the 320 petabyte scale, that's the UK Met Office, to enable our hardware for testing inside Microsoft effective immediately. So... So are you working with Anand Ramakrishna? Yes. Okay, so that's on a third-party cloud, right? The site that's not in Microsoft, but the third-party cloud place, is that right? It is right for the initial process, but again, if you look at the scales you're referring to, these scales, and again, we can go into detailed data that's... compares the space, the power, the cooling, the efficiency, the price at the end of the day, you need to have specialized skills for this type of project, right? Otherwise, it's just going to pay a factor more, and it will be consuming a factor more power. So, so again, like, we, yeah, so maybe at the end of, oh, I see the Ray posted the spec. So if we, yeah, go ahead. - Yeah, that is the existing spec. I can actually share with you the latest spec based on Turing as well. - Okay, yeah. Do you use any DPUs for the storage? - Yeah, so our D nodes are designed with. DPUs. So we're using DPUs, the DNodes do not have any CPUs in them, and it's the same blue fields that you guys that you just talked about, and it's a part of the design, correct. But again, this, we're using all commodity hardware, but our design isn't standard to the way Microsoft Azure design is, and And if, and again, that's why I asked about timeline of the project, right? And more than that, even NSV4, which is a VM we're going to have initial availability with in December, and we're going to GA a marketplace offering in February, even that's not ready tomorrow, right? So if you want to start, Alon, just join. or CTO. So, so is it a problem, again I'm trying to go with the flow of your process and and kind of to understand what will work and what will make sense but if Anand, if Anand will build a cluster that will speak for your requirements, is that going to be possible? to use that cluster or is that going to be an okay? It's not a problem, it is just this effort right now we are we are deciding the hardware because as you can see right we want to launch to customer by November September November next year we have to get the hardware decided because there is a long procurement time and to get them delivered into data center and get approved. We can take yeah if you can send the spec of the rack to us after the meeting and the estimated cost and how many will be needed and based on. 400k GPU but of course we can scale down based on what's realistic for the 30 to 40 megawatt the first data center we can take it to you know to the hardware team and see if that's a feasible and just for full transparency internally we are having a project that is using DPU in front of the SSD arrays for Microsoft Azure Storage from Azure Storage team. We are likely going to evaluate that hardware. So I want to give Alon a quick recap and then she can also validate that I got it right. So this is Alon, a new AI cloud. It's not Azure to be side-by-side next to an Azure data center. The thinking process is to build something that will scale eventually to 400,000 GPUs, right? And what and it needs to be ready timeline wise in Q4 of next year. So I think you mentioned November, October. November of next year if I got it right and right now you're evaluating the right approach, the right technology approach for storage and VAST is in a way what you want to test with and for an immediate testing request I want I want to reflect my experience with another project which is a Microsoft project so we started engagement with the UK Met Office. So if you guys are not aware the UK Met Office is a Microsoft project because Microsoft won, I think it's a 1.2 billion dollar deal over 10 years and the UK Met reached out more than a year ago and with a similar request they wanted to build hundreds of petabytes at scale for CPU and GPU, and we went through the process that you are trying to go through right now, which is the process of what's on the track, which VMs are available, how do we move forward, and it took us a year to understand that for their requirements, which are still shy compared to what you're asking for, the right way to go is is to also just look at the the most optimized hardware design for the requirement, which is what we have designed for customers at Core Wave Scale, and now they're moving forward into testing hardware with Anand's team. So, you know, based on that experience and based off a year of efforts and activity, I think that if you want to really understand, you know, possibilities and what can VAST provide, and the benefit for this type of scale, it will be much, much easier and much more efficient in my mind, in parallel to trying to figure out what's available with Azure today and tomorrow, to also set up a test with Anand, with the algorithm that sits within his team. Yeah, I want the things to go in parallel though, because obviously the hardware we cannot get it into the Azure data center tomorrow, probably not even a few months. But I cannot pause the evaluation because I have to compare with other options in the meanwhile. That's why I'm asking if possible, you can give us some temporary, not say temporary license, but something that evaluation license and instructions so we can deploy VaST into some Azure storage hardware and do the evaluation. I understand that the, you know, to achieve the optimal, most optimal performance and scale. So for us, your hardware is likely going to perform better. So we'll take that into consideration and trying to mimic that kind of setup.

[00:28:40.83]  Jason Vallery- So quickly, can I jump in and just clarify? So what I'm hearing is what you want to do as a proof of concept using Azure qualified storage hardware. So, I mean, I have to be careful. I'm wearing lots of hats here, but this is a storage. fast flash cluster like xio kind of cluster that exists today that's already qualified in the azure fleet you want to run vast bare metal on that that's right yeah so alone the the best server

[00:29:09.66]   Remotei'm aware of and maybe there are other servers in the microsoft fleet that we never got exposure to Wait a second, hold on. I think what you're describing here is a different set of hardware that can be available for us, right? So we would need to, so let me just say this in a very high level, right? We're talking about a very high level of hardware that can be available for us, right? So we would need to, so let me just say this in a very high level, right? We're talking about a very high level of hardware that can be available for us, right? So we would need to, so let me just say this in a very high level, right? We're talking about very expensive GPUs that need access to lots of data very fast the speed of which we can feed those GPUs is highly determined on the underlying hardware components on the network on all of those different atoms hardware components it's not just soft software right yeah we are happy to review and tell you whether we can support it, how long will it take us, and how it would affect performance. The question is whether it's the best, whether the outcome of that experiment would be giving you all the data that you would need to make the decision, right? If you look, if you review the environment that Microsoft AI is already paying for inside of CoreVM. That's an environment that's not testing, it's in production for more than a year. We can give you access to a lab POC that's hosted on VAST. We can loan you hardware. We can also support your hardware, but I think we need to be strategic in thinking. We need to understand whether supporting that hardware and making that investment is also what you're going to have in production, and we also need to understand whether it's going to be reflective of what, you know, performance we can provide. I think I love that. I love that idea, like if you can loan some hardware for us to in the, because we just need our own engineers to get in to run the test, and in the meanwhile, we're even developing this test, right, as we are ramping up. and I love the idea like if we can compare okay this is your hardware this is our hardware with your software and this is our software our hardware stack and have side-by-side comparison with the data I think that will be the most convincing evidence. Okay and and and when it comes down to for so for the track of running on your hardware where is that hardware that you imagine a year from now is going to be deployed en masse in this data center or for this new cloud? All those are open to build the right, the best solution without being constrained to specific hardware SKUs or something like that. So that's the challenging part of this puzzle, right? Things are all moving around. So we have a new hardware design. for the storage to accelerate which is involving DPU but it's different DPU put it that way so that's that's that's the other hardware I was talking about we need to evaluate for that we definitely need to develop a code or decor so for us that's another thing I want to ask is Since you are already using the Bluefield 3 DPU, are you just using the DASH API to program it, or you directly call the low-level APIs of Bluefield? - So on the Bluefield, we use the Nveni over Fabrics offload. - Right, but what API do you call to program the DPU? I'm not familiar with the exact naming but it's it's really we're okay so it's called NVMe over Fabrics. I think it's doca is kind of the the overarching kind of uh you know um driver and software suite coming from NVIDIA and through doca we just configure the NVMe over Fabrics target. Yeah if we can get some uh follow-up after that. because the other DPU we're evaluating not evaluating likely we're going to use that support the dash API which is supposed to be the standard that Bluefield also support and this other DPU is supposed to be a standard API for all the DPUs so if you yeah dash dash d-a-s-h so if you call through that then it's almost like a no code change to deploy VAST on that new hardware. So is that going to be a server that only has DPUs on it, or it's also going to have regular CPUs on it? Yes, it's only DPU, but the DPU has the ARM complex, which is where you can run the code, the control plane code. So this is exactly... So this is exactly like the VASD box. I don't know, I don't think you're familiar with it. This is exactly what it is. Like, I'm going to just show you, this is a box that has blue fields on it and a bunch of flash. No X86, like we're using the arm, we're using the blue field, also the motherboard, exposing all the NVMe devices, and these are ruler, A1L, I think, as you can see. Azure also uses that form factor for QLC. So maybe we are getting very aligned on the hardware front as well. But I think there's two different tracks. I think one of them is validating the software. Vast is a software company, and we can easily adapt the software to different hardware environments. But if you want to see the software at its prime as soon as possible. it's best to do that on hardware that we already have certified and qualified and working, right? And then we can offer again, shipping gear anywhere in the world where there is a lab that your folks can work with. Or we can give you a hosted POC, we can give you VPN access to a lab that VAST has. Okay, that's one track. the software validating VEST does what it claims it does best, okay. Then the other track, though, we want to deploy in a year and we want to do it in the Azure native way, meaning using the Azure hardware. I've been on numerous calls, LAOS 4 and 5, and all of the potential designs and decisions. we're happy to partner with you on whatever we're going to land on here for this deployment, but those things just take time. So I think those need to be two separate tracks where one of them is you raise confidence and this is the right software for delivering what we need for 400,000 GPUs, and then we can walk this path. hardware and find the best solution that that works for both sides and I can tell you that leveraging servers that have Bluefields on them is what Microsoft is already enjoying inside of inside of Core. Now if I look you know you know if Dash is related to Sonic and programming on the network side we're using the simpler way. We're using it as, it's running Linux, right? It's pretty simple. It's a standard operating system. We're not programming the NIC in terms of flows and things that are low-level networking stuff. We're just asking the Bluefield to be an NVMe of a fabric's target, which means it uploads the handling of specific requests over RDMA. read writes to SSDs. I think we would need to, I think there's potentially a gap here between what you guys are building on the, not on the hardware side, but on what does it, you know, what does programming the blue field means for Azure? I wouldn't be surprised if there's a lot that goes into overlay, underlay, networking programming, where we are. what we're leveraging from the Bluefield today is really just this offload that knows how to do I/O to the SSDs and even that's optional. We can also do that on the ARM cores using SPDK.

[00:36:58.14]  Jason ValleryI think this goes multi-tenant point versus single tenant. If these are single tenant environments, do you anticipate in this timeline you'll have some virtual network abstraction so that you can carve up your IP space? Or is it

[00:37:15.26]   RemoteGoing to be just a flat network that everybody can all the nodes are on? So it is in the design for the data center it will be multi-tenancy of course. There will be SDN layer. We will not expose network to the customer. That's day one. Yeah. Yeah. OK. Yeah. But with that still we are not in the we are not like making it extremely. We are we're not having more than one tenant. Put it that way. We will. designed to have this SDN layer on it, but we are not going to have multiple tenant on this data center. Okay. - Key, how do you feel about the separation of validating software based on something that, you know, we can deliver on- in your environment or ours how do you feel about the separation of this track versus building something you know understanding figuring out what's what's the best solution down the line. I love that. I love that. I love that. I think that's the that's actually my intention when I came in coming into the meeting is I want to first evaluate the software with your guys help of course. on this same, right, because we want to compare Apple to Apple, so same hardware, we want to run VaST, maybe Lustre, maybe, you know, something else, Azure Storage, blob storage, and compare the performance, and then we can add, get a loan from you, your hardware compared to current hardware we have, and potentially when we get our own DP. you that hardware and evaluate if we can deploy on that and compare. I think that will give us a comprehensive image of all the choices, and then we can make a recommendation to the leadership, which one we want to go with. That being said, we could be limited by the approval process, whether we can have your hardware in our dataset. But if we have the software evaluated, then we have more confidence. Even if we cannot use your hardware, we can still recommend to use your software inside our data center.

[00:39:34.23]  Jason ValleryCan you share with us a proposed hardware SKU that you would be deploying us on in terms of the Azure native SKUs that you're looking at?

[00:39:44.21]   RemoteSo it will be like. you probably heard like internally we call it like a Gen 9, right, this kind of storage pod, but swapped with the Bluefield 3 kind of DPU.

[00:39:58.63]  Jason ValleryOne of the things we'll run into, and again I have to be careful what I know and don't know, but is the density of the flash that's qualified? So, you know, we're looking at density flash on our side here at VaST versus what I know Azure to have qualified, that means what effectively you're looking at is the performance per petabyte or performance per watt ratios will be at a disadvantage using the Azure native SKUs that are currently available to you, and so I think it's about that longer-term roadmap, how we get. get more flash options available with higher densities to drive the ratios that are gonna be more interesting for these high levels of capacity when you start talking about exabyte scale. So I don't know, like what's that?

[00:40:43.63]   Remote- Yeah, understand, that is a very good point. But I wanna work on, you know, we have a limit, right? There are certain things we can. to certain things we just cannot do in Azure. I want to make sure I first land the vast software on this new data center, if possible. The hardware will be addition. - Can we get the spec of the hardware? Just again, maybe you can share it now or as a follow-up, like what is the spec of the hardware you wish to run the POC with? Yeah, we don't have it now, but we can follow up. Yanzhao, let's take that as an action item, we can share later. Okay, and in a theoretical world, if you get access to vast hardware inside Microsoft, which is based on the other process with Anand, tomorrow, will you want to start testing that tomorrow to understand how vast software works on hardware that is... tuned for our software. For the UK-MET project when is that when is production scale in terms of your that hardware is going to be deployed? So that okay so for UK-MET they have the same production timeline Q4 next year calendar year and and and based on that we have the same as here so the same idea that the launch as I said we have plan a plan B so they'll plan a is to test on Microsoft hardware right to understand the software and plan B that they already initiated is to work with Anand and to get the ability to test with vast hardware, we'd announced him. So I'm just saying, you know, we might be able to tap into that process and get Yuchi access to vast hardware in Microsoft sooner than you expect, meaning not six months down the road, and then you get the benefit of exploring our software, but only understanding what the software can do when it runs on the right hardware. so running plan A and plan B. I just want to be able to, I want to get back to you next week and share what can be the right timeline behind getting access to our hardware in Azure. Maybe it's sooner than you think. Okay, okay. I can follow up with Anand as well to see what is their plan there. I highly suspect what he's doing is on server. party cloud in another place, not inside Azure, right? Is that inside Azure, do you see? >> It is inside Azure, but it's Azure dedicated. >> Azure what? >> It's dedicated. >> It's extended though? >> No. She don't like how we do with Oracle and all those types of weird stuff. it's it's it's almost similar to that yeah oh okay so that's the third party cloud right it's still within azure but it's still azure but like it's like third-party hardware but like but right that still goes along with with the long idea of plan a plan b all right so for plan a winning our software running on your hardware we'd love to see the we're a bit of material that you have in mind and and and then we can share with your observations and the timeline for us to be able to to run a PLC and what what can the PLC is meaning what should you expect with this hardware so we need to understand what the hardware is in parallel you can follow up with an end I will follow up with an end and we might be able to set up a sooner than later for you to really understand what VAST is all about, software and hardware. Right, but again, I want to emphasize, I need to evaluate the software first, because there is a higher chance we can get the software deployed than getting the whole set, including hardware, right? So that's why we can share the spec and stuff, but I don't know how much it's going to the decision what I'm looking for is a sooner than later as soon as possible we can deploy a POC on our in our lab hardware and some performance testing tool we start to heating that and to get some sense of what is performance and scale of vast software. when you say your your lab for a POC is that using your hardware or you will take hardware from us to the lab. >> QINGYING ZHANG: I how soon can we get your hardware. Because I'm thinking it's our hardware. Right. >> DIRECTOR DEWOLF: It could be in your place in days. We can we can easily ship in days. >> QINGYING ZHANG: Oh I'll take that. >> DIRECTOR DEWOLF: Okay. easy okay yeah like just let us know the power I guess cooling is just air cooling right not a liquid coolant. Okay Yanzhao I think we let's give a heads up to Lihua that's in our lab Stargate lab we can see if we have a space to host a rack

[00:45:44.04]  Jason ValleryFrom rust.

[00:45:47.69]   RemoteYeah, can you just provide the power spec for your rack? You know, how much power, and also the adapter, those kind of the things. I need to make sure, you know, the lab space, and also your rack size, right? So I'm not sure whether it's a standard or it's a customized. I need to find the space. - It's all very standard. We also don't deliver an entire... The entire rack is just servers, and the number of servers would be based on how much capacity and performance you would like to see. If you want us to start with a rack, and you tell us, "Hey, I have 30 kilowatts available," we can send as much to fill up that rack and try to provide as much performance and capacity or something that balances performance and capacity within a rack. we'll tell you how many uplinks we need 400 gig internet interfaces between our switches and yours and it's all very simple. So what is your server size? Is it a 90 inch size? So the D-Box that we mentioned with the blue field on it, it's a single rack unit and we can squeeze more than a petabyte in a single rack. and even two petabytes with the densest SSDs that we have, so we can we size that based on how much capacity you need, one D-Box, two, three, four, ten, and we size compute nodes which are one rack unit standard x86 compute nodes for how much performance you would need. Keith, do you have any kind of ballpark figures of how much performance per terabyte you want? Do you want us to optimize for capacity, optimize... for performance something that's balanced? I think eventually I heard I mean don't laugh this is just information we collected is uh I believe it's seven uh seven terabyte per second per petabyte was it right? was what MI gave to us, the performance target. Or did I miss the terabyte to petabyte? - It should be the petabytes. - OK. - 500, if I remember correctly. So it's like 500 petabytes in total. - No, no, no. - You mean per second, right? Yes, throughput I'm talking about. Throughput per GPU is 1.6 gatebytes or gatebit. No, no, no, 2.47. Is it bits or bytes? We need to be kind of specific. Sorry. Okay, how about you send your question and I can follow up. You can follow up and let us know, right? customers are not exactly, you know, they don't have their requirements completely ironed out, that's perfectly fine. We would size the cluster and the dynamic, I mean, the simplicity of assets, you can just add more C nodes if you want more performance per terabyte. Okay, how about we just get one D node and one C node, one or two C nodes, depends, and if we want to really hammer it, do you need to give us also one switch? loan us one switch there? We can use your network, but it's simplest if we also provide. So I think the minimal cluster would be something like three compute nodes for resiliency, a pair of switches for resiliency, and a D-box, it's resilient because it's dual controller, two blue fields, and that's it. Oh, I heard this, so it's okay, you can use our switch, right? We can, but we need to make sure that it's configured to support Rocky, RDMA, things like that. Oh, we have the RDMA, Rocky is no problem. So I think you mentioned that it's 5610, is that SP4 or SP3, I don't remember. You know, the switch AC case, the SP3 or SP4? We support both. I see. OK. We support both, and by the way, again, the C nodes and D nodes, we're not married to them. We have, you know, commodity servers, just really simple pizza boxes that have both compute and flash coming from places like Supermicro. or Lenovo or Cisco, or this is how we usually an Azure VM looks like. But this is what we deployed most of, this is what we offer when we send POC kits out there, and I think this is really the fastest way to go because this is under our control and we have access to that hardware and we can just send it right away, okay? - So who should I, you know, who will work with me? for the hardware delivery so. Right. Yeah, so that is the way I would like to have both tracks going side by side, right? The hardware delivery and the deploy of software on the existing Azure Lab machines. Who can we work with on that? Also, Ray? - Yeah, so Ray will cover the hardware side of the POC. I will start by reviewing, getting the spec of the hardware to kind of understand, map what needs to be done in order to run on that, and I'll bring an architect on board that will collaborate with me and then collaborate with you for that. Okay, sounds good. So we'll follow up with our hardware spec and get back to you how to configure, deploy Bust on this hardware, our hardware. Okay, sorry, one more question. So will you ship the hardware, the software? software will be inside, right? I do not need to ask software separately. - It's part of the appliance. - Okay, so it's together. - Yes, and if you want to, we can pre-configure the entire POC system before we deliver it on site as well for you. >> I see. Yeah, that one we can follow up offline. >> Okay. So we have a plan then. >> Cool. Thank you guys. This is a very productive conversation. Thank you very much. >> Thank you guys. >> Okay. Have a good weekend. >> Yeah, you too. >> Bye-bye. (technical glitch overrides the speaker's words)Silence from 0 to 1.00Silence from 0 to 1.00Silence from 0 to 1.00Silence from 0 to 1.00Silence from 0 to 1.00Silence from 0 to 1.00BLANKSilence from 1 minute to 1 minute and 15 seconds You YouPAUSE you
```
