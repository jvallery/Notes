---
entities:
  projects:
  - '[[Pricing]]'
type: transcript
source_type: unknown
date: '2025-10-27'
---

# 20251027 0729 Parallels Transcription

**Date:** 2025-10-27  
**Participants:** Tomer, Jason Vallery, Yancey, Wolfie, Lior, Eiki, Eric, Timo

## Summary

The team debated how to align cloud pricing with the new onâ€‘prem model. Two options dominated: capacityâ€‘only pricing versus charging a fixed, normalized number of cores per petabyte to keep crossâ€‘cloud parity. Near term, they agreed to launch cloud with capacityâ€‘only pricing and set stricter discount guidance, then target a unified model decision by Feb 1. A threeâ€‘phase GTM was reaffirmed: private offers in customer tenants now, public PAYGO offers in ~6 months, and full SaaS likely in FY28. They rejected disabling excess vCPUs. Immediate action is to deliver an NBCU TCO and define marketplace entitlements and discounts.

## Key facts learned

- Instance core density varies widely across hyperscalers, skewing any core-based pricing if unnormalized.
- Option A: capacity-only pricing in cloud keeps simplicity and matches current field behavior.
- Option B: fixed cores per PB for cloud normalizes price across clouds and aligns with onâ€‘prem core+capacity model.
- Network limits (e.g., 100 Gbps eastâ€‘west) can cap performance; extra cores may not translate to higher throughput.
- Private offers (commit/subscription) will launch first; public PAYGO targeted in ~6 months; full SaaS likely FY28.
- Do not disable excess vCPUs; it adds engineering overhead and creates poor customer experience.
- Cloud infra COGS for NVMe VMs is high (~$0.15â€“$0.30/GB/mo), making early use cases mostly burst.
- Initial list price reference in market: capacity list ~$0.07/GB/mo (cloud); onâ€‘prem list rework targets ~$13/TB with core component.
- Competitors in cloud storage price mainly on capacity; Databricks-style core+capacity is a future direction for VAST.
- Key customers/use cases mentioned: NBCU, Two Sigma, UK Met Office, Zoom on OCI; hybrid ELAs and marketplace burn-down matter.

## Outcomes

- Short-term launch will use capacity-only pricing in cloud private offers.
- Three-phase approach confirmed: private offers now, public PAYGO in ~6 months, full SaaS later (likely FY28).
- Engineering will not implement core disabling; any core normalization will be commercial, not technical.
- NBCU TCO working session scheduled for today; broader TCO approach to be socialized.
- Field will receive firmer cloud discount guidance to avoid onâ€‘prem-like deep discounting.

## Decisions

- Use capacity-only pricing for cloud private offers until 2025-02-01.
- Do not disable or rate-limit vCPUs in cloud instances.
- Target Feb 1 for broader pricing rollout that aims to align onâ€‘prem and cloud models.
- Proceed with marketplace private offers (AWS, Azure, GCP, OCI) before public offers.

## Action items

- [x] Propose cloud private-offer pricing structure and discount guidance (capacity-only) for immediate use by field. @Tomer â« âœ… 2025-10-27
- [x] Forward NBCU TCO call invite to the core group (Jason, Lior, Eiki, Tomer, Wolfie, Eric). @Yancey â« ðŸ“… 2025-10-27 âœ… 2025-10-27
- [x] Join and support NBCU TCO working session; align on inputs (reserved pricing, egress, infra COGS). @Eric â« ðŸ“… 2025-10-27 âœ… 2025-10-27
- [x] Join NBCU TCO working session; validate pricing approach and competitive posture. @Jason Vallery ðŸ”¼ ðŸ“… 2025-10-27 âœ… 2025-10-27
- [x] Add core stakeholders to NBCU TCO sync invite. @Tomer ðŸ”¼ ðŸ“… 2025-10-27 âœ… 2025-10-27
- [x] Define and publish a cloud-specific discount policy for private offers to avoid extreme discounts. @Pricing vTeam â« âœ… 2025-10-27
- [x] Complete NBCU TCO analysis and share with customer by Wednesday. @Eric ðŸ”º ðŸ“… 2025-10-29 âœ… 2025-10-27
- [x] Decide whether to introduce fixed cores-per-PB normalization for cloud starting Feb 1 and document the model. @Pricing vTeam â« âœ… 2025-10-27
- [x] Define marketplace entitlements and pricing schema for Google private offer and begin approval process. @Yancey â« âœ… 2025-10-27
- [x] Plan and document a unified Feb 1 pricing rollout that keeps onâ€‘prem and cloud consistent. @Tomer â« ðŸ“… 2025-02-01 âœ… 2025-10-27

## Follow-ups

- [x] Develop public PAYGO offer (customer tenant) within ~6 months, including consumption metering and billing. @Product ðŸ”¼ âœ… 2025-10-27
- [x] Plan full multi-tenant SaaS offering (our account), target timing likely FY28; align pricing and COGS model. @Product ðŸ”¼ âœ… 2025-10-27
- [x] Deepen competitive pricing analysis vs Pure, NetApp, Isilon, WEKA, Hammerspace (models and price points). @Pricing vTeam ðŸ”¼ âœ… 2025-10-27
- [x] Evaluate feasibility and policy for an optional 'unlock all cores' add-on price in cloud (no core disabling). @Product ðŸ”½ âœ… 2025-10-27
- [x] Assess using performance tiers (instance class/throughput per PB) to simplify cloud pricing communication. @Product ðŸ”¼ âœ… 2025-10-27
- [x] Investigate Azure Compute Unit and other cross-gen normalization metrics for potential future use. @Tomer ðŸ”½ âœ… 2025-10-27
- [x] Define hybrid ELA conversion and deployment tracking across onâ€‘prem and cloud in Polaris/Uplink. @Polaris team ðŸ”¼ âœ… 2025-10-27

## Risks

- Cross-cloud price/performance inconsistency if core normalization is not adopted postâ€‘launch.
- Customer confusion or complaints if pricing becomes complex (synthetic units) or inconsistent.
- Replicating onâ€‘prem discounting behavior in cloud could erode margin from day one.
- Marketplace entitlement changes require lengthy approvals; launch timing is tight.
- Under-monetizing higher-level services/compute if capacity-only persists too long.
- NBCU cross-cloud replication may face significant egress costs impacting TCO.

## Open questions

- Postâ€‘Feb 1, will cloud adopt fixed cores-per-PB normalization or remain capacity-only with tiers?
- What exact discount bands should the field use for cloud private offers by deal size and term?
- How will hybrid ELAs handle conversion and entitlement across marketplace and onâ€‘prem deployments?
- What is the simplest way to message performance tiers (throughput per PB) without confusing customers?
- How will we monetize higher-level services (AIOS, databases, functions) in cloud without leaving value on the table?
- How should NBCUâ€™s cross-cloud egress be modeled in TCO, and what assumptions will they accept?
- What marketplace entitlement definitions are required now to avoid 3â€‘week delays on changes?

---

## Transcript (auto)

```text
[00:00:00.00]   Remote>> Hey, Jason, how are you doing, man? >> Good. How are you? >> I'm all right. >> Where are you based at? >> How long -- what's that? >> Where are you based at? What time of day is it for you? >> Yeah, I'm in the Boston area, so morning still. How long have you been with Bassnow?

[00:00:27.50]  Jason ValleryA week, last Monday was my first day.

[00:00:30.78]   RemoteOkay, alright, I thought so. Yeah. Welcome.

[00:00:34.50]  Jason ValleryLearning the ropes, learning the team, learning the products. So, yeah, Firehose for me, but yeah, as expected.

[00:00:40.50]   RemoteIt's good stuff. Yeah, yeah, it's always a... It's always a big lift to get up to speed on all the history and everything going on. I've only been here nine months, so I'm still relatively new myself, but it definitely feels like I have footing now, where for a while everything was new. >> Morning. Oh, Halloween comes and it forces me to do. Things I otherwise wouldn't, if you're really interested, I'll show you the pictures. - Of course. - No, it's PG, don't worry, it's PG. I was basically Marv from Home Alone. The whole family did a Home Alone thing, so I had to be Marv. - Well, probably the best character in Home Alone, so. - For sure. - Hey, sorry for being a little late. Helen should be joining too, we were just talking about you. - I'll let you guys in on it, since it's Monday. use the uh you look very fresh faced yeah this is the crew oh yeah my son was harry this is the pizza delivery guy and because my wife was supposed to be harry but julian had to be harry she had to be kevin which is not a good look for a 40 year old woman to play Kevin. Yeah, I couldn't leave that hair do, I had to take it all off. Yeah. Yeah. Happy Monday. Happy Monday. Yeah. Best day of the week, we all get to get back to work from Sunday. night's work. That was a half joke. It was a good one. A good one, just not funny. I haven't worked in like 12 hours. How about you? So I think, Tomer, this is your meeting. Yes. Yes. I think we got one. Yeah, it was like. actually adding something to the spreadsheet here that we talked about previously. So yeah, it's my meeting. Originally, I planned it to have just a small conversation with the cloud team following last week's meeting, team when you're meeting, it grew. So, we already walked through the thinking for the broader pricing changes. I can go through those, you know, just a refresher, and I guess what we also saw is that the plans we have and.., and the changes we're going to implement for the broader pricing are not going to align with cloud pricing, especially with the differences across service providers. So I'll just share the spreadsheet that I shared last week, on Thursday, I think. So, let me just, can everyone see this okay? Yeah. Yeah. Isn't this the same tumor that we went over? Yes. Friday? Yes. Just to, just to frame this, right? We're going to change the price list right for the broader pricing right we're going to change the price list for the capacity so the plan is to lower the capacity cost lower the core cost right to these numbers and then lower the discount levels and this works really well for for the the broader vast business right. We compare this with all the deals we've had so far, based on opportunity data and all that, and those numbers work. Now, when we apply this to the cloud, we can see that, so just a refresher, also for Leor, Leor, I think you missed most of the meeting last Thursday. So the number of cores per petabyte is huge, even after I divide it by two, because it's a vCPUs to cores, and also varies a lot, right? So this is the cores per petabyte, and you can see that because. of the LSV-4 because it's small right versus the relatively larger i3en in AWS right we get we get this very different prices so this dollar per per terabyte right you can see the various the range here right from 80 dollars to 41 dollars between the the various uh hyperscalers obviously because we're taking the number of cores and applying the core costs for all of them. Now what I did for and and this is just for the um the various capacities right so if we take 10 b VMs and this is if we take 30 VMs and with 30 VMs we get better arranger coding so the price goes down a little bit but still right so it goes from here from say 26 to 22 or to 16 or whatever but still there's a big range between the service providers because of the core per petabytes the essentially the capacity the instance capacity I said if we want to stay with this model you know just just for the sake of the this these calculations right what Wolfie and I talked about it was Friday that you know what can we do here right and and one of the options was what if we charge a fixed number of cores regardless of how many there are in the instances, right? So if we wanna keep the same model consistent between cloud and on-prem and everything, let's say for the sake of pricing calculations, every cloud petabyte, let's say, has 200 cores, right? And if you look at the on-prem, we also have a- based on the performance, right? So this cluster, for example, will have 320, and this cluster, because it uses larger D-boxes, it has a lower capacity, I'm sorry, lower number of cores per petabyte. So let's say if I take something like 200 cores per petabyte for the sake of pricing, right? and fix that, regardless of how many are here, right? And that gets us to this number of dollars per terabyte, and of course, we can play with that, right? I mean, we can say, OK, for the sake of argument, we have 400 and not-- 200 and obviously this this price will go higher and the percentage of core costs from the total cost increases so um you know so this is I mean somewhere we are now yeah but I mean if we do that I mean what what I should do not want to have happen is that there is a huge discrepancy between each one of the hyperscalers. I've been in that scenario before and that never never ends well because one of them is gonna say hey how come your software or your deployment is cheaper on AWS than with us? yeah if we do that we fix that we fix the problem image right because we say everything that runs in the cloud for for our calculation doesn't matter which hyper scalar right if we if we use the number of cores that are in the instances naturally you know calculating them based on what's there obviously there is a big discrepancy but then we can make it a fixed amount, right? Just to really keep things simple and consistent with the broader model, right? And this is one option. I think, I mean, you can see, right? It keeps things consistent between the hyperscalers, right? In this case, it's $8.8. per terabyte per month but so so you don't see your concern right it addressed here but in reality we're charging them for let's say you know we set this number right this is 300 cores even though the instances themselves have a lot of you know yeah exactly way Let me clarify what I'm hearing then, is this number, so what you're saying is this number,

[00:10:59.51]  Jason ValleryWould it be something we would even necessarily share with the customer or are we just using this ratio to define the pricing on the back end and we would still just charge based on capacity and we would use this to sort of offset the compute number or how are you thinking about communicating the 300 number or 200 or whatever you just said?

[00:11:15.20]   RemoteEye to the customer? So the customer, especially if that's an on-prem customer, that, I mean, we're doing this to keep consistency between on-prem and cloud. Our pricing model will be communicated for the on-prem part, right? Or in general, right? Vast charges for cores and capacity, we charge for compute. compute resources under vast management. These are CPU cycles, right? And capacity that we manage, right? So all this is under vast software management. That's how we charge. For on-prem, we calculate the actual number of cores in the cluster. For cloud, this is what we communicate, is that we normalized on a fixed number And even if the customer has like for example in this petabyte example you might have like 900 cores per petabyte, but ultimately we will only be charging for 300. whatever, like a fixed price. - Yes, exactly, exactly, and this is really just, but I mean, the downside of it, so obviously it keeps things simple. It allows us to make things consistent between on-prem and cloud, but in reality, they will have more cores under vast management, let's say, you know, especially on Azure, for example. So for engine use cases, for database use cases, those scores are still under vast software management and they're worth the cycles they provide, right? The customer will be able to run more database and more engine and more functions and more things, and we charge them for 300, right? You know, there's a downside, but Yansi, you mentioned that we're addressing, you know, storage use cases, high speed, you know, fast storage use cases. You know, I don't think that this model works long term, but it will allow us to work right now until we add use cases. So Tomer, we're going to always just charge 300 cores. Hold on, it's one idea. So this is an idea, right? This is proposal A, right? Proposal B is something that we probably need to come up or let the cloud experts come up with additional options. But this is the option. we came up with to align, to make sense of the cloud, right, of this model in the cloud. Can I ask another question just, and it's kind of a follow-on from Jason's, but like I think you laid out clearly like how you how you would communicate this to customers, like will that make sense? Is that like a really, like A, what's the... like will that make sense relative to how we charge for on-prem where we're actually counting cores and then b like will that make sense in terms of like the value they're receiving to your point tomer if it's like oh i've got 900 cores on aws i've got 200 cores on google and you're charging me 300 for both like why like why does that Timo, it's never going to be that. We're always going to undercharge the CPU or the In 100 cores per petabit, so it's always going to be way higher in the TDP. cloud but I mean one other opportunity just when you're explaining it, it actually might get additional services that we have more on par with on-prem price or at least closer to it, right Tomer and Ulfie? So actually, at risk of taking us off track, I want to see if we could take a half a step back. I have a question that I'll save also on the actual infrastructure costs of the cloud, if you're looking at LSV4, I3, right? If those aren't congruent, but our pricing is, then the offerings still may not be congruent. between clouds and Yancey, one of the things you said last week, which I think we should open up, is what are the lessons that you guys have learned deploying in the cloud that we need to make sure we're avoiding as we're looking at pricing? One of those things being we need to have pricing consistent between each of these clouds or they're going to complain and we're going to have a problem and they're not going to like our offer. What are the... other things, if there are other things we need to be aware of when putting this model together. I know that we jumped right in with a possible option A, but what are we ignoring that might help us shape option B? What are the cloud principles, I don't want to say tenants, but the cloud principles that should guide how we shape this? Maybe I would... wasn't a part of it but I haven't had that discussion yet and there's probably other lessons you guys have learned over the past few years doing this that might help us shape this better. Is that a conversation worth having? Yeah, absolutely. I mean, first off, where we are going now is more aligned with, for example, Databricks in the cloud. We are initially going to be compared to our storage competitors and that's basically just pure capacity and they want to be able to compare apples and apples even though they're not, even though they're comparing a grape to a watermelon based on the functionality. and capabilities of vast versus the likes of, you know, DDN, VECA, NetApp, Pure, whatever. So, I mean, if we want to be consistent with the initial sort of competitive market, then we should just do very simple capacity pricing. But, you know, this is going to be more. cumbersome is a key on as well. Like when we go into the SaaS offering where we are carrying the, the, the, the infrastructure cogs, then, then we can actually, and maybe this is something that we should implement for the full multi-tenant SaaS offering. and not deployed in the customer tenant to begin with or the MVP. I'm just wondering if we should start with a very simple capacity, you know, $0.07 per gigabyte? Yeah, which is if we charge for fixed cores. I mean, we're getting to what I mentioned earlier, but we could charge only for capacity, which is the model that we've been doing today. It's just that we lower the capacity right now, we're at $69, $65 per terabyte, and we're lowering it in the new price list to $13. But the core is making up for it, and then some, right? What about the actual underlying infrastructure costs if you scroll left, Helmer, if we look at, even if we say that the storage price and the cores are fixed, is the customer paying the same for this instance versus this instance? No, right, and how much of price disparity between those shapes, between those clouds, because if we're trying to get price congruency and the customer says, wait a minute, I can go to Azure with this model and I can get 2,600 cores for free, I'm saying free, but it actually costs a little bit more. How do we evaluate these two? Because the infrastructure cost matters to a customer, maybe not to us in this model. - Yeah. - Do we know? - Yeah, I mean, that's why I'm saying, like when we control the infrastructure and it's running in our account or a managed hyperscaler account, then it changes drastically because then... we are the ones that can actually set that but right now in the MVP what customers are looking at for like the maturity of customers they are looking at our software pricing and if we are I mean we are always consistent on our software pricing whether it's you know basically just pure capacity but if we're doing the TCO calculations for the customer who is running the COGS, it is going to vary drastically between hyperscalers. But hopefully, like Leo, you're going, isn't Leo on the meeting? There's so many people on the meeting, I have to scroll. For example, if the new LSV5 that is coming up is going to have 130 terabytes of local NVMe SSDs, then that brings it very close to AWS pricing, and you know, we are working with each one of the hyperscalers. and getting more optimized instances for VAST. So with time it's going to become more, yeah, well, it's going to have a bigger correlation between each one of the hyperscalers. But right now it's, it varies drastically. So, so what that strategy, we talked. about it in a conversation just before this one started. We are still in the lift and shift, meaning we took the products from on-prem, we lift it, we shifted it to cloud, and that's the limitation because we need to use expensive NVMe VMs. It lends today somewhere between 15, least price, 15 cents to 30 cents a gigabyte. It depends on which VM you're lending on, and we have a process with all cloud providers to get better pricing. Getting better pricing is just getting bigger drives in those VMs. Because double up the drives is not going to double up the price. It's just going to be slightly more expensive. So today it's 15 to 30 cents, which makes it a solution that is really... only good for burst into cloud it's too expensive to to compete against new clouds like you know the pricing of Aldus Core we've go to market right so they are going to be two three times more expensive than Core even if they discount so that's where we land today now the design conversations we're having for 18 months down the road is we will solve it by changing the design of the data plane on cloud, but we're not there today. So, Wolfie, to your question, you know, we would lend about 10% of the overall deal price, even if we don't heavily discount. So, you know, if we end up at 3 cents a gigabyte a month, which is what, 60% discount on today's lease price, which is amazing if you compare it. to on-prem, it will be roughly 10% of the total deal size. That's where we stand, and that's why the initial deals on cloud, unless it will be an exceptional deal because of size, it's going to be very limited to burst, and that's it. Now, I prefer, based on what I've seen right now-- so you're right that each cloud will have-- different VMs at any given point in time and for that different pricing. So right now Microsoft can't compete against Amazon because Microsoft's are 26 cents a gigabyte a month and Amazon can the best configuration will be 15 cents. Huge difference because the drives are so small but Q1 they're coming out with six times bigger capacity so maybe Amazon will be more expensive Q1. I like the concept of us having the same price on all clouds, then it's their problem to discount, but it's not that we're creating that reason for them, and based on what I've just seen, between option A and option B, I think I probably think that we should start with capacity-only pricing on cloud until we figure out how cloud really works for us, and then, you know. then we decide to phase it, and even if we change the list price, we can always keep the same list price for cloud for at least a few more quarters because it's really a different deployment. Or we can just give much lower discounts on cloud and change the list price. Keep it capacitive. So simplicity wins. We have so many moving parts. Simplicity wins.

[00:24:48.30]  Jason Vallery- A couple of questions. So I think, you know, one of the things I mentioned when we were speaking last week is this idea that you've got some amount of performance per petabyte volume, and so if we look at the model that Tomer was presenting and saying you get 300 cores or whatever it is, that's effectively sort of codifying it. To say you have this amount of compute capacity by the amount of storage. I like that idea from a simple licensing perspective, but I do see a world where in the future, customers may actually want to have more compute than that ratio, and in that world, how do we make sure we're recovering and earning our value there? So that's not in that model. I think the monthly... versus my assumption here is what we're talking about in this model where you know seven dollars per terabyte or whatever it ends up being we're billing on a monthly based on consumption not a pre-commitment is that a fair assumption on my part that that's how we would want to position

[00:25:45.65]   RemoteThis this is actually a commitment pre-commitment i actually i thought one second you don't see my I thought the initial customer, the on-prem customers, let's just say they want to burst a 50, really, that's the numbers, tens of petabytes on-prem, and they want to build 300 terabytes on cloud because it's so expensive. They just want to burst, and they will probably prefer to buy a three, five years commitment, and our price, even if they don't use the VMs all the time, that they don't care. For them the ability to shut down and start the VMs they think will be more important than using paying us on-demand. So these are going to be private offers at the beginning for one year, three year, five year commit. They're not going to be day one on-demand on the marketplace, right Jonsi? Initial release is still not private offers. Yes, but Jason is correct. Once we blown sass it's all basically gonna be pay as you go pay as you go yes so I'm trying to separate the sass is like a year out or who knows like I'm trying to just address the the initial deals initial deals private offers multi-year contracts it's like subscription right it's not it's not on demand so now your Okay, your point on compute, I totally agree. But you need to look at the use case of the user-customer stories for the initial release, and it's really for storage. So the fact that all of the VMs in cloud were designed for databases and are over-provisioned on CPUs doesn't help us. This is really across the board. the VMs we're using were designed for databases and we're using them because they are the best network fan-in fan-out NVMe capacity, right? So they are the best for our, you know, lift and shift concept. So once we change and we start getting more optimized VMs or we start addressing use cases which are compute use cases, then we should not leave money on the table. That's why I'm saying at the beginning... it's actually good to just start capacity, and then once we figure out a way to overcome the over-provisioned CPU problem, and we start promoting cloud-native solutions on cloud, because we get more persistency on cloud, then we really go heavily on the same concept as on-prem and charge-for-compute.

[00:28:03.65]  Jason Vallery- So let me respond to that first, and then I'll go back to the other topic. So, I saw an email last night from Rana saying, 5.4 is the first AIOS release. What that tells me is that we're seeing all the value of the higher-level services and we're fully invested there. So if we go out with a model that doesn't monetize that correctly, we're missing a ton of opportunity if that's really the vision we're going towards, and I fully agree with that vision. So I agree with your point of like, yeah, storage, that's all they want. They're buying us for storage, so we don't to that, but that doesn't align with the bigger strategy that we've been marching towards. On the consumption model versus reservation model, so on the SaaS world, for sure, this is a per month fee that we would have to go out. But I actually still think even in the world where it's running in a customer's tenant, they still want some dynamic model behind it. side, you know, Microsoft's not going to charge them for a VM that's not running. I mean, maybe they'll go reserve instances, but I think that's an important component for us to consider. Like in the storage world, like I think I mentioned this when we spoke last week, there's this reserved capacity model in exchange for a reservation that says, "I am going to store a PepeByte for three years," or whatever it is. You you get some percentage discount. But there's also a pay go up, and I think it's important that we would have both opportunities for a customer who's bursting to the cloud. Well, a burst implies that that's not a permanent situation, and so I think you have to have two price points, one for a reservation that's tied to a term and one that's fully flexible and then charged based on consumption.

[00:29:38.06]   RemoteThat's exactly what we are doing and the MVP is only private offers but then we are going to have a public offer even though it runs within the customer tenant not just right away and that is going to be a payco model so you're basically paying for what you are using at any given time. that's going to be the public offer and then I feel like once we actually get to that point where we have a public and a private offer, once we have the public offer I think we have to take into account the compute notes and the number of compute notes because ultimately we want to be able to offer customers a way to... actually increase their compute without having to deploy an additional e-box, because when you have a compute node, then maybe you want to be able to deploy it within the native Kubernetes. Like you want to have C nodes running in AKS, as an example, or you want to be able to somehow grow without having to grow the cluster, because you are a heavy database user or you're using our message queues or Kafka, whatever requires all the CPUs and you want to take more advantage of that, we have to be able to monetize and take advantage of that. I'm 100% aligned with you, Jason. talking about right now is the MVP for the the ELAs basically you have like an enterprise license agreement that you're getting this for like for example UK Met Office they're probably I mean all likelihood we're basically just gonna you know charge them for a five-year term or a three-year term for whatever amount, like 320 petabytes is a lot of storage for UK-MET, and that's just going to be a private offer, and these larger use cases. Sorry, I missed the beginning. I'm just babbling here. But are we thinking about, like, once we do compute or core in the cloud that it's a fractional core then? Because obviously, like, the virtual cores in the cloud are not the same as you would have on-prem. No, of course, it's virtual CPUs versus physical CPUs. It's still many many more. Can I ask just a quick clarifying question, Fancy? I think I heard two different things and I'm not sure I think in terms of so a key thing just from a infrastructure like vast infrastructure perspective on the finance side operating side is you know fixed price versus consumption, and initially, or as of Friday, I think the initial part of this conversation, it sounded like it was all fixed until we go full SaaS offering. I think I may have just been different for you, which is actually in the earlier, like non-SAS offering, it would be fixed for private offer, but then we would want to introduce PAYGO for a public offer? Yes. Okay. So that's a difference, and what would be the timing on the public offer kind of consumption or PAYGO model? I would say six to eight months. get to the public offer. Yeah, Eiki, what would you say? Probably six months. Because I mean, ultimately, we are trying to get something to market as quickly as possible, and that is with the private offer, and the private offer needs to go to AWS, Azure, Google, OCI, likely and then we start working on the public offer and in the same time we're also working on the full-blown SaaS offering which is running in our account. So it's like a three-phased approach, private offer to begin with, get everything up, get or something to sell, then, then, then... We do the public offer for running in the customer tenant, and then we do the SaaS. - Got it, okay, and so six to eight months to the public. Okay, because, you know, the Friday call was basically about FY '27 planning, and so we just need to understand, you know, kind of what we're trying to hit during FY '27, what we'll need to have in place, for example, consumption model work, which we don't have today. No, no, absolutely. But ultimately, we also have to sit down with finance and of course, Wolfie and Tomer to figure out when we are carrying the cogs, it completely changes the overall budgeting. Different ballgame. Yep. Now, before that, even with running in the customer tenant, I'm not sure if we're going to cover it today, but we're getting very close to general availability. I would love to have an understanding on how do we want to price. I mean, it doesn't matter if we're changing the pricing or keeping it storage, I think we all agree that we do not want to go for the same discount levels that we did on-prem. These are much smaller clusters. and people are expecting to pay more so how much how much are we going to discount I was the discount approval going to work even for the private offers even before we are ready for the on-demand pricing I'm sure if we don't cover it today we need to cover it ASAP because people are asking about pricing I couldn't ignore and maybe just one quick just one more clarifying question that's helpful to us for FY27, for the full SaaS offering, as you say, Yancey, that's a different ballgame than the other two. Current understanding is that's likely not an FY27 thing. It'd be something we're preparing for in FY27, and then, you know, speaking to launch in FY28. that valid for planning purposes? It definitely is valid I believe just based on like you know the ever-changing priorities and the pressure on like for example Lior now has a very good opportunity with Zoom on OCI like how big of a... momentum shift is that going to be like but I mean ultimately I think phasing it out private offer and running in the customer tenant yeah public offer running in the customer tenant to Jason's point where it's going to be a pay go and then the full-blown SaaS offering you know And that's the sort of phase-out. Helpful, helpful, thank you, and then the other, the comment I would make, Lior, is just, broadly speaking, like I couldn't support that more. I would say, we know we're going to be phasing out. Well, it's easy to kind of optimize for the launch that's happening now. We should really think through, because we're going to have that pricing in the market, and we should think through, you know, through to the second. or third stage so we so it all aligns and will make sense plus one to that point i mean yeah you know history history matters a lot in pricing and we're going to be making history so we better make it history we want to live with exactly yeah have we yeah and i don't know if

[00:37:24.39]  Jason Vallerywe've done the analysis um in this exercise but how this stacks up to price points we're seeing from Pure, Wacka, HammerSpace, and just kind of understanding of where we're going to sit in the competitive landscape against what their pricing models look like and what we know they're charging customers. Do we have that intelligence yet?

[00:37:44.67]   RemoteYeah, we have a bit of that, Jason, in a comparison manner. But I mean, it varies. Like, for example, Vecca is way cheaper than NetApp Pure or, you know, Isilon.

[00:38:02.17]  Jason ValleryBut that's just because they're offloaded to Object, right?

[00:38:06.54]   RemoteYes. Like, 80-85% is the average for Vecca that is running on Object, and then you have offerings like... Hammerspace that is a hundred percent object. Yeah, so I guess I'm interested in understanding how their structure

[00:38:22.89]  Jason ValleryIn terms of how they're doing their model. Is it purely capacity-based? Do they have a compute component to it?

[00:38:30.17]   RemoteNo, all all the storage competitors are pure capacity Well, but but ultimately to Tomer and Wolfie's point, we are going more down the path of Databricks, you know, where they are charging for cores and capacity. I mean, let me float an idea here.

[00:38:52.42]  Jason ValleryWhy don't we look at something like a synthetic vast data unit as the charge point and then be to scale that based on the different deployment topologies and charge for something like that, and then a vast compute unit or something like that. That's how, when you look at many of the managed services at that layer kind of go, is they define some sort of synthetic unit that is tied to a license, and then they're able to scale that based on the deployment. With multiplying that feels like a natural way to think about this. I

[00:39:28.28]   RemoteBeen there Yeah, let's do that. I think it we're hard So once we go into the like first party like real steps Because it needs to align with like what the cloud how the cloud charges so I I did that with a product called Cloud Insight, a monitoring, which we basically created a managed unit, which was a combination of core memory and capacity. For example, a VM wasn't one unit, it was three VMs that was in one managed unit. There was a hundred terabyte of capacity within one managed unit and that was the sort of elements that you needed to track and bill for. The thing is it became very confusing to the customer and they were never able to do any competitive comparison and it resulted in a lot of complaints from customers having lack of clarity how much the solution would actually cost.

[00:40:42.69]  Jason ValleryYeah, I'll have to think about that a little more. I certainly want simplicity. I mean, that's the number one feedback in storage from the cloud side is when you start to... different levers of transactions and operations. It just becomes really unpredictable. I do think

[00:41:00.02]   RemotePredictability needs to be one of the core tenants for AnchorDoc. Jason, that's a big part of the TCOs we run against the cloud is when you factor in operations costs and transfer costs. You like 2 to 3x the cost per gig depending on the environment and what you're using it for. it's, it has historically played to our advantage for repatriation. Now we're talking about a different world, which actually brings me to a different question, and I don't know if we're ready to shift topics slightly, but we're having a lot of the DLA conversations now with customers who want the flexibility to deploy on-prem and in the cloud. I don't know if we have consensus on whether or not we're giving some conversion. or the price is close enough that you can deploy wherever you need to based on the needs of the time. I mean, I don't know if the Met Office is a good example of this, but it's coming out more and more, and I don't know what the current posture is, so I'll put that out as a question. Like, for example, the two-sigma deal, which is like 60 percent. on-prem 40% in Google and GCP and they their preference is to transact everything through the Google marketplace in order to burn down their commits and that is both for the on-prem licenses and the cloud licenses. So I am huge advocate for being able to sell those hybrid ELX. I think it's going to be very beneficial for Vast going forward. This was probably 40% of the deals done by NetApp as an example. you know, because customers are on-prem, there are very few that are 100% in one cloud, there's very few that are like all 100% on-prem, like every one of our customers have some level of cloud deployments, that's 100%. The scale of the customers we want to be selling to are generally ones that have both? And multi-cloud, yes, and multi-cloud, yeah. The model either has to be so simple and transparent that a terabyte on-prem and a terabyte off-prem is exactly the same, or whatever conversion matrix we build needs to be very, very easy to understand, Jason, to your point. We don't want to confuse customers. Yeah.INTERPOSING VOICES if we charge just for capacity in the cloud then it will be cheaper in the cloud right the vast license will be lower cost in the cloud when are we starting with the new pricing first of february we want to begin the year with a new model that accommodates as much of this as possible we're not changing anything we have a a soft launch, like launch right now, we're not going to have hundreds of deals, like we're just launching the product on cloud, and maybe it makes sense to not to change the pricing on cloud before the pricing is changed on prem. Right now, it's still capacity based, right? Like the same pricing is seven cents a gigabyte a month. So I would say for simplicity, we can buy a few more months to it because we need to show pricing with customers next week, the week after, right? So we can launch the pricing for cloud and on-prem together and based on the way I'm seeing it, if it's an ELA, it's somebody like 2SIG, they want to transact a big deal, then if it's a big deal, it needs to be the same pricing on-prem and on-cloud and if they transacted on the marketplace, they can use the or let's just say that we're technically figuring it out. - But when you look at customers... - But in February that won't work, right? - Well, hold on, what I think I heard Johnsy say was, if we deploy it in the cloud, customers wanna take and get their credits for how much spend they're using with GCP or UCI or whatever. license, we can deploy it anywhere. They're just purchasing the GCP. So really it's not a technical problem at all. It's a backend back office problem, right? Of knowing that that license was purchased through GCP and I'm deploying that license on-prem, right? Cause at the end of the day, they just care about their transaction and bringing down their total cost or, or hitting their. there's minimal spend in the cloud of their choice. - Yeah, and we're charging them for the allocation no matter where it's used. So every hour we're pushing in the usage, the usage sort of in parentheses, because it's always gonna be the same, essentially, and that, to us, really doesn't matter where that's. deployed. But in Polaris, the control plane of the cloud, you know, the only thing we need to know is that if there's a certain amount of percentage or a fixed amount of that, that's already been deployed so that we can manage the entitlement in the cloud. But it's really simple, but that's a given. I think that's probably solved, right? We'll be in the marketplace, we'll have private offers, we'll transact through the marketplace, so that's good. But I want to refer to, I mean, what Lior was saying, right, about keeping consistency, right, from February 1st. So let me just finish the kind of the thought I wanted to start with, right? So what I'm saying is, you know, February 1st is where we're launching new pricing let's have consistency simple so back to you Jason like let's simple is having the same strategy I guess on both like if you want to charge for compute and capacity we should do it on both now if I separate the opportunities in cloud right now there is the two seek types which are 80 petabytes huge these the consumption on cloud on those deals is going to be much much smaller their on-prem. That's the reality of where the product is. So even if we keep consistency and it's the same price for this type of deals, we're not leaving money on the table because, you know, 80 petabytes will be 79 petabytes on-prem and a petabyte on cloud. So keeping it simple is better than trying to charge more on cloud. Now, the other deals are right now going to be ELAs or private just for cloud so somebody just wants to buy a petabyte on cloud like Zoom and when somebody wants to buy just for cloud you shouldn't get the same discount as on-prem because it's more expensive that's my viewpoint same list price but it's but it's not transacting an hybrid deal of everything then it can be a different price and it's really good And when you get to pay as you grow, pay as you go needs to be more expensive as well. Because pay as you go is just you pay when you use, so you're going to pay more money for usage, but you're not going to use it all the time. I think that's kind of the logical separation. I agree with everything you said, Lior. So the problem is that if we want to keep consistency on February 1st, will charge without setting you know like a number of cores or whatever right everything is on everything on prem is going to work right everything in the cloud is gonna i mean it's going to be skewed by the high number of cores right so so the the price on the cloud will be outrageous because it has hundreds of cores that that the user don't you know doesn't need yeah so i agree so that's the issue right and if we charge only for capacity let's say like we do today then it will be cheaper in the cloud because february 1st on-prem customers will pay a significant amount a significant portion of their license fee on-prem will be cores right so a small part will actually be or not a smaller but I mean depending on the configuration but but let's say 50% will be cores 50% will be capacity if we charge only for capacity in the cloud obviously we're charging over them 50% yeah but I mean ultimately then we are down to which I kind of I'm okay with but then we're to option B in your, by having a fixed set of cores in the cloud per petabyte. We also have to scale it down a little bit because there are going to be customers that are doing, you know, 100 to 300 terabytes, you know, and that. we just have to have you know portion early like have a cpu count per 100 terabytes as well yes yes so i mean the the cores per petabyte in my calculation is uh is actually just a relative right so so if you get half a petabyte it's half that number right that's how and how we've been doing it today with core credits and things like that.

[00:49:57.04]  Jason ValleryBut then do we limit them, like this was the conversation we had last week, are we going to disable the cores that the VM has above whatever we're including in the ratio, or do we let them use them?

[00:50:09.39]   RemoteSo this is something I will need to run, you know. scope on on the engineering side right with the with the effort involved in doing that right and shutting down course and I don't know if we'll get to do that we shut down course yeah I mean I think it's the bad guy move I think it

[00:50:32.99]  Jason ValleryGoes back and we're kind of talking in circles but then it goes back to saying yes to be trusting for course

[00:50:39.94]   RemoteThey want to use it and they can you can unlock them no but if we charge for example when you have a thousand and we disabled 700 you're not paying vast for a thousand you pay 300 and you're getting what you paid for you could make the argument that the instance is more expensive and you're not benefiting from it but we can say you know these scores are not needed And that's why they're disabled. This would be crazy because ultimately we would be rate limiting our customers. Like customers are already paying for the instances, and they're paying for all the cores, and all of a sudden we're going to disable. I can't express how much I'm against this. I'm also against disabling. because it's extra engineering work for for mechanics of pricing right and I rather have engineers work on real stuff and rather than then you know fixing core pricing issues. Homer I got a dumb question here if we did that can the customer use those cores for other things not Vast related? I just want to let me answer Wolfie here. They cannot because we are completely managing that instance and the infrastructure behind us. Okay. So they cannot utilize those cores for any other stuff. It does put pressure on the cloud providers to... and not that we have enough pressure to put to even be relevant, they could laugh us off, but to look at more optimized shapes, right? I mean, this is, in an ideal world, that's where we get to. Somebody made the comment earlier, I forget who, that they have an uncompetitive, vast offer with shapes that have an abundance of unnecessary course. - But if you think about what you're doing too, is you're now saying. that cloud one will have better performance than cloud two for the same shape and the same price. So even though we're getting our price per petabyte the same, I will perform better in the cloud that has more CPUs that are available to my instance in which I'm deployed on. So I will have a better performance cloud, which is okay. Because then it will force the other clouds to do something better. But it will also break down on what we do when we do a reverse TCO for customers on which cloud should I deploy in. For those customers that have two clouds that they're trying to burn down their costs in, they're going to say, "Where does VaST give me the best performance?" And it might be GCP, based upon the shape in which it's deployed. they've given us. The option of limiting or setting a fixed number of cores actually I think would go against our interests in with cloud providers because we'll set the same price across everything and let's say Azure that has more cores which we don't need customers will be able to use those say for engine or whatever for free right or they get more performance right so so you get the same price on aws and same price on azure azure is more uh more performant and you know you're not paying for that performance so there's no incentive for azure to fix that right but it is an incentive for a for the other clouds to fix it but we don't need that's the thing right we we don't need all those cores that's the problem fix this meaning the opposite meaning reducing the number of cores per capacity i mean so i can see a middle ground

[00:54:12.25]  Jason ValleryBetween what we're talking about here putting this decision back on the customer so ultimately they get a fixed amount of cores per pebble bite that comes with the pebble bite license and if they want to unlock the rest of the course that's an optional price point to enable use of all of the cores and then you kind of push the decision back to them we're saying you know if you want to use them you can but there's an additional charge to VAS to be able to unlock them

[00:54:34.83]   Remoteis that the middle ground here? There is a small problem another one is that we sometimes use old VMs like as an example AWS we are using i3 it's a third generation, the 8th generation. The cores we're using are very slow and very old. A core is not a core, right? So when Roni did testing on the Z388, even though there were enough cores, the over-provisioned cores, he said that we're slow, we're not getting to the performance levels because there are not enough cores. So, cores are the same. Yeah, so what I'm touch limiting the course and we have right now zero zero around zero leverage of convincing the hyperscalers on building stuff for us. The reason is we do not have a product. We've been talking with them for two and a half years making noises and we have zero business. Zero zero zero business on cloud. Microsoft are the first to finally open up. and they're still doing it with zero of us running on Azure. So once we have business, it will naturally come, they will start building more optimized VMs. Right now we're just writing the database VMs, that's the story. So I would say, don't touch the core, don't limit the cores, let the cloud deal with it, you know, and customers are very opinionated about which cloud they wish to use. So the question is, are we going to be good enough competing against the other players on Azure? It's not that they're going to compare us to AWS and then move the use case from Azure to AWS because of us. They know exactly where they want to run their use cases. They won't change it because of our pricing or our performance. Yeah, I agree with that. Let's not be that arrogant that we control. where they deploy. They have already made their cloud decisions, and we have to, you know, abide by that, and, I mean, I feel like we only have two options here, Tomer. we go with just pure capacity and have the comparison or we go with your plan B which is we charge for fixed amount of course per petabyte or terabyte.

[00:56:54.19]  Jason ValleryHow many customers are we likely, what we're doing is we're setting a precedent for the customers that get this via private offer, and maybe there's a risk that because this is a small number of customers and they're large enough in scope that we know we're going to get favorable deal terms and we still have retained flexibility to revisit this decision when we go to a public offer. So maybe that's OK.

[00:57:16.30]   RemoteBut I mean, ultimately, if we if we go for the comparison. like for example if you do it just to azure preview files or azure narrow files or elastic files or efs we are even with this model of seven cents list price we are in aws and google cheaper than those alternatives from a storage perspective In Azure we are not because we only have 23 terabytes per VM. So you see like how important density is and there is like this weekly design review with Ren and Shakar, Asaf, me and Eiki. on trying to figure out, you know, how we use persistency in object or on EBS or Azure premium SSDs or Google hyper disks. But I mean that is going to take a while to actually re-architect the full solution, and Renan is talking about this needs to happen in order for us, even on-prem, to be able to be more software defiant and go on top of any hardware. So he feels like an overall redesign is needed. But how long is that going to take? 18, 24, 36 years? months. I don't know. But at least we cannot wait in the cloud for that. I think that's the big thing. We need to get going with something. I think the arrogance comment was spot on, and I do think, if I give my opinion here, the introduction, of course, in some respect, I think is an important concept. for customers, both on-prem and in the cloud, and if that's a fixed ratio that kind of neutralizes at least vast pricing cross-cloud, while it doesn't address the infrastructure shaping congruity, I think that is important because it is now centered to our pricing methodology and the product direction that we're going in, and to ignore it completely, I think. might be a miss I could be convinced otherwise but this is what my guts telling me and I think you know later you said something earlier that I want to put a pin in I don't think it's for now because again it just gets too complex but this notion of processor speed right if we're on a third generation that's not the same as the eighth generation if we're looking at Turin versus Ice Lake is not necessarily a core, and that has a big determinant on how customers build clusters. A core two years ago and a core two years from now, we're not the same, even though our pricing model has us charging the same, and so we may leave a lot of money on the table. I'm going to go on prem for a second, right? If customers go modernize their compute estate and dramatically their core performance if their performance needs aren't growing or outpacing the improvements in processor speeds, right? So there's other dynamics that we should get into at some point, I don't think now is the time, but these are considerations that we should be a year or two ahead of once we launch a product and get some customers using it. We haven't even talked about function as a service as well like that in that in the cloud should be native cloud functions not like kubernetes cluster you know our whole topic we need to figure out i i agree figure as the direction first before we execute but uh yeah assuming that we'll You know, a customer that has access to Lambda will run function on VaST because it's better, right? I mean, that's arrogant as well.

[01:01:18.94]  Jason ValleryLet me just comment.

[01:01:19.79]   RemoteUltimately, we are literally going live in like one and a half weeks. Two weeks. Two weeks. Yeah, in Google, and I agree with Wolfie that we should have consistency and we can introduce the core pricing. But we need to make a decision this week, if we are going to go down that path. Because we have to define each one of the... the entitlements in the offer, and it can literally take three weeks of an approval process to make changes to the pricing schema and the entitlements from the Google Marketplace Okay, so the decisions, we need to decide if we're staying with price of capacity only until the 1st of February or we switch to capacity. Yes. No, no, I mean, the first, I mean, this is just like we're selling on-prem with the current price model, pricing model, I mean, until February 1st, nothing, nothing changes. No, but, but on, okay, but on cloud. we said that we're not charging for compute at all. That was the decision. Okay, so if we say that it's only capacity, we're not charging for compute, it will change on the 1st of February. So we can share the fact that, you know, it's pricing only until we change, like, you know, we can be very clear. So that's the first decision, and then I would say if it's an hybrid approach, meaning somebody wants to transact, they use deal on the marketplace, then the pricing discount is really with the accounting. So to succeed, we need to decide what's the discount for an 80 petabyte deal, even if it's transacted on GCP, and if it's a transaction that needs to happen on the marketplace for cloud consumption, meaning burst, we need to have an agreed-upon discount structure that isn't 99% discount. okay that's what we need to also agree upon so yeah so we're lowering because we're changing the dollar per gig for on-prem right i mean we're changing the the capacity skews and like everything is on the table the discount levels for on-prem are gonna drop and so we need to see if if that drop is you know going low enough for you know for the class and we can keep it consistent even on the discount levels, or we need lower discounts in the cloud. I mean, but it all depends on the dollar per gig we want to get to. Lower discounts, that's for lower discounts. Just one note, Tomer, on that. It's one thing to lower the list price. in order to have a less high of a discount, which is generally a good thing because the 99 masks, you're like, oh, it's a point, but the point actually hurts us a lot. But the more important thing is to have consistency at that level. So to actually have, between similar customers, have consistent pricing, because that's what ends up hurting us. the actual discount number like the average discount number coming down is great but it's more around consistency at what we don't control with the price let's have that conversation in a different forum yeah we have a policy that's yeah go ahead go ahead what i'm saying is you're changing the lease prices so you don't you stop get into 99% and you can discount on a normal level, but you're going to end up with the same pricing, which is roughly 90% discount average today. My argument is that for the on-demand, again, not the big deals, right, for the on-demand hundreds of terabytes consumption, customers are going to go and create a private offer and they will happily pay two times per gigabyte than what they're paying. on prem, right? There is no reason to go that deep on our pricing because they will pay for hardware, let's just say, 20 cents a gigabyte a month, and if our software costs them 2 cents a gigabyte a month for 3 cents, it's within the competition, it's within normal numbers on cloud. Now, if one of those customers, let's just say, jump trading, will say, "Hey, this is too expensive and I want to get additional discounts," we can always end them there. that. But I just don't want to leave money on the table when we start quoting public cloud native, right? Not those hybrid deals, and that's what we're looking for advice. I'm saying whatever least price it is, it cannot be the same behavior as on-prem, because then we will just destroy our pricing on cloud from the get-go. We need to define a discount structure that makes sense. those deals on cloud and it's less aggressive than on-prem. That introduces a conversion that needs to take place if we have customers writing ELAs that include both. If it's a dollar on-prem but it's two dollars in the cloud then a terabyte becomes worth half as much in the cloud as what you're and we're not necessarily writing ELAs that way and I don't know how customers would respond to that no so I'm saying just one last no comment and I will shut up I'm saying something else I'm saying it's okay if somebody that's my idea right not saying it's right if somebody comes with transacted in tomorrow and he to transact on the marketplace or even on prem and it's a huge deal we're signing one of those huge deals 80 petabyte whatever then I think the right way to approach it is to if he wants to do it to enable him to use the license wherever he wants to use the license the vast story even if it lands 5% on cloud 10% on cloud same pricing so for any old deal that we already closed or future deals, if it's an hybrid deal and it's one of those, you know, the hybrid deal and it's a big deal, then it should be the same price, and that's simple, nothing needs to change. What I'm saying is that there are going to be other deals of our customers, existing customers, that will just want to transact, a marketplace transaction to burst into cloud. Outside of the the LA, despite the toffers, and these deals should not have the same discount because that's what I'm saying. Now, if somebody signs a deal tomorrow, which is an ELA for everything, let's keep it simple. Let's let him consume on cloud or on prem. Let's not change that. Not no behind it. Yeah. But I mean, ultimately, ultimately. Wolfie, we are always going to be consistent on the software licenses, which basically is the revenue to vast on-prem and in the cloud. The TCO looks completely different, of course, and that's just due to the cloud infrastructure cost. For sure. For us, to quote an ELA, we can always apply the same software licenses to VaST, whether it's on-prem or in the cloud. But if we want to see the TCO calculations, we have to take into effect, you know, that you need employees to run your on-premise data center. feeling, depreciation, all of that, so I mean that's just an exercise that we have to have in the world. Yeah, so do you have another solution? Sorry, Tom, just one quick comment on that. We will need at some point, I think I shot a note off last week, Tom, I don't remember who I even put on there anymore, but the ability for a customer to see their 100 petabytes and pull that down to an on-prem or cloud deployment, and I don't know what that means, whether it all has to transact to a marketplace to be able to do that in the cloud, or there's some other mechanism with Uplink, you know, for hybrid customers. But just kind of put that to the side for a moment from a development perspective, how do customers actually deploy? with these hybrid ELAs, many of which we're writing now. So food for thought for later. - I mean, ultimately Polaris should be the one that is monitoring those deployments, and of course we are syncing everything in Polaris into AppLink. So customers will see all of their clusters running on-prem or in the cloud. So, I mean, this goes into the registration of customers. You know, how are the customers registered into AppLink? We can't have one customer name or like a UUID for on-prem and another one for cloud. There needs to be a correlation between, so we actually have a clear view. of all of their data estate? So I have I have a proposal I mean if we are going you know to begin with right we are going with a fixed number of cores for for the cloud calculation to keep things consistent or whatever that that number that fixed number of course could be on the And what I mean is that for on-prem customers, right, I mean, they have a choice. They can build a faster cluster or a slower cluster based on the Cs and Ds. I mean, that obviously changes for Ebox, but even for Ebox, they can select the number of, I'm sorry, the drive capacity, and that's defined, that would define. in the capacity per core ratio, right? So stop me if there's no, if you're not following, but what I'm saying is that a customer could deploy a cluster with a number of C nodes to get to 40 cores per petabyte or to get to 1,000 core per petabyte, right? Depending on the performance they wanna. So let's say the high performance and small series box, we'll get them to 640 cores per petabyte. Now we could say that everything they want in the cloud is on the fast side, right? So we could say, no matter how much you like, how you define your on-prem cluster, like this or like this or whatever, right? In the cloud, it's always 600, right? Because-- - That makes a lot of sense, makes a lot of sense.

[01:11:49.88]  Jason Vallery- Where's the diminishing returns in terms of, as you add cores, do you actually get more performance or are you network bound? Like, you know, there's gotta be a threshold of core to Peppa by ratio where more cores don't actually improve things.

[01:12:02.12]   RemoteAbsolutely, and with the Epochs deployment, Jason, to your point, which is 100% correct, we are... Especially in the cloud, we don't have... In the cloud, we are going to be limited by, you know, the 100 gigabyte network for the east-west traffic in the Epochs deployment. So even like from a storage. perspective, we're not going to get much more performance with additional cores because the bottleneck is actually going to be the network.

[01:12:33.05]  Jason ValleryYeah, and then there's probably across the providers, I'm less familiar, but I'm assuming the different instance types across providers will have different networking characteristics.

[01:12:41.40]   RemoteAbsolutely, and I would love to get to what Tomer is saying. explaining in the cloud as well that we support multiple different instances based on the performance characteristics that the customer needs for each one of those deployments. I think that is where we need to go, but right now we support two instances on AWS, i3 and i7. One on Google and one on Asher and that's You're right so maybe taking Tomer's idea a step forward we look at the VMs we support and We qualify them as the right tier based on your table So we pre qualify them even though they might have much more cores because of the network. They're actually not performance, so we just qualify it and then our list price is very simple. If you're buying us right now on cloud and you're using the i3, which is a third generation very slow VM, then it's not a performance tier. If you're buying us on cloud with the i7, which is seventh generation, then it's a performance tier. So we can just tactically define those VMs for the right tier and then use the number of cores of one prem to apply it to the cloud, and then we don't care about both cores and clouds.

[01:13:56.62]  Jason Vallery- This is the pricing model I was advocating earlier, which is, and this is what object does at Microsoft, is it's a throughput per petabyte issue. Like you get SLA, SLO of throughput per petabyte stored, and you have different price points if it's running on premium hardware, standard hardware, and so that, this idea continues. into the world, you get to offload, when we're putting this stuff into Blob anyway, you sell it at a cheaper price point because the throughput per petabyte is less.

[01:14:25.45]   RemoteSo I think, I mean, you already took it a step further, and I agree, the lines were what Jason was saying earlier, and it goes, I mean, if we distilled it further, it's actually looking at the, it's coming up with some of a ratio or vast units right which there's a strong objection from from Brandon and Jeff not to do that you know I've tried I mean there's we could try again or whatever but but what what we can do here again to keep things simple is to assume that every instance um no matter where or which generation can saturate the network it has, right? And it doesn't matter, and that's why, you know, an instance that has 1,000 cores, I'm exaggerating, right? We won't charge you for 1,000 cores, we'll charge you for 300. If it has 400, we'll also charge you for 300, right? So the normalized number is gonna be lower than everything, you know, no matter where we are, right? so lower than the total no matter on which cloud we were, assuming we're saturating the performance at hand, and we price based on that performance saturation, right? So we're always at the top tier, even though there's some spare cores, and in practice, what will happen is that customers on-prem most likely won't get to that ratio right so cloud will be more expensive but tomer if you go to your spreadsheet right now and correct change change you were playing around with 200 to 300 but if you add everything to 640 what what's the list price for our Yes, okay, so I have to $10, $12. Yes, so I have to but, but then, yes, so I mean the idea is that it will be more expensive we say we use the same discount level, then we keep it more expensive in the cloud versus on prem. If we can get to that to that ratio right that's a theoretical. Now I didn't explain that there's numbers here. are actually the vcpus because it's really just the instant calculation so i i've cut it by half for the for the dollar calculations but this is the vcpu numbers so i have to double that if we want to equivalent it to the ones on top we need to double um double the numbers here so so what you wanted to put, Yonsi, so let's say we go with the - if I go with a high performance one on, on, on. Yeah, so that will be like a 1200 here. Yeah. So, yeah, because you are dividing by two. Yeah, because here, the core price has, I'm sorry.

[01:17:22.44]  Jason ValleryTo throw out another variable to look at, at least on Azure, and I don't know if the other clouds have this, they have something called an Azure compute unit, which is a way that they normalize across generations to say perceived performance versus just CPU count, to go back to that statement of like, generationally, you're going to get more efficient cores. and later series. So I don't know if we want to actually look at that versus vCPU. Have you looked at the ACU metric that Microsoft publishes?

[01:17:50.46]   Remote- I have not, but I don't think, I mean, it's not something we can count reliably, you know, consistently across clouds, right?

[01:18:01.82]  Jason ValleryRight. It is an Azure unit for generation.

[01:18:06.40]   RemoteYeah.

[01:18:08.58]  Jason ValleryBut vCPU is the same, right? Because they're different generations of processors as well. So it's still an Apple store. It's a mess. Like, if I think about this from a first principles perspective, there's three things we're controlling. There's realized customer value. there's the cost profile and the margin profile, and ultimately like where we shift the risk of optimizing these things is the decision of if we try to control all of that with our pricing model, I see us pushing towards a synthetic unit that then we can apply the variances that are inevitable between generations, I mean we haven't even talked about the variances between regions and SKU availability and clouds, and so if we're trying to control all of those variables, then ultimately it means we take control of our own synthetic unit. Or, if what we're trying to do is standardize on just the value of the software and then letting the customer be exposed to all those variances and deployment topologies, then that's a different model. So it's really, where do we sit on that spectrum? Are we trying to consistently get a pricing model that always recovers maximum margin and revenue to us, or are we trying to empower the customer with all of the realized value that the underlying hardware provides? And that's, I think, ultimately, which will drive us in one direction or the other.

[01:19:31.43]   Remote- Yeah, I mean, what you said, I mean, the answer is it depends, right? Because if we build a cluster today and the customer uses it for storage only, no matter what they do, they won't realize the value of all the cores in that cluster. But if they're gonna use, I don't know, Kafka ingest, then they have cores for that, right? So they will realize that value. We're assuming that it's the former, right? Because we're addressing storage use cases first, high-performance storage use cases. So we can saturate the links, right? We can generate the throughput with. Part of the cores in the in the instance and that's why I mean the logic right in the explainer will be okay Well, we you have a thousand cores. We only charge you for 600 - So when are we going to get-- - Everything you're saying that we were advocating for, it's, yeah. - So we are launching in two weeks, and we need three weeks to be ready to have everything ready for the marketplace, which means we are behind already, and then all the quotes that we have in the field right now are just using pricing from on-prem, because we had to price stuff, and people are asking for TCOs, like NBC. is asking for a TCO comparison this week to be done against Kumulon on Azure, I need decisions, that's what I'm saying. We can change the decision, it's going to be limited to 50 opportunities give or take between now and the 1st of February, but we need to decide and based on that decision, I want to communicate something that is consistent. So, maybe it's important. It's important to keep consistency with where we are today. I mean, the vast field is not familiar with the upcoming changes and whatever. We can't introduce, you know, a newer pricing model in the cloud compared to, you know, the broader, right? So where we are today with the cloud pricing with just only for the capacity for the 70 or the 69. 965 per 100 terabytes, right, per month. I mean, that's the pricing you launch with, in my opinion, right? Unless someone thinks other ones. In February, we do what we do. Now, when you say launch and private offers, so GCP is already set up like that, right? The private offer in GCP is-- everything is launch is coming with Yonsi saying that launch is ready and this is really a Polaris launching to transact everything through the marketplace with full automation of the private offer this is different than what we're doing today it's not a placeholder anymore so I agree on the yeah keep it the same as what we communicated before and changing the first of February. It's not my decision to agree, but we need a decision there, and I also want to have a decision about the on-demand deals and what's the discount structure that we're going to direct the field to follow. So it's not the same as go 99% discount, right? So that's the second decision, and once we have those decisions, then we have much more time to decide. discuss the right strategy for the 1st of February. So maybe I will suggest a pricing structure for those on-demand deals and you guys can review. Kind of based on the size of deals I've seen people asking for, which are hundreds of terabytes, again these are going to be private offers for multi-year so that's not the on-demand we don't have on-demand right now on our license we're not ready for that year I think Jonsi said eight to ten months before it would be fully operational to do PSU grow so yeah I said six months but uh you know we have to be able to do like the other thing is We are, of course, working on the LFTR program as well, and that has certain requirements for a SAS offering and specific API translation from BaaS to ARM. So we are working on all of that at the same time, and of course, if it comes up-- AWS FXX vast that is going to change it as well you know the prioritization and how what we need to deliver yeah okay just phase one we're launching the only thing we know yeah the phase one we're launching marketplace lifted isn't ready it's it's just phase one between now and the first of February, it's marketplace, it's private offers, it's a multi-year lease, and we need to have a pricing strategy. If we don't set a pricing strategy, our field is going to use the same discount that they're using on-prem, and it will be too late to stop it. We need to give them a pricing strategy out of the gate. Elmer, why don't we take a look at the deal? you have on the table. If you need pricing guidance at this point, we're probably running a TCO one by one. If they have an on-premise deployment, that makes the conversation a little bit easier, but depending on who we're competing with, what cloud that they're going to be in, that's going to determine how much we think we could charge. We should get as much as possible without pissing off our customers. I think that's always true. That's the best So, why don't we spend a little time if you've got questions? Yeah. Okay. Eric, we start with NBCU. That's the next one. Yeah. So, I'm just going to include you. Eric, are you the go-to person here? Me and Zola spent a lot of time with that team, looking at other things. When I talked to them last, they were pretty underdeveloped in their TCO work. Maybe you've got them further along, so let's get together. So, one thing that is kind of unique right now with NPCU is we have actually gotten their research pricing from Azure and from AWS, or what their actual discount is. That helps us tremendously. You know doing the TCO calculations because I mean if you're doing reserved you can go up to like 70% discount So that changes the game tremendously That's good information to have Yeah, let's take a look I'd say if you want to be on that maybe it's a good learning opportunity for all for all of us as we go down this path. - Yeah, absolutely, and we have a meeting. Tomer, there is this TCO sync with the account team. There are two meetings today, well. I don't know. Let me add you guys into that discussion, because I think that is going to be a really good exercise when a customer, like NBC is moving everything into cloud. Then, you know, I think it's a good exercise for us to be, you know, Jason, Leo. Aki, Tomer and Wolfie. I mean maybe we should just use this as an exercise because the customers are asking for the TGO calculations and we need to share that with them on Wednesday. We've done something similar to test this model, right, for on-prem. We need to do that. for cloud as well, so I agree. - So can I ask you back to the-- - We may be hearing different things from the account team, by the way. When I last met with them, their champion was trying to keep everything on-prem and they were building out an on-prem. Now we know that they're doing a lot in the cloud and the management directive is to go there, but I think we're hearing mixed things and there's different internal stakeholders at NBCU that want different outcomes, so. I don't get where you're going as long as it's VaST, but let's make sure we base it on... So Armando, the guy that is the VaST champion, he wants to have on-prem, Azure, and AWS, and he wants us to demonstrate that, because the NPCU cluster on-prem, or the POC cluster... cluster is there now and that's why we are we managed to move them from on from Azure to AWS as a POC to on-prem to AWS as a POC because we actually have the maturity there in order to take this forward.

[01:28:31.59]  Jason ValleryLike the data is cached in the cloud, or how do they think about it?

[01:28:36.71]   RemoteSo, the way they think about it is, outside of Armando, who is the vast champion there, they're thinking about, you know, replicating between Azure and AWS. specific data sets for video distribution and all of that.

[01:28:59.48]  Jason ValleryHow do they think about then the egress fees as it relates to TCO?

[01:29:04.10]   RemoteI precisely brought that up and they said they have that handled. I don't know how, but the egress charges from AWS and Azure, when you're actually leaving Azure, to cross-region replication, you're actually going to a different hyperscaler, it's going to be immense. I have a lot of thoughts on this topic we can probably take for a different conversation. So that call is at 11am Pacific today and maybe we should all join that party as well. So Jason, do you want to be on that TCO call as well?

[01:29:41.74]  Jason Vallery- Yes, he said 11 Pacific.

[01:29:43.35]   Remote- 11 Pacific, I've asked the account exec to send an invite. So Eric, you Jason, Lola, whoever wants to join.

[01:29:52.29]  Jason Vallery- I have a conflict to end unfortunately.

[01:29:54.35]   Remote- I need to drop by the way. Take care, everybody. Thank you. - I also need to drop another call. But anyhow, you don't see, maybe forward it to people. - Yeah, I'll let everybody do this. Cool. - Okay, bye. - Thanks guys.
```
