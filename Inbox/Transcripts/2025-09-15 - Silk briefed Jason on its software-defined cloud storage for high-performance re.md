---
entities:
  customers:
  - '[[Silk]]'
type: transcript
source_type: unknown
date: '2025-09-15'
---

# 20250915 0800 Teams Meeting (Parallels) Transcription

**Date:** 2025-09-15  
**Participants:** Jason, Chris, Tom

## Summary

Silk briefed Jason on its software-defined cloud storage for high-performance relational databases, highlighting rising AI-driven load on systems of record. They discussed Silk‚Äôs architecture, performance limits, and approaches to protect production DBs (accelerate prod vs instant copy). Silk requested RDMA support on L-series to reduce CPU overhead. Next, an intro to founder/CEO Jay Menon via Ong is expected; Jason was asked to keep Silk in mind for customers exceeding Azure native performance.

## Key facts learned

- Silk is software-defined cloud storage optimized for structured DB workloads across Azure/GCP/AWS.
- Data pod supports ~1‚Äì500 databases up to ~1 PB with n+x resilience and read cache (cNodes).
- Sub-millisecond latency; tens of GB/s throughput; ~40 GB/s per host observed with Boost VMs.
- Current limit ~2‚Äì3M transactions/sec; typical IO size discussed around 64k.
- Durable layer via PV2 with added erasure coding or UCL series using ephemeral media; data always compressed; optional dedupe.
- Often underpins VM-based file shares; sometimes competes with Azure NetApp Files and claims better cost/perf than ANF Ultra.
- AI/agentic access is unpredictably increasing load on production relational databases.
- Two patterns: make production DB as fast as possible vs create near-real-time instant copies (tens of seconds lag).
- Feature request: RDMA on L-series front end to cut CPU overhead; no clear Azure timeline; working with Gal‚Äôs team.
- Reference customer domains: trading, retail, distribution, healthcare; peak spikes (e.g., market open).

## Outcomes

- Shared Silk architecture and AI-related production DB use cases.
- Aligned to pursue an introduction to CEO Jay Menon via Ong.
- Jason asked to consider Silk when native Azure storage cannot meet performance.

## Decisions

- Proceed with connecting to Jay Menon through Ong.

## Action items

- [x] Coordinate introduction to Jay Menon as suggested by Ong @Ong ‚è´ ‚úÖ 2025-10-26
- [x] Advise what topics Jay Menon cares about to tailor the session @Jason üîº ‚úÖ 2025-10-26
- [x] Follow up with Gal‚Äôs team on RDMA support timeline for L-series/Duo Boost @Silk engineering team üîº ‚úÖ 2025-10-26
- [x] Share customer opportunities that exceed native Azure storage performance with Silk @Jason üîº ‚úÖ 2025-10-26

## Follow-ups

- [x] Schedule meeting with Jay Menon after intro @Chris üîº ‚úÖ 2025-10-26
- [x] Prepare concise real-world use cases highlighting real-time vs near-real-time DB access needs @Tom üîΩ ‚úÖ 2025-10-26

## Risks

- AI agents can overwhelm production systems of record (single-VM DB throughput limits).
- Lack of RDMA on L-series may cap performance and increase CPU overhead.
- Azure native storage/network ceilings may constrain high-performance single-host database workloads.

## Open questions

- Which use cases truly require real-time access to systems of record versus tolerating tens of seconds lag?
- Is ~2‚Äì3M TPS at ~1 PB sufficient for target scenarios, or is higher needed?
- What are typical transaction sizes and patterns for AI-driven access?
- When should customers accelerate production DB versus use fast near-real-time clones?
- What does Jay Menon care about most to ensure a useful meeting?
- What is the Azure timeline for RDMA support on L-series/Duo Boost?

---

## Transcript (auto)

```text
[00:00:00.00] Remote : I'm literally boarding a plane, so I'll probably have to go on you today in a sec here, but I did not want to miss. the opportunity to connect with you. The guys are joining as well. Oh, here's the first one. Is there a lot of background noise, Jason? Okay, okay. Super, yeah, you're loud and fair. Hey, Chris. - Hello. Hi, Jason. Is it? I don't know because I just landed from Boston to London this morning and my body doesn't know what time it is. I've no idea. It's actually, in a way it's too short because they give you a meal and then they weigh you up for breakfast. you get about two hours sleep you say tonight five and a half flowers I was flight yeah anyway I'm gonna try and put on my brain face and pretend that I'm not falling asleep yeah put on a put on a pink dress what time are you where are you Jason where about somewhere in America or something. Oh, okay. Yes, we are facing some. We are facing some. I Am traveling from Miami to Medellin, Colombia of all places. Yeah That's a big good trip Personal trip Who knows, there might be a potential customer in the group. But no, it's a personal choice. We don't have a lot of business in Columbia yet. - Well, good morning, sir, Jason. I believe it's very early in the morning for you. I'm not far from London, which is why I talk funny, but not as funny as Chris, and yeah, it's early afternoon and I've just come off a call with my CEO. So you'll have to wait a moment before I can talk sensibly. No, not too bad, but he's very high energy in Tennessee. What have I missed in the discussion? Apologies for being late. Oh, we were just, exactly, just lamenting my jet lag, Tom. Yeah, so you've got your... excuse it in the first. Yes, exactly that. Now, okay, let's crack on. So, Jason, thanks for meeting us. You've just come back from Sebastopol, I think J√ºrgen told us. Wow, okay. yousilence I mean, it's quite a coincidence that Juergen has made that introduction. So, I think I said in my message, but we had a call with Ong last week, and then one of the outcomes of that was Ong said, "Oh, I want to introduce you to a guy called Jay Menon." until then we've never heard of or spoken to so uh you've somehow reached both of those people in one one fell swoop fungible yeah Okay, so what do you know about us? I'm glad I brought you along because I already would have been out of my mind. depth here if I try to do this on my own. Okay, so Jason, question for you, which of our competitors are you most familiar with so I can do a contrast? Okay, that's interesting. So we probably wouldn't expect ourselves normally to find in the same opportunities as Vaast and Weka because we get used for different use cases. So as you say, we're software-defined cloud storage, we do everything we can to take sets of resources and it doesn't have to be ephemeral media. We have two different architectures and I can show you a picture in a minute in case it helps, but we have two different architectures that allow us to combine lots of resources in Azure, Google, AWS. together as close to the machines driving the workload as possible to give a very low latency, highest possible throughput response to the workloads but in a very durable way. So we're relatively inexpensive so we're not we're not typically used to like HPC huge hundreds of petabyte type configurations. We mostly get used for a single source of records type workloads, databases, almost always used for databases. So we scale up to about a petabyte in a single configuration, in a single zone, and we can take workloads that drive in the tens of gigabytes a second at sub millisecond latency, and I guess what's most interesting for us is that we're finding a lot of customers that have deployed Silk already are starting to see significant increases in load hitting their Silk systems because they're starting to have AI workload. whether it's rag, agentic workloads, different models that build inferencing sets that are now needing to access their single point-of-truth relational system. It's for a lot of the use cases that our customers are talking to us about they're they're saying it's not good enough to be pointing the the models at the second copy the data in the data lake house etc they want to expose their relational system to the agent or in fact a lot of the time they don't have control if they're if for example our retail customers or our distribution customers or are exposing their capabilities on the web they're starting to see workloads increase unpredictably from different AI-based sources of access. Does that make sense? Am I talking nonsense? So let me share my screen a moment if I can. Do I have permissions to share? Interesting. It's saying 'Protected Meetings'. Yep. Okay. Yep. OK. Now the button's gone grey now. So it looks like there's a button. share button is currently non-operational bear with me let me join exit and rejoin they know it's also non-operational for me when I joined it was operational now it's grayed out so I don't think it's useful maybe I possibly have yeah can I share Tom why don't you share Chris you have to send me the link what you want to share There's nothing I like better than being your PA. Well, it's nice to get you to do some work. you want can you open that file for sure Chris? Where have you shared it with me? Sorry, just give me a second. What's that? Just to confuse you. What's that? That's not, okay right, there we go. Bear with us Jason. Strangely, I think my IT people noticed I was part of the Microsoft organisation last week. week and removed me yes which I was not pleased about and then I don't know why I was ever part of the Microsoft organization so curious right okay I have the I have I have the ability now and I'm gonna give you this don't do slide 20 slide 20 in presenter mode here we go and let's do that oh no that's the perfect big button under that i'm hoping you can see my screen i can see your screen okay How far do you want me to build it out, Tom? It's not updating for me, I don't know, Iago. So, our architecture is designed to support somewhere between 1 and 500 databases in a single data pod, and that can be up to a petabyte of capacity. By default, we test up to eight of eight nodes in our performance layer, but we can go much higher should we need to in terms of the performance. We used to do benchmarks with 64 nodes in the performance layer, for example, and that performance layer has an element of cache in there that you can use. a read cache. It's deployed in a n+x architecture, so it could be n+1, n+2, n+3, it depends how resilient the customers want it to be. So we can run where pretty much all the data is in the read cache of those cNodes and they can be positioned as close as possible to the database. workloads so they can be in a PPG with the database machines to get the very lowest latency and the maximum amount of throughput and then we persist the data either as you said to a set of VMs using the ephemeral media or we persist it to a set of pv2 volumes that are protected. with our own erasure code, but we always compress the data and we optionally de-duplicate the data. So, almost always, the limiting factor of performance for any of these workloads is the physical throughput of the database machine that's talking to us. So, we're very keen and like the new capabilities with the Boost VMs, where we can demonstrate just under 40 gigabytes second of throughput from a single host, which means that we can get the, I guess it's different from the very wide scale-out models we get most used when you have database environments that rely on the performance from a single virtual machine. Yes, it's a structured data. We don't do file level sharing on our machines. We quite often get used to underneath file share layer running. on virtual machines because actually the file sharing capabilities within modern versions of Windows is very comparable to or in a lot of respects better than some of the NAS providers, the sort of legacy NAS providers. We do find ourselves fairly often competing with things like the ANF offering, and we can build very cost-efficient solutions that will significantly outperform the ultra configurations from ANF. Yes, all. So we choose one of the architectures, and we either build the UCL series to make our durable layer, and it ends up being probably not as durable as the PV2, where we add extra erasure code protection on top of the excellent durability of pv2 but it's still very reliable and it's used to run various financial trading sort of organizations workloads so it's proven to be trusted in different as your regions across the world for all of our customers. So, I guess we're thinking, at the moment, where the main impact that AI is having on the deployments we've got already and the customers are finding us for the use cases that our architecture fits today, is when... The AI, either the AI model has been built or the AI component that requires this huge access to the petabytes of capacity with the hundreds of gigasecond has happened, but they also need access to the system of record database, the things that are held in their existing environment that they can't change. So the SQL databases, the Oracle databases, the Postgres databases, the Postgres systems that run their business. I'm interested if you see those systems being hit at all by AI as a risk to those production systems because when you've got our customers that are doing things like the fraud detection. doing the detection of trends in healthcare or the distribution workload, all these sort of legacy businesses that we're seeing that are starting to see AI capabilities being put into their application, and then the risk for them is that they can overwhelm the relational databases that they're having that can only rely on a single VM performance. So I'm interested in that metric, the millions of transactions per second per PepeBuy, because that's, I think... the limits of our system at the moment is in the order of two to three million transactions per second and the limit of the capacity is a petabyte so that is an order that we can get to but do you think it needs to be higher than that? - Here's a question for you, how, if you have two million of these transactions a second petabyte scale, how big is how big is each one of these transactions likely to be? How many, how much data is recalled? On the 64k, so with those... small transactions at 2 million a second, our disaggregated of the hardware can deliver that level of performance to a very small number of host VMs. So we can, from a single database machine or a couple of database machines running very legacy desktops of database like Postgres or SQL or Oracle, we can push that level of transactional performance by disaggregating at the block level but also keeping the the elements for architecture as close to possible as possible to the the point of record, the database machine. So within Azure at the moment we we can push that rate of transactions a second. Here's a question for you Jason, if sorry Chris you're pointing Yes, have we got a hard stop on my Windows? Okay, great. So when our customers are looking at giving some agentic access or some level of AI capability to their application that is underpinned by these two approaches to allowing them the access. We can either just make the production database system run as fast as the underlying hardware in Azure can cope with, which we are a bit limited to block storage and the limitations of the network of those machine types that you can run these databases on. using our the proximity placement groups and we can get to very low lengths in the millions of transactions. So the question is, is it better to make the the production system as fast as possible or should we be focusing more on the use case using our ability to take like an instant copy of the production system within a few seconds when I say a few seconds. there's probably tens of seconds to create a second view of the database that that can be triggered by the mc model context protocol or whatever integrated into their their workflows. So I guess I guess it really depends on the use case whether they they can cope with being tens of seconds behind the production or the very latest copy. the data on the very latest transactions or whether they need to completely de-risk the production side. It's interesting that one of our proponents of our biggest usage of AI is a trading organization and their whole environment is scaled to cope with the load of like five minutes during market open but during that five minutes there's also they're also being absolutely hit by different agents running out there in the wild that are needing to access their production data. to see what everybody else is doing, so it's all about - I'll show you the slide actually, so the reason this is interesting because when we presented to Ong last week, this was the slide that we were on for a long time, he was very interested in actual, you know, real customer use cases that need, you know, very like real-time levels of inferencing, and it was after this that he suggested that he wanted to have a follow-up call with Jay so I guess that maybe that's coming from my understanding what as you said like actual customer interaction what customers do So that's the key question for us, I guess, is how many of the different use cases you're seeing are actually going to rely on having real-time access to the transactional records. We've, I guess, within our existing environments, we've got a lot of customers. where they haven't re-architected things to be optimal for AI-type operations, and they're seeing increased usage hit the Cilk systems, but how much benefit, how many different use cases in the future are gonna need that real-time MCP-type access to the data, the primary source. of truth. Interesting. I guess, so, so as you heard, Ong wanted to make an introduction for us today, and that will be to our, our founder and CEO and it's my job to try and make sure that those meetings are actually useful for everyone involved. So my question to you is, and this may not be easily answerable, is what does Jay care about that we can kind of like add value to? Now that you've got a bit of kind of understanding of what we do, why would Ong be interested in introducing us to Jay? Yeah. You One thing we would love to have is RDMA access on the front end of the L-series because at the moment even with the new capabilities with the DuoBoost, I don't think we've got a timeline for supporting RDMA. It's something we can do very easily and that would give us a big boost to reducing the CPU load on the machine to access us. and Yeah, I know our engineering team are very keen on getting that capability and as you I think yeah, we may be closer on other clouds, but that's a small discussion for today, but we would love to be able to do that. Yeah, we will be. You Okay Yeah, we're working with a gal piglins team, but do you know yeah, yeah, okay Yeah, so we should check with him. Okay, Jason really appreciate time. Thank you very much I think the most important takeaway for us on this call might be, Jason, is if you come across a customer requirement where they need more performance than they can possibly get in Azure with any of the native offerings, then hopefully you can remember Silk may be able to help them. Thank you. Cheers. (upbeat music)
```
