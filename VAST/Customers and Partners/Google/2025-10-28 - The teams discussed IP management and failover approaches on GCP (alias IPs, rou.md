---
type: "group-meeting"
created: { { DATE } }
title: "20251028 1008 Parallels Transcription"
participants: [{ { PARTICIPANTS } }]
ai_extracted: true
transcript_path: "00 Inbox/Transcripts/20251028 1008 Parallels Transcription.txt"
tags: [meeting, group]
---

# 20251028 1008 Parallels Transcription

**Date:** 2025-10-28  
**Participants:** Asaf Levy, bengit@google.com, Billy Kettler, bobnapaa@google.com, Eirikur Hrafnsson, fanyanf@google.com, hildebrand@google.com, iraoren@google.com, johndowney@google.com, Jonsi Stefansson, Karl Vietmeier, Lior Genzel, Mordechai Blaunstein, Olivia Bouree, Ronnie Lazar, shengqiu@google.com, Tomer Hagay, wpien@google.com, yardenh@google.com, yevgenik@google.com

## Summary

The teams discussed IP management and failover approaches on GCP (alias IPs, route-based, ILB) and their tradeoffs, especially under upcoming RDMA-enabled Z4M shapes. Current understanding is alias IPs will not be supported with RDMA; RDMA will use a separate subnet/interface, with two interfaces per instance (RDMA and TCP), potentially on the same VPC but different subnets. PSC Interfaces will be required for cross-project RDMA instead of VPC peering. There is a race window with unassign/reassign IP APIs. The group agreed to revisit options via a shared pros/cons doc and to bring in Googleâ€™s networking team. Ben requested short-term testing and longer-term customer volume projections to support capacity planning. Ronnie will send Terraform snippets for static IP reservation and provide an estimate for adapting/testing the new shapes.

## Key facts learned

- GCP IP reassignment requires remove then reassign, creating a short race window.
- MIGs can use a pool of reserved static IPs for primary addresses; alias IP behavior differs.
- Alias IPs are expected to be unsupported with RDMA; RDMA uses a separate subnet/interface.
- Z4M will launch with inter-node RDMA; GPU-direct storage RDMA is a separate effort.
- Two interfaces (RDMA and TCP) are expected per Z4M instance; may be same VPC on different subnets.
- Cross-project RDMA will require Private Service Connect interfaces; VPC peering will not be supported.
- Route-based failover has latency/convergence considerations; ILB introduces pricing/feature tradeoffs.
- Per-VM bandwidth is capped; adding NICs does not increase aggregate bandwidth.
- Initial test scale target: roughly 10â€“30 instances; CI and scale testing will require more.
- Ben is the new Google PM counterpart for this effort.

## Outcomes

- Ben introduced as the new Google PM counterpart.
- Agreement to revisit networking options (ILB, alias IP, route-based) via a shared document.
- Plan to involve Googleâ€™s networking team for a focused follow-up on RDMA networking.
- Commitment to share Terraform snippets for static IP reservation and to provide testing adaptation estimates.
- Request for testing and customer volume projections to support capacity planning.

## Decisions

- Create a shared pros/cons document to re-evaluate VIP/failover options.
- Engage Google networking for a follow-up deep dive on RDMA and cross-project connectivity.
- Begin sizing work starting with testing projections, then customer projections.

## Action items

- [x] Send Terraform snippets showing how static VIP IPs are provisioned/reserved. @Ronnie Lazar ðŸ”¼ âœ… 2025-11-08
- [x] Provide estimate for adapting and testing new RDMA (Z4M) shapes. @Ronnie Lazar â« âœ… 2025-11-08
- [x] Share testing and customer volume projections (include CI needs and client drivers). @Ronnie Lazar â« âœ… 2025-11-08
- [x] Initiate shared document comparing ILB, alias IP, and route-based failover (pros/cons, pricing, features). @Ben ðŸ”¼ âœ… 2025-11-08
- [x] Schedule follow-up session with Google networking team on RDMA networking and cross-project design. @Billy Kettler â« âœ… 2025-11-08
- [x] Confirm RDMA support details: alias IP status, separate subnet/interface model, and feasibility of route-based failover. @Billy Kettler â« âœ… 2025-11-08
- [x] Clarify Z4M NIC topology and bandwidth allocation (number of physical/logical NICs; per-interface bandwidth). @Ben ðŸ”¼ âœ… 2025-11-08
- [x] Evaluate PSCI requirements and implications for cross-project RDMA data plane. @Google networking team ðŸ”¼ âœ… 2025-11-08

## Follow-ups

- [x] Determine if MIG-managed static IP pools can help reserve VIPs and mitigate the unassign/reassign race window. @Google networking team ðŸ”¼ âœ… 2025-11-08
- [x] Quantify API execution time and network convergence for route changes and IP reassignments. @Google networking team ðŸ”¼ âœ… 2025-11-08
- [x] Advise whether dual NICs provide kernel/queueing efficiency benefits despite no extra bandwidth. @Google networking team ðŸ”½ âœ… 2025-11-08
- [x] Confirm cross-project RDMA constraints (PSCI required; VPC peering unsupported) and performance impact. @Google networking team ðŸ”¼ âœ… 2025-11-08

## Risks

- IP reassignment race window could lead to unintended reassignment during failover.
- Alias IPs likely unsupported on RDMA may constrain VIP design options.
- PSCI adds complexity for cross-project RDMA and may impact performance or features.
- Route-based failover may have unacceptable convergence/latency for client reconnects.
- ILB pricing and feature constraints may limit viability for multi-tenant/QoS needs.
- Unclear NIC topology/bandwidth allocation on new shapes could affect architecture decisions.
- Insufficient capacity for testing could delay validation timelines.

## Open questions

- Exactly when does GCP consider an IP reserved vs free, and how can failover avoid reassignment during the unassign/reassign window?
- Will alias IPs be supported in any form with RDMA on Z4M, or must VIPs avoid alias IP entirely?
- What is the precise NIC topology and bandwidth per interface on Z4M (RDMA vs TCP)?
- Can route-based failover meet latency and convergence targets for client reconnects on RDMA workloads?
- For cross-project RDMA, can PSCI deliver required performance/features and manageable complexity?
- Do multiple NICs improve efficiency (queues/stack) even without increasing total bandwidth?
- Is ILB pricing/feature set acceptable for multi-tenant and QoS needs in this design?

---

## Transcript (auto)

```text
[00:00:00.05]   RemoteBut if you let's say in a virtual machine I create something, right, I create a web server and I assign it an IP. If those IPs are not listed as consumed by the GCP control plane, then another process, someone creates a VM, someone adds a route, those can be consumed by the GCP control plane. It's all playing when it's when it's shoulders consume. by GCP? When the IP was allocated and an object was created for it, or only when it's connected to an interface over a VM? That's basically the question, right? Yes, it has. If it's not, if the GCP DHCP server control plane, Billy, correct me if I'm wrong. doesn't recognize that as an allocated reserved IP it is free and available for any other service or what to consider. But I think Roni's point is that during failover or even graceful or or not right you have to first we don't have we don't have a an API that says move this IP from here to here. We have an API that says remove the IP and then reassign the IP, and then a separate API that says reassign. So there's this window of opportunity where the IP has been unassigned. It's not allocated anywhere, and then it needs to be reassigned elsewhere. - I was doing some research on this a little while ago 'cause I get irritated by the fact that our IP addresses randomly distributed and so I was doing a little research on it and if you, you can tell the MIG, you can give the MIG a pool of static IPs, a range of them and it will reserve those and not free them up and it will reuse them when it creates a new VM. I'm pretty sure that was, I read that. MIG, that's probably the primary IP address. right? Not the alias IPs. That's going to be separate. I think that's what... Let me share something. Because a VIP pool IP is not going to ever go away, right? I mean, if a VM goes away, the VIP IP stays configured in the VMS. So here we have IPs that we're using for VIPs and they're not in use by any. um by any vm right now but they they do have an object that we named so this is the name that we provided right here um so maybe this means that dhcp google's dhcp sorry things that it is used not connected to a VM. That's our assumption, or our understanding, but maybe we are wrong. Maybe can you, does this get provisioned via Terraform, these specific IPs? Sorry, what did you ask? Do these get provisioned by your Terraform? Yes. Do you mind sending me the... the relevant bits and terraform the provision these and I'll follow up. Okay, I'll find it, I'll send it. Yeah, yeah, it's, it's, I was fiddling with this a while back and... But, I mean, ultimately, I'm being asked to jump on a customer call, so I have to drop, but, I mean, ultimately, the simplicity and the end user experience would be a hell of a lot better with Alias IP, if you guys are able to figure it out. It's up to you guys, okay? If there's a problem with API, that's the recommended way. Routing, unless something changes, we spoke about it. it a lot with Billy. Yeah actually now might be a good time to revisit that with the visibility we have from engineering and product at the moment and it'd be a good segue to introduce Ben. I don't know if you've been on one of these calls or you've met the team. Yonzi, before you drop, maybe let, uh, quickly Ben can introduce himself. >> Hey, everybody, I'm Ben. I'm a PM over here at Google. I work on block storage mostly, but I'm sort of the -- I'm your PM counterpoint with Billy. has left the company, and so Ben is kind of stepping in to be our. - Yeah, I'm your new Ruben, but I'm much shorter. - We've all, I've only seen him in Zoom, so. - Oh, so there you go. - I mean, you're taller. - Ronnie, I'm also more handsome. I don't know if you know that. >> Yes, I think this might be worth revisiting, Ronnie, just to evaluate all the options again. I know that we looked at ILB, we looked at alias IP, we looked at route-based approach. I know none of them were optimal and I know alias IP and route-based, there was the latency and complexity and convergence issues. How long does it take to actually? A, run the API command and then B, how long does it take for our network to converge and clients to reconnect? And I think the primary concern with ILB, and you tell me if this is if I'm wrong, but maybe we can, we can start a docking and kind of collaborate on pros and cons of each. I think it was predominantly around pricing, but I don't know if you had other concerns generally it's pricing and it's also a lot of features and that we have tenancy features and quality of service features are around this many we'll still need to go to have two three four or six LBs per VM and we'll need to be able to control where they're moving maybe it's all possible I think depending on the price right yeah so this brings up actually we should probably have a conversation on this sooner rather than later so Ben I actually don't know that PSCI We need to figure out whether PSCI will even work for their use case. - Private service connect? - Yeah, so with RDMA, any cross-project connections will need to be done via PSCI, PSC interfaces. VPC peering won't be supported. PSCI is basically an implementation of the load balancer. So it's going to have introduced complexities, I think. - Are we talking about using RDMA between the clients and the cluster or only between the cluster nodes? - I think we've talked about all both. We have, of course, the basic is a cross-cluster node. We also have clients that use RDMA, as well as TCP when using RDMA nodes, and I think there was also in connection to GPUs, right? That was also mentioned. - Which are specialized use cases. - Yeah. - So yeah, RDMA and GPU direct storage is something, so there's a, within Z4M, my understanding is Z4M will launch with inter-node RDMA. So in other words, Z4M to Z4M will support RDMA. There's a separate program to enable RDMA between GPUs and Z4M. support GPU storage. So, to answer your question, Carl, it's both. My understanding is that that is interested in both, but the Z4M, the Z4M, the belief is that that will potentially have a bigger impact, and we will support that. - With the RDMA instances, we will not have the option of not alias IPs and not route-based IPs. - That's my current, well, potentially route-based IPs will work, we need to move... - With private service, yeah, that should... - Well, this is, ignore private service connect for a minute. Let's assume everything's in the same VPC. My understanding is, current understanding is the alias IPs will not be supported, but I'm still waiting to get final confirmation on that. What I do know is that when you look at going across projects, today will, today I think that would traditionally be done via VPC peering, and in the future that will not be that will not be supported with RDMA, but would there would there be a problem with just using a large enough Cedar and then just allocating IPs right at startup will allocate them directly out of the primary IP pool. Through the control plane right so that they. You're conflating two things, so. - I might be. - With cross-projects, you need separate VPCs. VPCs don't span projects. - I'm looking at the same VPC. I'm not worried about cross-project right now. But within the same VPC, if you, rather than do. it after the fact as an alias IP, but if you provision them right up front and they're provisioned out of the primary address pool during the configuration of the cluster, would those be considered alias IPs? Or, 'cause they're from the primary CDR that the management interfaces get IPs from. Alias IP is a little fungible, right? 'Cause it's either a second CDR. you're sitting on top of the subnet, or it's also a primary IP that's allocated later, I guess. But it's a little unclear to me where that divided line is. Yeah. - I'm not sure I've completely followed. - Well, I was reading the docs just yesterday, right? And 'cause I thought that, I originally thought that alias IPs were only that you create an, you add an additional cedar range to a subnet definition. So if my cedar, my subnet is 192.10.1./24, then that's the primary IP range. If I then in the subnet configuration add something specifically called an alias IP range at 10.10.1.0/24. But according to the documentation, an alias IP can also be assigned from the primary pool, and I'm not quite sure where that, how this control plane distinguishes between that, right? I mean, I get that I have a secondary range of IPs configured in the subnet. right so but it it was a little confusing in the documentation that that an alias ip can also be assigned from the primary ip range are you it's confusing to me i can show you in the docs but i don't want to i don't want to sidetrack this call on that billy that's something we can take offline right okay yeah yeah i wasn't sure if you're asking about that in the context of rdma i The net takeaway is that RDMA will not support alias IPs. Actually, it will be a separate subnet altogether that will be provisioned for RDMA. - Okay, yeah, that's the answer 'cause that was confusing because if it supports using primary IP, I mean, the definition of alias was a little, not clear to me, but that makes sense what you just said. So, we will have a separate subnet for RDMA and we can bring up a VNIC, a TCP VNIC that will get an IP. Where does it get the IP from? My understanding is that there'll be two interfaces. One interface will be RDMA, one interface will be standard tcp oh okay okay so i missed that and do i get the full bandwidth just using the rdma um interface good question i don't know ben do you know the answer to that might have to follow up internally i don't know whether you'll have like a single physical or like two physical nicks and the bandwidth And you'll have two logical interfaces, like Phoenix, one that supports-- Sorry, are you asking specifically on the new shape, or-- Yeah. Let me get you a better answer. Yeah, let's come back to the-- I have-- I see. My suggestion would be, let's get-- to get clarity on exactly where we're going with PSCI and alias IPs, whether route-based failover will work, and then my suggestion would be actually we pull in the network team for maybe a follow-up discussion to figure out how this would work in practice with Z4M, either in the same project or in the event that... So in the future, you guys want to go potentially with a SAS-based solution. So that will be cross-project. The data plane will cross-project, and so we'll have to figure out how to solve for that. So-- Carl asked a while ago, and I think I sent them, I asked the question, I don't remember did I get an answer, regarding will it be better, will we get better performance if we use two separate NICs? on our VMs? Understanding that I understand that that and it's not bandwidth because you can add two NICs to a VM But you never get more than 100 gig, right? I mean bandwidth is allocated per VM not per NIC But it just isn't it more efficient in the system to have two NICs breaking out front and back because I Because I know the answer is gonna be no you don't get more bandwidth if you add more NICs. That I understand. So the way I interpreted that question as as more of an application type question, so from our perspective I don't think that we, I've never heard us recommend two NICs. In fact we generally recommend a single a single NIC. There are some partners though who who to your point, do prefer to manage multiple NIC queues. I think that's more, that becomes more of an application question than a rule question in my opinion. - Yeah, that question was more directed at us running, not so much Google, because I mean, I, yeah, you don't, you don't get more bandwidth by adding another. - No, of course we don't, but maybe in the kernel. you have more queues, or if you have two interfaces. - Oh, question. Anyway, I think maybe that's it. But it kind of comes back to, you just mentioned, Billy, I mean, you're going to get two NICs for an RDMA deployment, right? - So Z4M will have two. How those two physical interfaces, whether you actually see or next to that are TCP IP capable and two that are RDMA capable, or just one of each. I'm not clear yet. I don't know how to. Yeah, it's going to be configured. Separation really come to it. - A little difference between what you have today versus where we're moving towards with those two NICs. Historically, they've had to be on separate VPCs. Now they're saying they can be on the same VPC, just different subnets. - That's interesting. Yeah, that changes the game a little bit. 'Cause the extra, the two VPCs kind of complicates things a bit. Additionally, that does complicate the, like if you provision a separate virtual NIC, that separate virtual NIC does need to be on a separate VPC. So one, I think part of the consideration when answering your question, Carl, would be is that what you want? or does Nick A and Nick B need to talk to each other, or does Nick A from client need to talk to Nick B as well as Nick A? Well, then you need to think about VPC peering or some sort of route-based approach to solving for that. - I guess in my conceptualization when I was thinking about that, and I sort of was that if more mirrors are on-premises, deployments where we separate out a front-end from a back-end network, and we have, but I mean, the e-boxes are a little different. So, but you have client connections, management connections, and then just e-box to e-box, right? D-box connections to each other, but that was, I mean, it was more of just something I was thinking about. I wasn't really proposing it necessarily, but... So, I owe you, Billy, from last week, an estimation of how long it will take us to adapt and test the new shape, the URDMA shapes, and I'm sorry I totally forgot about it until this morning, and I started making some worries. I don't have an answer yet. I need to talk to more people. We'll have it for next week. Let's see anything. I don't think there's any other outstanding issues, right? I think we're good. From a platform perspective, I think a lot of the work is still being done around Marketplace. Ben did you have anything? No, I just wanted to introduce myself and start the conversation. I think we should. like we need to answer some questions on networking, both the way it works today and the way it will work in the future, and then I guess the big open question, which I don't think we can answer today given the stuff we've got to still sort through, is it would be very helpful for us to understand volume you think you'll drive over time so that we can figure out what we need to do internally to make sure that you have the capacity in place to do your testing and then also so we can think about like what's needed in the future to make the all the various services work. as smooth as possible. In other words, if we can make a case internally like, "Hey, this like you guys on the new shapes will drive X amount of traffic," it gets a lot easier to say like, "So let's prioritize making sure we iron out all the rough edges." So if you guys have done that in some previous conversation, great. Billy or somebody else can share it with me. If not, we should probably start doing some sort of sizing. work. You mean for the for the phase that will be testing the RDMA support or in the long term in both but like let's start with the first phase. Ben are you asking like customer projections as well as testing projections? Let's start with testing projections then let's start and then then we should definitely think about customer projections, and maybe you guys have already done that. I'm newer to the conversation. Maybe you guys have already done that somewhere else and I can, somebody could just share it to me, but then we should, yeah. - I think we have, yeah, go ahead, Ronnie. - For testing, we can usually do between 10 on the low side to 30 which will be make us more comfortable so we can paralyze more that's for the initial testing just to see if it works or not later if we want to bring it into the product then we also need to consider ci's then we're talking about uh 52a instances Okay. Maybe do you mind, Ronnie, when you share your projections also include your thinking around that as well? Sure. When we talk about scale as well, doing scale testing, what clients you would also need? That's a good question so we're not when I assume or we have two cases one is that we have RDMA enabled clients and one is that the clients are TCP and we'll need probably some larger clients to drive more traffic. to assume you would just approach it as let's say z4m can do let's just say you have one z4m node we know it'll be able to do i think 400 gigabit a second Thank you very much.
```
