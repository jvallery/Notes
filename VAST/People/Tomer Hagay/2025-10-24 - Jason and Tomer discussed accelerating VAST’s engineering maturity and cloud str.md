---
type: "1-1"
created: { { DATE } }
status: "done"
counterpart: "[[Tomer Hagey]]"
role: ""
team: ""
company: ""
series: "1-1/Tomer Hagey"
cadence: "Weekly"
meeting_mode: "Video"
location_or_link: ""
calendar_url: ""
start_time: ""
duration_min: "30"
privacy: "internal"
ai_extracted: true
transcript_path: "00 Inbox/Transcripts/20251024 1412 Teams Meeting (Parallels) Transcription.txt"
tags: [meeting, "1-1"]
---

# 1:1 â€” Tomer Hagey â€” 2025-10-24

> [!info] Purpose
> Build trust, surface and remove blockers, align on priorities, coach & grow.

## Summary

Jason and Tomer discussed accelerating VASTâ€™s engineering maturity and cloud strategy. They contrasted Microsoftâ€™s AI-first dev workflows (forced training, structured PR/audits) with VASTâ€™s current waterfall, RFE-driven process and release slippage. Tomer detailed the SFDC RFE pipeline, Jira linkage, customer Co-Pilot Slack support, and Global Namespace (strict consistency, read leases now; write leases targeted for 5.5 preview). They aligned that lift-and-shift to cloud is unlikely; success requires a performant layer over object with a global cache. Next steps include validating cloud prioritization, proposing AI-enabled dev workflows, getting hands-on via OVA/lab access, and a follow-up next week.

## Key facts learned

- VAST engineering size is ~400 developers.
- Releases slip without clear consequences (e.g., 5.4 moved months); many hotfixes per release (e.g., 5.2).
- Current model is waterfall-ish and highly RFE-driven via Salesforce; Jira used at ticket level (not consistently epics).
- RFE triage led by Jonathan Hayes; features in SFDC aggregate multiple RFEs and sync to Jira fixed version.
- Support model: Tier-3 Co-Pilots assigned per account; dedicated Slack channels per customer.
- Global Namespace: strict consistency, lease-based caching; NFS/SMB/S3 on satellites; write-leases planned for 5.5 (preview).
- Async replication + Global Namespace (5.4) enables active/active read-write with snapshot history.
- Cloud success likely requires a high-performance layer over object; lift-and-shift is unlikely to win.
- Microsoft approach cited: forced AI training days, structured workflows, PM-semester planning, epics.
- Jason introduced to Josh (Office of the CTO) for OVA/lab onboarding; Andy Perlsteiner oversees SE labs.

## Outcomes

- Alignment to use the cloud initiative as a lever to drive engineering process improvements.
- Agreement for Jason to get hands-on access (OVA/SE lab) to evaluate Global Namespace and workflows.
- Shared understanding that a SaaS ops model (Live Site, telemetry, 24x7) will be required for VAST-as-a-Service.
- Acknowledged need to define AI-enabled dev workflows and training cadence.

## Decisions

- Proceed with lab/OVA onboarding for Jason to evaluate capabilities firsthand.
- Plan a follow-up meeting next week to continue driving cloud/process work.

## Action items (for Tomer Hagey)

- [x] Align with Shachar on AI development program goals and adoption metrics @Jason Vallery â« âœ… 2025-10-26
- [x] Draft proposal for AI-enabled dev workflow (PR/audits, agent usage) and training cadence @Jason Vallery â« âœ… 2025-10-26
- [x] Validate cloud P0 prioritization and resourcing with Brendan and Jeff @Jason Vallery â« âœ… 2025-10-27
- [x] Review and document current end-to-end dev lifecycle, gates, and tooling (RFEâ†’Featureâ†’Jiraâ†’Release) @Jason Vallery ðŸ”¼ âœ… 2025-10-26
- [x] Contact Andy Perlsteiner to obtain SE lab access for hands-on evaluation @Jason Vallery ðŸ”¼ âœ… 2025-10-26
- [x] Coordinate with Josh (Office of the CTO) to obtain OVA/bits for local testing @Jason Vallery ðŸ”¼ âœ… 2025-10-26
- [x] Read Google Anywhere Cache documentation and compare to VAST Global Namespace; summarize gaps/opportunities @Jason Vallery ðŸ”¼ âœ… 2025-10-26
- [x] Set up local VAST OVA on home cluster; move test data and validate GPU workflows @Jason Vallery ðŸ”½ âœ… 2025-10-27
- [x] Schedule follow-up 1-1 for next week @Jason Vallery ðŸ”¼ âœ… 2025-10-26
- [x] Plan Tel Aviv discussion on write-lease semantics and redirection model for Global Namespace @Tomer Hagey ðŸ”¼ âœ… 2025-10-27
- [x] Connect with Rich to map full customer support structure and escalation paths @Jason Vallery ðŸ”½ âœ… 2025-10-26

## Follow-ups

- [x] Share FRD templates/examples for Jasonâ€™s review @Tomer Hagey ðŸ”¼ âœ… 2025-10-27
- [x] Introduce Jason to Eyal Tritel and Noah Cohen for planning cadence and scoping @Tomer Hagey ðŸ”¼ âœ… 2025-10-27
- [x] Provide access to PM SFDC RFE/Feature dashboards and Jira links @Tomer Hagey ðŸ”¼ âœ… 2025-10-27
- [x] Send Google Anywhere Cache reference links to Tomer @Jason Vallery ðŸ”½ âœ… 2025-10-26

## Risks

- Release slippage and weak accountability may erode customer trust.
- Low adoption of AI-enabled development could limit productivity and competitiveness.
- Hotfix/service-pack sprawl increases operational complexity and quality risk.
- Resistance to changing team structures and tooling may slow transformation.
- Lack of SaaS operations discipline (Live Site rotations, telemetry) risks cloud reliability.
- Cloud instance constraints may prevent competitive price-performance without a new data layer.
- Write-lease semantics and cross-region behaviors are still being defined; potential complexity and delays.

## Open questions

- What specific AI-enabled dev workflow, PR/audit standards, and training cadence should VAST adopt?
- Do we have target metrics for AI usage in development (e.g., % of code authored by AI)?
- What are the formal release gates and sign-offs from PM through engineering to release?
- Should VAST adopt epics and semester planning with a central prioritization committee?
- How will SaaS operations (Live Site rotations, telemetry, 24x7 support) be staffed and run?
- For write-leases, will reads redirect to the temporary owner or always route via origin during convergence?
- Will Harveyâ€™s data-layer team lead the core cloud primitives, and what is the timeline/dependencies?
- How can we reduce hotfix/service-pack fragmentation to improve quality and predictability?

> Next meeting (if any): (none)

---

<!-- ai:transcript:start -->

## Transcript (auto)

```text
[00:00:00.04] Jason Vallery :  Jason ValleryLike thinking about how we modernize software development, and, you know, I can tell you today at OpenAI, for example, 95% of their code is written by AI. Like they say this themselves, like the software devs are no longer writing code, they just yell at codex all day, and so, you know, Microsoft is actively going through how do they transform, and here's the numbers, like Azure Storage has 1,600 software developers. So, and you know, many of them are not super productive. Like, you know, they're cutting costs. of not really getting a lot done in a given week, and managing teams of that scale and that size against requirements and against project plans, and so it's just like each little beefed up kingdom within those 1600 people has egos and their own ways of doing things, and in theory, Microsoft says, oh, here's the set of two. We're all going to beat against this planning cycle and we're all going to go and track our work in this specific way, but a lot of devs say, "I'm not going to go do it." And what's actually happening right now culturally and from a leadership perspective is that there's a lot of housecleaning going on. Frankly, there's a lot of people losing their jobs, and, you know, where we're headed is a world where if you have like your rock star, like super smart systems developer, who just gets the platform and has a breadth understanding of the stack, and you empower that person with a number of coding agents, like that one person will get 10 times more done than a team of 20. That's like the world we're entering. You know, personally, over the summer, I ended up with two months off work. So I didn't go through all of my own personal kind of machinations, but there was a bunch of political shit that went down at Microsoft and... Um, I kind of stepped back from my previous role in roughly June, and so I was entitled to a sabbatical and I said, screw it. I'm going to go take it, and I took some time off and, you know, my roots are in software development. So like the first 10 years of my career, I was hands-on keyboard software developer, and, uh, I honestly haven't really sat down and written. in code in any meaningful way in probably a decade. You know, little projects here and there without thinking of any serious, and so I took those two months and I'm like, okay, I gotta go figure out what is state of the art software development look like in a world where you have coders, you know, and there are various like agentic coders out there now. You know, obviously there's the co-pilot capabilities, there's Anthropic and their Clawed agents, and there's, you know, what plugs into VS Code, and I actually spent the early half of the summer using Clawed and Anthropic, and then along comes Codex GPT-5 High that came out, Like maybe the end of July or the August and like it was a clear step function in terms of improvements, and these things like you can go as a product manager, so not a developer as a product manager.

[00:03:37.18] Remote : Yeah, I know. I mean, I've been, I've been doing a bunch of stuff.

[00:03:39.62] Jason Vallery :  Jason ValleryYeah. You write out the requirements, the high level design, you hand it over, it'll go work for like 15-20 hours. I had experiences where like you connect it up to a test cluster like that has Kubernetes running on it. It was like deploying containers, running tests against the APIs, outputting the logs, fixing the bugs, redeploying the containers, all without any of my interaction, and so, I mean, I just see this as, you know, sitting here with this lens of where we're going next and all of the work that needs to get done, my first priority is just to understand where we are as an organization in leveraging these technologies. Because we can talk all day about being the AI operating system, but if we're not building it using AI, we're... So, like, does Sahar have a culture that is, you guys must go learn this now and before you write the next line of code, you need to fully...

[00:04:41.51] Remote : Yeah. So I can tell you what I found during my last visit. It's very early steps and there's... So maybe I'll take a step back. So, Vast has a reputation, I don't know if that's across the board, the entire engineering team, but it has a reputation in Israel as a whole, as a place that is pretty difficult to get into from a, if you're a software developer, you have to be like top-notch, you know, really, really good. They just recently started hiring some younger people, younger meaning like brands. not a lot of experience, but still very, very strong, and so that's the starting point, but still within that, I've seen, and I've been told by, so there's a guy named Albert, I don't know if you've met Albert. So he leads the... the management, you know, the platform management, but also the UI/UX, API, some other components, and so he told me that, I mean, the read-only strong people, those that I like to call when I have a question, right? Those people know everything about everything. They just took, you know, the tools that we. that we gave them and started started running but they're becoming way more efficient you know uh it's just and and but most most engineers didn't right and that was uh about two two months ago so i know there is um there are some motions there right there's an effort but i don't know if really like a program or with with the goal that I'll never I never actually gonna ask Shahar but we can ask him I mean it's like do we have do I have a number right it's like you know 90 percent of the you know deliverables by xyz need to be delivered by whatever I don't think he has such a

[00:06:45.23] Jason Vallery :  Jason ValleryYou know, the way that Microsoft's approaching this problem is, um, forced training. Like, you know, there's a bit of a, there's probably even a moment where there's a bit of a productivity lull. But what's happening right now is that the way you use these things are being codified, like how to deploy them, how to leverage them, how PRs work. how you do audits and, you know, peer reviews, how you do, like, how you, you know, insert them into the dev cycles and workflows is something that's being structured, and then there is now, Microsoft has like forced AI training days for the dev teams. So like once every two weeks. there's like a full day where there's no dev work done and everybody goes and gets into a teams meeting and you know, there's a leader that's elected for a specific topic and they'll have a structured workshop that spans the entire day on that area of how to use these tools. I mean that for that to get there though that starts with, you've actually already sort of decided about what the new dev workflow looks like and where AI supports it and where AI complements it, like all of it is already figured out.

[00:08:03.20] Remote : Yeah.

[00:08:03.77] Jason Vallery :  Jason VallerySo like in some ways, I would really want to see that level of maturity, given like how many developers do we have in total?

[00:08:10.00] Remote : I think almost 400.

[00:08:12.71] Jason Vallery :  Jason VallerySo like... your point earlier of like, we don't have enough resources. I understand that perspective, but my perspective is 400 is too many, you know? And only from the sense that like, it's hard to organize the work of 400 people, and if you can structure these things differently and you can enable them to get so much more done more efficiently. Like, it's not about hiring new people. It's about leveraging the tools.

[00:08:41.55] Remote : - One of my best learning exercises or experiences, sorry, was when I worked for a company that, I mean, we were a startup and a unicorn, you know, IPO, and then a year after IPO, we went chapter 11. I don't know if you know the company Tintree was a storage for unique take on storage, awesome product, really liked it. But anyway, we got acquired afterwards. So I was there from where we were like 700 people, then we went to like 30, right? So I was part of those 30, right? help sell the, I guess, what's left and then help rebuilding, and, I mean, that's a longer conversation, again, another one for maybe over a beer in Tel Aviv or something. But, you know, what I saw, like looking into various areas that I never touched, and I'm not, you know, I'm not an expert developer or whatever. But, you know, you could really see the silos, right? You can see the, you know, at some point people don't talk to each other or it's hard to align whole teams versus, you know, just one person talking to another person. So, you know, like QA never really worked with the DevOps team or the We had like 80 racks of gear for testing and whatever. I looked at the QA automation, and again, areas that were just a small number of people, and I started poking into stuff to survive, and I saw how a super complex Python script on the QA side could be solved with just a trunk port on a Switch. right, because then you can define whatever villain you want instead of instead of figuring, querying the switch on what villain that board is or whatever. Anyway, all sorts of things that that eventually we were like 150 people, 140 people, something like that, after we got acquired and we've done a I wouldn't say the same amount of work, but pretty close from the 700. So anyway, it's for, I don't know if that's going to be a popular comment or not, but I'm going to say it anyway. When Elon Musk acquired Twitter and started cutting and whatever, I know, I mean, the method and whatever, but I really... It was after my experience there and I'm like, yeah, there's probably still such inefficiencies there, which again, I experienced everywhere I looked when there was just no other way. You have to, you have to do things to survive and just see those things. Yeah.

[00:11:34.90] Jason Vallery :  Jason ValleryIt's empathy, right? These are real people. lives. You're not like trying to destroy anyone's careers, but on the other side of it is like, you have a set of goals. How do you do it most efficiently with the fewest resources to get the most impact? And so, you know, I'm purely coming at it from that lens. I, you know, I don't, I'm not going to go in and have a conversation with Sarah saying, oh, you should get rid of half of your devs. I mean, you know, at least that's not the point I'm making. The point I'm how do we get them empowered to be able to deliver on all the promises that he's signing up for both Work X and both Work Y that you've mentioned before, and maybe even Work Z and, you know, A, B, and C as well, given the same set of resources, and that sometimes, like, yeah, you know, Microsoft is going through even, does that mean we change team structures and flatten organizations and... realign the way that we organize the work. I mean, obviously, when you get to that kind of scale, and I'm sure this is true here, but like, the teams end up getting organized by area of code base in some ways, right? Like, you know, you've got this team, Microsoft has a ratio of like, they typically say a team should have about, you know, seven software developers per PM, and those like eight resources should have one like, engineering lead over the top of that the work of those eight people. So that's kind of like what a rough pot of dev looks like at Microsoft and then so you assign some scope against that that is this part of the code base, and one of the things that's super interesting about AI is that like as these agents kind of evolve and get more context and more ability to manage larger code. bases, it'll likely mean that you don't even organize your code base that way, like, you know, we went in this world as an industry where we pivoted towards microservices, and a lot of that was honestly around how do you organize a dev team? You could say this unit of dev is this one microservice, and then now suddenly, like, you know, you could actually build monolithic applications that are probably managed by a... AI more effectively and be able to like make larger components because you don't have to have like the brain of this this set of devs all completely around all of it. It doesn't have to understand everything in the end because the AI could do that. So I'm off on a little bit of a rattle, but my entire point is I really want to think about these processes in terms of the to be state and what we should be doing from a PM side to a enable them to get there, and what kinds of pressure we should be putting on the dev leadership to adopt these technologies?

[00:14:01.52] Remote : I think that-- well, there's several things I need to say. So I don't think there is enough pressure on Shachar to deliver on specific areas. or well I mean enough pressure overall right so we have releases slipping and like dates so I mean no one respects schedules across the company right it's been a joke by sales right we like 5.4 was just released it was supposed to be like in June or something, right? And then early on, it was updated to August. So now no one really remembers June, and we're like, oh, it's August to October. Well, not a big deal, whatever. It's still a very big deal. There's no consequences. There's no, like, how do we make it better next time, whatever, and it's always like, you know, we didn't have enough-- servers to test on we we we increase the scope halfway through so there's more content and that's why you're putting whatever i just know there's there's a lot of that um there is there is an area that i think what what you mentioned could really help is um outsource or delegate or pass package parts of the areas that are now very monolithic. I'll give an example, you know, Roni's team doing e-boxes and cloud. A lot of the, not, I don't know if a lot, but a significant portion of the e-box work could be delegated to the e-box manufacturers, right? To, you know, to, like, because right now. we get I don't know a Dell box we install it in the lab and we then try to figure things out right and we could I mean if we could package those first steps that that we do in-house in a way that you know you just hand to to a partner and say when you get this okay in the prompt send us box before that just fix your old stuff right that'll be super easy to do or well much easier to do with with the approach that you're proposing rather than you know now we don't trust anyone everything is monolithic and in-house uh there's all sorts of i mean things that are probably less relevant to to you but i mean in the same category as a devalidation, so all those things. We spend a lot of cycles and we can just hand it over. So, I mean, I can say more about this, but I think it starts with some goals for innovation. engineering as a whole, and goals for efficiency and accountability and, you know, stability and all that we have, you know, we have customers complaining about the quality, right, upgrades are very problematic, you know, supposed to be non-disruptive, there's a lot of disruptions during upgrades. you know, there's there's some growing pains right a lot a lot to fix so I think you'll have the opportunity even if you just and you know I'm sure you'll you'll evolve your your positioning and like how you drive things and everything but if if cloud is I mean everything I mentioned will impact cloud right if you if you chase that through the cloud lens, right? It's like, you know, cloud needs this. If cloud is important, we must evolve our, you know, our thinking around release schedules or whatever, right? That could carry weight. Because right now, it's, you know...

[00:17:54.39] Jason Vallery :  Jason ValleryI certainly think when we get to a Zazz world, if we really are serious and.., and I think there's a lot of opportunity to do this. If we're serious about, you know, vast as a service and being able to go in and just connect an endpoint up and run a workload against a multi-tenant deployment in the cloud, that suddenly requires a whole different operational model, right? You need, you know, DevOps, you need LiveSight, are easy, you need 24/7/365 support structures. When you're just block software, none of that's true. You don't have to have all of that. So that'll fundamentally shift things. The way that works in Azure, obviously, is you might be a developer on FeatureXYZ, and going back to that team structure of sort of, you know, seven. and seven developers, one PM, one engineering manager, like that set of people, they all take rotations running Lifesign, and so they're responsible for managing the production infrastructure and they're responsible for getting the tickets for their part of the stack, and so when an issue is happening, they're responsible for monitoring. honoring, telemetry, instrumentation, all of that, and, you know, that's a different way. I mean, you tell me. - Yeah. - I don't think that's part of VAST today. Are you doing that kind of--

[00:19:19.80] Remote : - No.

[00:19:20.56] Jason Vallery :  Jason Vallery- In any way, right? And that changes things.

[00:19:22.34] Remote : - No, I mean, yeah. So, I mean, we do have some people that are at the front, I mean, we call them customers success, but they're really support people and then there is there's a team called v force That is essentially support developers That I mean they're part of engineering But they're kind of aligned with support and they can develop hot fixes and they can dive deeper into the code to troubleshoot things and things like that. So this is good and bad, right? I mean, sometimes it gets us out of situation very quickly and we're able to solve for issues that maybe would require waiting for the next minor release or whatever that would be, you know, instead of a shot fix from the vForce team. it's almost like it's some in some cases we're like a services company or something right you have like you know let me let me show you something this is and there's uh i mean there's a lot we can talk about i mean you mentioned Global Namespace, last time we spoke I can walk you through Global Namespace and how it works and all that, I'm here for you. So, look at this. This is the 5.2 release now. But you can see how many 5.2 releases we have, right, so there's some betas, Service Pack 1 for Ebox, Service Pack 1 Hotfix 1, Service Pack 1 Hotfix 2, so in some cases, right, this is like for Voltage Park, a customer, right, Best Tour, Service Pack 4, Service Pack 4 Hotfix, whatever, right, just imagine, you know, expedient, right, the hot fixes, whatever, this is just 5-2 and there's more. But, you see what I mean? So, and we're, you know, we're kind of, we're a waterfall on from cost of company, right? It's not like we, you know, we check in every 24 hours. whatever, and it's like, you know, it's ridiculous.

[00:21:48.70] Jason Vallery :  Jason Vallery- What do we use for-

[00:21:49.52] Remote : - So-

[00:21:50.34] Jason Vallery :  Jason Vallery- So where does the source sit, and how is it managed, and the ultimate, like, maybe you could walk me through the dev lifecycle a little bit, so I understand what they do, and how that translates into builds.

[00:22:00.52] Remote : - I'm actually not the best person to walk you through that, to be honest. you know I know parts of it but I don't I mean sure there's a lot of magic happening that I'm not aware of or I'm I have some gaps so I'd rather yeah

[00:22:19.25] Jason Vallery :  Jason ValleryProbably not waste your time on that to be honest I guess more honestly you say I'm sorry I guess that a more interesting level what you say is we're a waterfall on from software company and so I guess I'm interested I'm interested in how does that work from a PM requirements into a plan, into a commitment, into a dev cycle, to a check-in, to a build, to a release, to the next iteration? What does that lifespan look like? Where are the gates? Who's signing off on the gate? I don't know if you've got that as a process document.

[00:22:55.16] Remote : Yeah. Yeah, yeah. We had, I think, I mean, we have parts of it documented, but there's like all sorts of things in the middle that are not documented, and we just launched another element for RFPs, for requests for enhancements, which are a major part of what we do. So maybe I'll start with that. It's gonna be open.

[00:23:32.53] Jason Vallery :  Jason Vallery- I've been getting random RFP emails, so I'm on whatever distribution.

[00:23:36.77] Remote : - Oh, oh, yeah. Yeah, yeah, maybe Jeff added you to the PM alias or something.

[00:23:42.15] Jason Vallery :  Jason ValleryProbably.

[00:23:42.52] Remote : So, yeah, I mean, there were attempts to capture our fees from like early days of FAS and everything. We finally, I think we got it right a few months ago, a process that is built into it. SFDC and can you see this now?

[00:24:03.98] Jason Vallery :  Jason Vallery- Yep.

[00:24:04.85] Remote : - So I can show all, right? So a lot of, like a significant portion of the release content is coming from customers, right? Customers request, sometimes they're deal blockers, right? Customer needs feature X, Y, Z in order to, to deploy and implement VAS so they won't buy without it, and so what we've done is we've built this process into Salesforce where an SE will go here, will open an RFE, a new RFE. I'll just take a random one, right? So they'll say if it's a blocker or not that, you know, why, whatever the revenue attached to it, and most importantly, the opportunity assigned or associated with this RFE, right? And so, I mean, in many cases, they won't do a good job in that definition, right? They'll come up with the solution, not the problem, right? We need to steer them back, like, what are you trying to achieve? what is the actual blocker, not what the customer wants, right?

[00:25:13.05] Jason Vallery :  Jason Vallery- Who's triage is that? Is that you and the other PMs? Like who looks at the RFP initially and says, oh, here's how we solve that.

[00:25:19.72] Remote : - So this is, so the first triage is done by Jonathan and my team, Jonathan Hayes. You probably saw his email on the RFP update emails, and so he will try to first, I mean, he's not an expert for all the features, but he's been here for like five years. He knows a lot about Vast, about many areas. So he would try to find solutions, but first he will look at whether this is even, tell us what, you know, what they're trying to do, what they want in a good way, and what I didn't mention is that the SE manager, you see, so this one is not submitted, right? It has to be approved by the SE manager for this RFE to hit us. I mean, it's on the list, I can look at it, but officially I'm not yet processing it, or Jonathan is not yet processing it because it's not submitted. So we've added that. phase in order to clear some of the some of the junk we had because there was a lot of I mean someone would just write a sentence and send it to us and then we have to scramble and trying to figure what it is and why and what it right so so so now this just works better and um but But after Jonathan does initial cleanup, sometimes he would try to dive into the actual use case, you know, what the customer is trying to do, maybe find alternatives, see if there's work around, maybe pull the right people to see if we can solve the problem or whatever. Or he will reach out to one of the other PMs. I mean, we're not a big team, right? So there's so many options. Sometimes, like I can tell you, we had Qualcomm, for example, like the SE Open7 RFEs, and like, what is, like, what's all that? And it was essentially, I mean, and the SE didn't know how to, I mean, you know this, I won't spend too much time on that, but I mean, it's like, There was the collection of things that impacted how the customer is doing like a specific flow But if you change the flow from the beginning you cut like five of them, right? Five of those requests. So Journey works with them, works with Qualcomm Awesome, whatever, right? So so we're doing a lot of that and this is something something that because we're not, I mean, I explained a little bit last time we spoke about this mode of like how we, you know, not working in a traditional PM engineering mode here, right? We're doing a lot of the field facing stuff, and this is a, this is a big part of it, right? To get, to get a clean background. backlog of real features that must happen in order for us to close business, and that's what Bass has been about, you know, since day one, right? It's really this close alignment.

[00:28:22.14] Jason Vallery :  Jason VallerySo one of the things, like, how does PM, the field engineers, and When sales kind of work together on something like this, how much opportunity are the PMs getting to actually just jump on the phone with Qualcomm and kind of triage it more real-time versus the phone of like it went through the field and the sales engineer?

[00:28:47.16] Remote : So we do a lot, I mean, we're on a lot of customer calls. for various purposes, but also those type of cases. I mean, that's a big part of what we do. Sometimes we'll pull in an architect from engineering and join the call together. Sometimes we'll join a conversation, but usually it starts with the organization. either an SE reaching out with a question or whatever, or after already having the conversation, or an RFE that we say, "Hey, why do we need this?" And the SE could be, "Oh, I don't know exactly. The customer asked me blah, blah, blah." So then we'll jump on the call with them and ask the right questions. But it starts from the problem space

[00:29:38.04] Jason Vallery :  Jason VallerySo, um, yeah, let me ask, like, what does the support structure around an account look like? I mean, maybe this isn't a question for you. I should really go talk to rich and others, but yeah, I mean, but like from your lens, if you've got Qualcomm, who's Qualcomm engaging with, what experience do they have? What resources do they have to bring to bear? Like how does that look?

[00:30:00.55] Remote : Yes, so Qualcomm, yeah, I can cover a lot of that and maybe some maybe finer details you can ask people in the customer success side, but we have, have you heard of the Co-Pilot at Vast? oh i see that we have we have a minute left you know you don't have like the zoom is about to end or something right i thought i said it till four o'clock your time yeah i know but maybe you don't have when your account is not on the vast page you know account should be because it's a like it aligns with a 45 minutes limit of a free account I have a paid account I thought I mean this will probably be

[00:31:00.60] Jason Vallery :  Jason ValleryThe longest meeting I've had since I started that I scheduled I mean we can drop it and start a new meeting if that's useful

[00:31:09.00] Remote : - Yeah, yeah, let me send another Zoom link on Slack. - Okay, sounds good.

[00:31:15.93] Jason Vallery :  Jason ValleryTalk to you soon, bye.

[00:31:17.87] Remote : - Goodbye. Okay, all right, so yeah, so I mean the co-pilots right I started saying so we have we have that is the, you know, it's not a tier one, you know, someone who just routes call, call, right? It's like a tier three, call it, support engineer that knows how to log into, I mean, fix things, right? And the real engineer that is assigned to several accounts. So Qualcomm has their own copilot. There's a backup person. if he's out or whatever, but they have their go-to support person. There is a Slack workspace for support. Have you seen that? I don't know if you're on that, but we have a separate Slack space for support and customers, support the customers. We all are there, right? So, sometimes, I mean, it could be a chat between the co-pilot and, you know, the customer, right? And the support person will say, "Oh, that's a roadmap feature. Tomer, can you comment?" Right? And I'll jump on or whatever. Or, "That's a nice idea. Tomer, blah, blah, whatever. I'll join." So, it's very interactive. We get a lot of visibility. on and we have all customers there, except for Fed or whatever.

[00:33:05.24] Jason Vallery :  Jason ValleryBut they each have their own channel is what you're saying.

[00:33:08.18] Remote : Yes. Yes. Each have their own channel, and if I want to validate something, let's say Tesla asked for, you know, S3, STS, or whatever, and we're working on that, and I have a question. I can shoot it on Slack. right, and like, you know, to the specific person I spoke with, whatever. It's very, very efficient. I'm actually working on being able to submit polls to all channels together. So, like, you know, would you be interested in S3 over RDMA? right? And, you know, like a question, like a separate question on each channel, right? The results are aggregated in one place. So for that, I mean, there's some IT collaboration. I have a meeting on Sunday with the head of IT because we need to, I need access to like private channels, which I do, but the tool has that need to need access. Anyway. So it's very good from that perspective.

[00:34:11.08] Jason Vallery :  Jason Vallery- Like, are most customers on that channel? Do they have good representation from their teams?

[00:34:16.69] Remote : - Yeah, yeah, I mean, that's their go-to. I mean, like, except for some exceptions. I mean, there's some exceptions, but that's the go-to kind of interact with Vast. That's how they open cases, that's how they ask questions. questions, that's how they get answers from coordinating, I don't know, sessions, whatever. It's all on Slack.

[00:34:42.57] Jason Vallery :  Jason ValleryIt's interesting. I mean, I'm coming at it from the lens of what we did at Microsoft, obviously. Well, first of all, we didn't use Slack, so that was a whole new thing. I was on their Slack, but that was the only Slack I ever used. We did that with Teams channels, and so I was just coming at it from the lens of like, do all of our customers actually use Slack? Is that a thing? Maybe it is. I don't know. Maybe Slack is more popular than I thought. But that's funny.

[00:35:10.87] Remote : Yeah, no, I mean, I'm on the main. like not the main but I mean channels I care about and you can see I'm not reading a lot of like I'm behind on whatever but you know expedient right I mean you know I don't know LNNL. I mean, everyone's here. there's there's hundreds of them I I guess called on on stuff right so like this is with G research right and whatever so I mean it's it's very that's you know that's a thing right I mean that that's the go-to place for everyone and it's very efficient. I mean, it's awesome for me, right? I can go jump on any, like there's no, you know, we're very kind of, you know, if you'll see how Jeff treats salespeople, right? It's like therapy. So, you know, sometimes there is the companies have this approach of, you know, don't don't don't reach out to my customer, right, if you're not through me or whatever, that doesn't exist that I can jump on on any channel anytime and ask any customer a question about, you know, whatever. Right, and like, you know,

[00:36:45.67] Jason Vallery :  Jason VallerySo, so walk me through what kinds of technical resources sit in the field to help in these conversations.

[00:36:55.91] Remote : So, yeah, you're not, you know, right, right. So I think you need to, yeah, you asked specifically about Qualcomm, or you took Qualcomm, I think, I think as a good example to, to walk through, right. So they have their, their, co-pilot, right, for post-sale and whatever. There is the office of the CTO and Andy Perlsteiner. Have you met Andy? I don't think so, no. Okay, so I think you should. He's, you know, been here from early days, has a large team of experts are like an overlay for various areas. So they're doing a lot of the enablement, like the technical enablement. But also, they're kind of parachuted in to help account teams, to help the SEs, right? So there's the account SE. There's the co-pilot, and an octoperson, office and city. person that are normally, you know, self-sufficient in the field will join when, when, you know, the OXA person is on, maybe had. So the SE conversations are usually very, I don't know, I mean, we have good SEs, but it won't be very deep, right, they'll, they'll try to be those trusted advisors. But, you know, they're, they're not, I mean, they haven't probably haven't seen enough to come up with creative ideas or whatever, right. So that's where Office of the CTO comes in, and if they, they still need, I don't know, like roadmap or, or whatever, then, or requests or whatever, then they'll, they'll inform us and we'll jump on in many cases, when, you know, office of the CTL or like an octa person is not involved, and then we'll get the request that we can, you know, those are the cases we can steer it to like work around or whatever. But if an octa person is involved, usually they cover a lot of it's it's very difficult to to find um to be more creative after they've already exhausted those those options or whatever so so that's that's usually how um so so for in qualcomm for example The, so this was, and these were. requests to PM, but they never involved an Okta person, I joined, had a session with a customer, the customer is super happy to just walk through their build distribution process, right, which is, but to the SE, they just said, Hey, we need this, this, this and this, right? SC never asked for what are they trying to do. So they walk through how they create builds, you know, they copy them, they distribute them, they change permissions. So every engineer gets their own build space and whatever, right? And then I'm like, okay, so let's do this from the beginning and you can do other things. But I mean, so we'll get, when it comes to like feature requests, so it's either gonna come from, you know, I mean, usually from SEs, rarely from co-pilots directly to engineering, right? So co-pilot, like support will open a GR to engineering and get to get, you know, to get. them do something we've been steering those back to if it's a feature right if it's something new that needs to be developed the copilot is supposed to tell the SE I just spoke with the customer and they want to have feature XYZ and the SE will go and often open an RFE in Salesforce as I showed you. Now I haven't showed you the whole process in salesforce because uh i need i need to show you that because it's uh because it i mean i missed the important parts or not important but like also important parts so so the first step is you know the sdq comes in, he gets his RFP approved afterwards by his boss. Then we get that Jonathan's looking at it, we're trying to figure what's going on. If we need to prioritize it at some point or whatever, you know, if it's a blocker, we try to manage it as soon as possible, but it could be, you know, just a GUI enhancement. requests or whatever, that low priority. Regardless, what we do then is we open what we call a feature. So in SMDC, we have the ability to open features, and features is this aggregation of-- of RFEs, in this case there's one, right? So this is only PMs do, right? There's something that only PMs access and we aggregate. So there's multiple RFEs that could talk about the, I don't know, quality of service. They all fall into a specific quality of service feature. we'll tie them into this quality of service feature here and you'll see them here we can aggregate the amount of business associated with with those RFEs and the RFEs that I showed you earlier they have to be they have to have like one right they have to have an opportunity assigned to it but if let's say the same SE, say an SE from Japan needs to they need to they're asking for a feature that is required by their five accounts they need to open five RFEs So we've done that on purpose because there's different priorities for the different accounts, right, it could be critical for one, it could be before in the previous process we didn't have that and it was a mess, right, people just filed on, right, you had a blocker RFE and people say, yeah, me too, me too, me too, and you don't know who's blocker, who's not. So, so now we have this many tools. one and so if i go back to features right um so so then what we do next right so we group that with three engines with engineering i'll explain what uh what that means and then we assign Jira's Jira ticket to this feature go back I'm sorry that is the engineering work yes yes and then the Jira ticket in Jira let's say some for someone starts working on it, it will have a fixed version, right. Let's say we targeted for version 5.5, right, then this will update, right, it's it's in sync. So as soon as you update Jira, this will update. All the RFPs associated with this will update as well, so the SE can go here and see fixed version change, right. version change and then they'll see, you know, what version it is. So all the way from engineering JIRA to, you know, the ticket, right, to the feature, to the RFE

[00:44:48.15] Jason Vallery :  Jason ValleryAnd the SE. Do features compose into anything higher level, like the way, just to bring the process from Microsoft, the way this would look at Microsoft? is that you'd have a feature request, and I think that is analogous to an RFE here, and you'd have the many to one relationship of this customer asked for it, that customer asked for it. Each one of those feature requests would document the what, the why, like what is it they need, why is it important, the same sorts of things of revenue blocking, milestones, and all of those. feature requests compose up to a feature, a feature would have the design documents attached to it. So it would have the, well, it started with like, what is the, there'd be a business justification document attached to it. Like, are we going to build it? What's the justification for doing it? It would have the sign off on that, and then it would have the design. document for how it's going to be built both from a like user experience perspective and then there would be the engineering would get involved with a so there'd be a functional design and an engineering design basically and then a feature composes up to an epic and an epic is where like user stories sit so yeah it's thematic it's like this release we're gonna cover We're gonna focus this release on this theme and so many features can pose up to an epic And then the like status tracking Milestones release management all sits at sort of the epic Yeah, is there any of it?

[00:46:23.30] Remote : So I tried to push for epics like when I joined, you know three years ago we did it there was one one feature that we did with Epic and that didn't, I mean, it was, I couldn't bring the organization to, to, to do more of that, to be honest, or, you know, Stratfor seemed to do more of that. They're very unorganized, I guess, when it comes to, comes to Jira, you know, putting data in Jira's. But I'm, I'm with you. I mean, these need to be so the feature from SFDC need to need to actually align within as with the Jira epic and not a Jira ticket. We're on on a Jira ticket right now. The design documents and all the content around it comes. later right so so what what happens today is that you know let's say we you know we qualify the RFE we open the feature we open the ticket we we actually have a conversation with two main people in engineering or or two key people in engineering uh one is a y'all try tell which I mentioned to you, and the other one is Noah Cohen, which I think I also mentioned. So, there are interface to engineering. They sit in Tel Aviv. They'll understand the ask from us, right? They'll do some initial scoping. So, they have regular meetings with the various teams. They'll figure, you know, the resources needed, the scope of, you know, maybe t-shirt sizing of feature and things like that, and then we'll work together to fit those into minor or major releases. There's no kind of scientific way. I mean, we're just trying to fit the pieces of this to the jigsaw puzzle. right it's it's uh but but that's that's how we we manage that and and then design documents um afterwards so we have uh the frds have you seen some of the frds already i mean you'll have

[00:48:40.74] Jason Vallery :  Jason ValleryPlenty of time to do that i mean that's that's what i want to get into is like how, again, just giving the Microsoft lens, it would be you would, as the PM, you would see a bunch of feature requests come into your area, right? So I guess, let me give you the high-level Microsoft view of how I would see this working, is that each PM has an area of ownership, and that area of ownership is like, this is the part of the system that is your area. like you own the API, you own tier, you own durability, you own whatever, right? So each PM has their own, and you would see all of the feature requests kind of coming into your inbox if it's associated with your area, and then you as a PM, you know, Microsoft works on a semester planning cycle. So every six months, you set the work for the next six months, and so as a PM, PM, you'd see all of these things come together and you would decide, like, based on the feature request, here's what I think we should go tackle during the next semester, and you would build a business justification doc for each of the sets of features that you would want to push through the next semester, and so that's the PM to write that up. There's a standard template like revenue opportunity, what you're unblocking, why. all day, and then all the business justification docs that the PMs wrote would go to an internal committee to review and stack rank those and cost those, and so you know high-level costing gets done, the leadership team would go and sit and poke holes in all of your business justification docs and then at the end of that there would be a yes/no decision. we're going to do this, we're not going to do this. Go to the backlog stays on the stays in the.

[00:50:22.18] Remote : >> Yeah, so maybe if you pause here for a second, I'll.

[00:50:26.42] Jason Vallery :  Jason Vallery>> Show me that.

[00:50:26.99] Remote : >> Share the equivalent part of that, right? So in our case, it's, I mean, there's just a lot of iterations, right? And sometimes, those release. or those features going to minor releases, sometimes they go to major releases, right, and sometimes it's a blocker for a deal with a PO that we are expecting to get next month, and we need a commitment and scope quickly and say, "Okay, we can deliver that. If you sign today, you know, PO plus six months," or whatever. So, we'll have weekly conversations with Noah and based on what's in our pipeline, right, based on the latest and top RFEs, we'll jump on a call. I mean, it's an hour call. Sometimes you skip a week if there's nothing urgent, but we jump on a call and Jonathan and I, sorry, Jonathan, myself, Noah, and Eyal, and we're the doing the initial review of like, you know, that business justification that you mentioned, nothing written, nothing, you know, it's all data in the RFP that we bring forward. But after we've done a lot of validation, that this is not, you know, BS, right? To put it plainly, we were very, you with account teams of like you know to figure whether it's the only feature that blocks them from from from selling right maybe there's like 20 others and you know we'll we'll be surprised you know after after doing whatever I mean there's all sorts of things that that we validate and then we come prepared to this meeting with Eyal and Noah say here you know we think we we need to do this or we need to take this seriously see how we can what's the best we can do for this customer, for this blah blah whatever, and that's our weekly or bi-weekly cadence, depending on the sizing, and they would go and work in Tel Aviv and we'll huddle back. At some point, it goes to Shafar, and Shafar is that executive that pokes holes, and it's very, very difficult to BS him, right? I mean, it's like, he will call the account manager and say, "You really think that this feature "will help you sell this? "No, I don't believe it. "Show me, blah, blah, blah." You will, you know, this is, it's sometimes managed like, I don't know, like a like a supermarket or whatever. I mean, it's like, you know. I don't know if you get you get where I'm I mean, it's it's very unofficial and very like one off, whatever. But. Well, I mean, that's after we have all of this, right? It's like it's been but but that's that's the way it's. it's done, right? But I mean, one of the main differences is it's very rapid, right? We get an RFE, we manage it, sometimes immediately, if it's a blocker for an IPO that needs to close in

[00:53:28.28] Jason Vallery :  Jason Vallerya couple of weeks or so. But sorry, go ahead. Well, I mean, I think that's one of the growing pains any software company goes through when you've got, you know, your entire focus is winning the next deal, and so then engineering kind of follows sales in a very tightly coupled way. Yeah. Versus you hit a certain like critical mass and you know, every customer in every deal is not obviously at the top of your stack, and so you have to start thinking more thematically and like, how do I unlock the most value for the most customers versus just winning a deal, and, you know, I feel like maybe we're heading to that transition point where it's more about like, how do I be a little bit more intentional and thematic than just chasing the next PO?

[00:54:11.38] Remote : So I think there's two areas that we have the opportunity to just set the process as it should be, and that's the world I'm coming from as well. I worked for Cisco, you know, whatever. So cloud is where you could set the standard, right? Or kind of the procedures or whatever, align the organization to work in a mode that makes sense at scale, and also because we're building business for, you know, Bas never believed it. and build it and they will come, right? Well, we always build what they want, right? They already came and we're just missing a couple of things, right? Not always, but that's been the majority of the workload. So cloud and all the new stuff, and there is a person on my team, Prasibh. who's responsible, a PM responsible for the engine and database stuff, and this is also, I mean, it's all new and we don't have enough customers to tell us what they want, right? It's new for our customers, especially, you know, the personas we speak with, right? I mean, you know, most of our sellers talk to storage people. Storage people don't know, you know. what a RAG pipeline is, or they're starting to learn. So anyway, we're trying to put the right pieces in place, at least at this point, until we'll have enough pool, and I mean, you know how this works, right? So it's a new area. So that's where we're starting, or we're working to. to work in a better, more structured way. Cloud, I haven't done much because, you know, there's no serious investment in it. So you have the opportunity, I think, to do that. But everything else, like all the storage pieces, that's the mode we've been in, and that's currently where we are. It's very, very difficult.

[00:56:16.17] Jason Vallery :  Jason VallerySteer it to something more organized. Well unless Brendan and Jeff are bullshitting me they say that you know they're fully ready to pivot the entire organization to the cloud. We'll see what happens there. What do you mean? Like how do we focus on winning the cloud is their p0 from what my understanding is. If that's true, then we should have the coverage we need to pivot the engineering team towards delivering the features that make the cloud successful. That's not just...

[00:56:48.86] Remote : Yeah, no, but I think...

[00:56:49.86] Jason Vallery :  Jason ValleryThat's like... Yeah. I think about it. Most of the things that I feel like we need to go do are actually going to sit on the data layers, and it's going to be driven by Harvey, and not out of...

[00:56:59.10] Remote : Yes. we'll get everything that needs to be done done, you know, when it comes to the automation and marketplace and consumption models and, you know, whatever, right? We'll figure pricing, all that. It's really back to the point of, I don't think we'll be successful with the lift and shift.

[00:57:18.03] Jason Vallery :  Jason ValleryI don't think so either. I mean, I made this point on the call with Ron yesterday, but I would be incredibly surprised. skeptical if any of the hyperscalers ever delivered an instance type that allows us to be successful. Like, it's not in their interests. It's not in their engineering roadmap. It's just not part of the way they design things, and so our only opportunity to be successful is to build a layer over the primitives that the cloud provides, you know, object primarily, and so if we can build a layer over object in tree... if you look on Vimeo's cash instead of its durability, you have a chance, and then we're about extending the cloud into on-prem, and then that's where I think the whole story comes together. But that is a different operating model. Building a SaaS service is a different operating model, and so to pull that off, we're gonna have to go and really convince a bunch of people in Tel Aviv that that's. what's their need to build, they need to build next.

[00:58:10.83] Remote : - Yes, and they'll tell you that their friends out at Weka, a few, I don't know, a few kilometers away built that and they haven't got significant revenue out of it, right? So Weka has, you know, the ability to tier and dynamically add instances or add S3 capacity. behind you know behind those instances or whatever and you know it's it's nice but it's I mean it's it's not what they what retires people's quotas so so like what's the what's the temp for for for a solution like that well I don't know it

[00:58:50.34] Jason Vallery :  Jason ValleryDepends on where we get to in terms of price performance compared to the native cloud solutions, right? Like a customer is going to buy us towards the native cloud solution if it gives them adequate performance and a better price point. So you have to be able to beat the cloud itself on price performance, and you're going to have a hard time beating it on price because you're building on top of the cloud. So it has to be the performance that wins you the game, and I think that there is an opportunity to build a more performant layer. in front of object than what certainly Microsoft or Amazon has done, I think Google's questionable. I think Google's done a good job.

[00:59:22.93] Remote : - Google sale, I think I refer, I alluded to something last time we spoke. So, to Google still, I mean, so I've been in touch with, with a few people on the Google side for a long while, I think maybe over two years. There's a guy here who is one of their architects for storage, and recently, they've involved us with getting feedback to their next gen, you know, like high capacity instance, and I know they're talking to other vendors and all that. But sometimes, you know, I get people from Google call me and say, Hey, you know, we're trying to position, we're trying to sell data, tell me about SquarePoint. I don't, I don't think I have. So SquarePoint, there's a lot time BaaS customer. It's like one of the earlier BaaS customers and they they were maybe maybe I like I didn't mention their name but they were about I don't know six months ago or so they were evaluating on whether whether to expand their their data center or go to GCP because they're using GCP now. So they told GCP, here's what we need. I actually have the numbers, because on-prem, they're using VAST, right? So either going to GCP or purchasing VAST or more VAST on-prem, and the architect from GCP me say, "Hey, you know, SquarePoint is asking for this, this, and this, and we can't deliver it. Can you help us?" And it was because Google could never deliver what SquarePoint can get on-prem like any day of the week, and, you know, it's vast, right? And vast in building their own infrastructure. structure. So anyway, the team from Google tried to convince us to go and propose something together, like, but, you know, we're trying to get better shapes forever, right? We're, you know, we're trying to build something more optimized. Now you're saying, okay, use the current shapes and help me deliver this. I, you know, I don't think we can, unless... we put this in the roadmap and to work together, whatever. Oh no, no, that never, that will never happen because it's like too many, too many groups at Google that, you know, they need to be involved or whatever, right? So that will never. So anyway, we never went with them. They positioned Lustre, which is a hyperdisk in the backend, and I mean, I know it can't deliver what SquarePoint wanted, and... I spoke with the account manager at the time. He told me, "Yeah, don't worry about it. They'll never get that from Google. So just don't spend cycles on that." And that was indeed the case eventually. So I think Google relatively is doing a good job, but people.., and that's where, I mean, I had many interactions, I mean, not dozens. but enough interaction with, especially with GCP because there's a good alignment between vast customers and GCP, you know, we have a lot of joint customers. We had one call with Tower. I think it was tower. Yeah. tower is a very large GCP customer that is also a very large vast customer as I don't know like 60 petabyte or something I mean very large excluding you know the AI the Neo clouds or XAI or whatever so so anyway we jumped on the all, it was the first time the GCP account team was on together with us and the customer. The customer started going through like what they're doing with Vast and how they expect you know this to work on GCP and have it seamless between you know they want to they don't want to think when they spin up a pod for a set of users. right? They don't want to think whether it's on-prem or cloud, they just want to click a button and something figures out where and it deploys the same, same experience, same whatever, and then they shared what they're doing with VAST, and the Google theme was shocks. I'm not exaggerating. I mean, like there were such shocks that that's what we deliver on-prem, in terms of the capacity, the throughput, the the the ratio between you know through throughput and capacity that's something that you know that they they can't they still cannot achieve and very far from so so what many people at gcp yeah tell me i mean if we could just make object faster it will solve all our problems but they can't or whatever

[01:04:31.47] Jason Vallery :  Jason ValleryWhen I'm on Google's advanced stack, I think about a few things. I think multi-region buckets, like, it's killer. Having a multi-region bucket is awesome. They have this caching capability that allows you to, you know, if you're accessing your data from compute running different Google regions, you access a single S3 endpoint, but then it'll cache on read and in GCS storage adjacent to the GPU. So this is the global namespace working well, right? You've got promised durability at a geo level of like North America, so Google could lose an entire Google region. in North America and your data is still durable and available. You've got performance because it caches on read onto an adjacent GCS node in the region where the compute's at. So if you've got a GPU cluster in each Google region, like they all can access the same data set with the same level of performance. So that's the first killer thing that Google has that neither Amazon or And that's the thing that I think we could build, like we could really build that.

[01:05:42.12] Remote : We have, we have it. I mean, I need to show you global namespace, but it does what what you just mentioned.

[01:05:49.50] Jason Vallery :  Jason VallerySo that's, that's point A, Google just has that out of the box, and then their GCS fuse drive is pretty cool too. Like the idea and I don't know if you've looked at like coral reefs load of cash. they're going to do the same thing but like you know we talked about this in this last call where we're talking about the the marketing place but you know when you think about a GPU cluster that's got you know eight thousand twelve thousand whatever you end in GPUs in it the way those are provisioned today is there's a one-to-one mapping between a local NVMe disk and the GPU, and those local NVMe are 16 terabytes. So if you think about you're in a world with 16 terabytes times 100,000 GPUs or something, you're talking about several hundred petabytes of NVMe, and that being able to fully leverage that in a tiered manner. logically extending the distributed system that is the storage platform into the GPU hosts itself is killer, because then you're getting all of that capacity at an even more performant basis, and so Cori's load to catch is this, where they're using that local NVMe on the GPU host to catch S3 on the GPUs themselves. So, and Google has this with GCS Fuse. So, if you take this idea that you're leveraging the local NVMe, the adjacent GCS racks against a global namespace, you now have like super ultra performance storage that is durable and consistent across like North America, no matter where the GPUs are located. Like that doesn't work on my--

[01:07:28.43] Remote : - Do you know if they have, caching capability has some some sophistication around prefetching and

[01:07:38.81] Jason Vallery :  Jason ValleryPredictions and things like that? Yeah, so Google has that. There's like a policy engine behind it so you can set policies around prefetching namespaces or like prefixes in the namespace but that's exactly what's needed is you need to be able to set like a file glob pattern or a prefix or whatever so that it matches it via policy, and then, you know, the cash on read, you know, those kinds of cases are all valid.

[01:08:04.68] Remote : - Yeah. So we have, yeah, so we've built some, I think probably, you know, additional sophistication around this. So what can you describe? is what we call explicit pre-fetching. So you can submit an API call or use the GUI to pre-fetch, to pre-warm the cache, essentially. It doesn't mean that you need to wait for that task to complete, right? You can start. So, I mean, by default or just the default behavior is that we'll fetch whatever the client requests, right? So, well, that's, I think, kind of, we're starting from the, from the middle. I can, I can kind of walk you through global namespace, just don't have energy.

[01:08:48.54] Jason Vallery :  Jason ValleryYeah, yeah, sure. I was just, um, I don't know if you've looked, like, I would recommend you go read the Anywhere Cache documentation from Google, like, in all of the different cases it supports, that's sort of just pulling up. But yeah, why don't you show me what we got, like where the constraints are.

[01:09:05.68] Remote : - Yeah, I think it's not constraints. Actually, I'd say advantages. Let me pull up like one of the earlier decks that I've put together for our global namespace. I haven't opened it in a while, but I conveniently, I called it Global Nations Original. to, yeah, okay. So with our global namespace, it's not just reads, it's read/write, but let me, this is like a really early slide. or slide deck, you see this? - Yeah. - But I wanted to go through some of the, like no fluff and some of the fundamentals. So when we, we looked at like, how do we want to do global namespace, right? There's several decisions we want, I mean, we had to make and there is, like different approaches, right? You could do eventual consistency or strict consistency, for example, and we decided that we'll do strict consistency. So, I mean, the goal was to have you know, kind of zero administrative overhead, so everything just works, right? These are, I mean, these are this part of the mission statement, right? Everything just works. read/write access from everywhere, perform I/O as locally as possible, as much as possible, strict consistency, as I mentioned, and then decouple resiliency from access, right? So we don't wanna use things like synchronous replication or any type of data protection style features to achieve those. can combine data protection with data access features, right? Or we used to, we wanted to, right? When we've put this together, we thought, okay, we'll have features around data availability and features around data access, and sometimes we'll use a combination of both, right? So this is where we started, and then we're like, okay, why not? it's good for right so let's say and this is again pretty early and naive but but let's say i have this vast new york with slash data and i have vast london and with slash research and i have this cluster in the middle that want to borrow some data from each right and it could be much smaller right as long as they've had enough capacity to host a working set, right, like I said, the cache size to match that, right? And if it's, if the working set doesn't, you know, doesn't match or doesn't, is bigger or whatever, it's okay, right? Your cache won't be as efficient, but you can still operate. So, so one of the, you know, I spoke with a lot of customers about this, you know, cloud and no cloud or whatever. I think one of the, well, several findings that that I've had from those conversations, I'm talking, you know, several years back, one of the major pain points was that, well, before pain points, right, so, so, like, a significant number of the vast customers had large data repositories on like various locations then they needed to be accessed from other locations right then it could be you know one central repository with many sites reading from it or other locations or multiple repositories are everywhere and then people or machines need to access those from from other places. One of the main challenges was that they didn't know what data is actually accessed, or jobs that ran didn't really define exactly what data, you know, which file they actually need, right? So they ended up copying, or I mean, a lot of them still do, right? They copy everything everywhere, or too much data everywhere, right? And so being more opportunistic, being able to start a job immediately, but maybe have the job run a bit slower instead of waiting for all the data to be copied and then start the job, right? You gain a lot, and you end up copying less data. good job fetching the data and trying to understand what the client needs, right? So that's that's kind of the background for this. So far so good. Any comments? That makes sense. Okay, so, you know, we talked with customers, it's about cloud bursting, right? You can see this is this really, I mean, Disney at the time, they did some rendering in the cloud, and they said, well, we use this KNFSD from GCP, right, to cache some data in the cloud, and it's whatever, right? And it sucked, right? They had a lot of issues with it, and whatever, okay, we... can do better, and so had all those scenarios of what this is good for and all that. But this is the part I wanted to get to, and there's some animation. You can see the slide now, right? So the way that global namespace works, and we don't have to dive deep into this is that, let's say, I mean, this is an animation that describes, you know, how things work. So I have this vast New York, and there's slash assets here, vast assets, and let's say someone created a file, call it matrix video. in New York. The first thing I do when I want to extend that namespace to somewhere else is I build trust, right? I build peering relationship between this and another cluster, another bias cluster, and that peering relation is something we have for various features, right? It's not unique to global namespace. we do async replication, sync replication, for snapshot replication, things like that, and so we have this peering to be encrypted, and then I can represent the slash assets in London as well. It doesn't have to be called the same, it could be a different, it could be named, I don't know, assets New York or whatever, but everything... So for clients in London, this will look like a local file system, a local path that they can access and as soon as they access data, so let's say this guy want to open this matrix video, what happens is that we'll start fetching the data and together with the data, this cluster in New York will grant what we call a read-lease, right? So, we have this leasing mechanism, essentially a metadata indicator, right? So, in New York, there's the metadata tree that the cluster holds, and it says, okay, I'm obviously oversimplifying, but it says for matrix video, So for this matrix.vid, I granted the lease to London, and then London can cache this data as long-- I mean, it will cache the data and will maintain the lease and conserve the data locally. It doesn't have to wait for the lease to arrive. It can-- through and serve the data and then the lease arrives later. These are asynchronous processes but but the idea is that it can serve the data locally as many times as it needs to because the data is cached and it has a lease that was granted from New York. Now, so how do you...

[01:17:38.90] Jason Vallery :  Jason Vallery- One thing you said there that I don't get around is, you said the lease could arrive after the data. My assumption here is that the lease is used to invalidate the cash. So if you don't have a lease to read it, how do you trust that it's not dirty?

[01:18:01.40] Remote : Yes, so on an initial read, the data is almost like a pass-through, and also there's several going on there is uh there's um internally there's like a snapshot id or something like that there's there's counters that that allow the the cluster here to stream data as it arrives directly to the client without uh without having to wait for a lease and that that's only for this initial initial phase. So if I open a file, how do I, I mean, you could really see this in some scenarios where you read the same file multiple times, and then eventually it will, will get faster or it will get local because it got the lease but up until then it streams the data directly to serve the client requested so maybe I'm not representing it correctly but the the lease is there. for when the, so, well, another important point, right? The lease is at an inode level granularity, right? So it's assigned to a file or something that the client might request a part of a file. I mean, almost often it will request a part of a file and then we'll serve that directly. and then we'll pre-fetch the rest of the file and then assign the lease for it. Right, that's another case where a lease will come later. Make sense?

[01:20:04.02] Jason Vallery :  Jason ValleryYeah, I mean, the way I'm thinking about it is that London is effectively a reverse proxy back to New York until the lease is established.

[01:20:10.94] Remote : Yes.

[01:20:11.94] Jason Vallery :  Jason ValleryThe lease is established, you can actually cash.

[01:20:15.06] Remote : Yes. - Yeah, I guess that's a good way of putting that. - How does-- - Yeah.

[01:20:23.82] Jason Vallery :  Jason Vallery- How does the, and maybe you'll get to this, how does the lease and the catching behave if this connection between London and New York is severed? So if I'm sitting in London-- - Yeah, yeah. but the lease is severed back from New York and I can't trust that it hasn't been expired, what happens?

[01:20:44.31] Remote : - Yeah, so I'll get to that in a second. So that's an important point, and that's where, I mean, some of the implementation details around having, you know, making sure that this is, this has strict consistency, right? So maybe I'll just say now, right? So if this connection breaks. rates, then the client, I'm sorry, the cluster here will stop accepting rights until timeout is a certain timeout is met, and during which this lease will expire automatically. So this I'm sorry, this cluster will see that there's no connection to the source, right? We call this the origin. This is the origin. This is the satellite, in our terms, right? So the satellite will see that the origin is not, there's no keepalives or whatever. It will invalidate its leases after a certain timeout. That is the timeout where this origin cluster... will hold rights to make sure everything is consistent. Only after--

[01:21:53.10] Jason Vallery :  Jason ValleryJust to clarify, only on files that have an established read lease? Or on all rights?

[01:21:58.47] Remote : No, on the former. Makes sense. Yeah. mean that's it's pretty straightforward the and then I mean this is just explaining that this is file level granularity right so if someone creates an avatar video right it lands here nothing happens but if someone were to modify and I obviously oversimplified this but if someone were to modify this matrix video as soon as this data hits this cluster before or before acknowledging the right, it will invalidate the lease here and then acknowledge the right. So I mean that's some of the basics. Now a lease is granted and then caching is applied to like any piece of data right so like metadata is also data that is fetched and has a lease assigned, right? So if someone LSs, we'll fetch the data for the result of DLS and we'll apply a lease to it and we'll cash it, right? So the next LS will be local. There is pre-fetching that is also in place refer to, refer to earlier, right? So we do prefetching in, in various levels, and there's some, some capabilities that we're improving with every release on trying to predict what the client would want next, right? So an obvious one is as soon as you read a part of the file, we'll fix the rest of the file. works really well for like video or large files where the client reads data in pieces, in chunks, and so we'll fetch the byte range that the client asks for and then fetch the rest and you know sometimes we'll fetch the rest before the client needs it, right, and you'll see you know things things work out. So you'll see, you'll have a hit on the first read and then it kind of evens out. But we also have some other capabilities to detect patterns. So if you're reading file, you know, I don't know, file A, B, C, we'll prefetch file D, E, F, or whatever, right? if you ls will, I mean this is the setting, right, but you can prefetch subdirectories so that if you traverse a tree then you don't have to wait for every time you go one down. So there's some smarts around this and there's also what we call explicit prefetching. that explicit prefetching allows you to say, I want to prefetch or pre-warm the cache for, and that's what I was covering earlier, right? So for a dataset, I want to prefetch that, but you don't have to wait because, you know, the caching and the global analysis will work regardless, right? So you can start the prefetching. and let's say five minutes, and if the entire pre-fetching takes 10 minutes, you can start your job five minutes after. Statistically, some parts will be faster, other parts will be fetched on request. So it really depends on the workload and the number of files. right, the size of files and things like that all right now that's exactly what I wanted to cover so right now when when the rights go you know they go all the way to to the source now this is a part I mean like I mentioned this is an early really early deck so I don't have the next thing about right so we've released this in 5.1 now we're in 5.4 now so I mean this is this is a year and a half old I mean this feature right it's been here for for a while and we improved it over time so we started just just to give you full background right because we're multi-protocol you could access you know any file or any element right with through you know the various protocols right right on nfs read on smb or whatever right object when it comes to global namespace we started with nfs only So we were multi-protocol on the origin and NFS v3, right, because stateless, right, much easier. So we started with that on the satellite, then added SMB and recently added S3. So we have now multi-protocol on the other side, and, um... we've done other optimizations over time so and some knobs and stuff that work much better and the last part that we're adding now and this is scheduled for 5.5 right as i mentioned we just raised for 5.4 5.5 is scheduled for around march time frame and this is where we'll have right leases. Right leases essentially would work in a way that within the name space there could be an origin and multiple satellites but there is there could be temporary owners that will hold the kind of the for the piece of data, right? For the segment, for, you know, we'll still, I don't know exactly the granularity that we'll settle on, but if you could have a name space where the origin is not doing any rights, and now we detect heavy rights coming from satellite A, we'll forward, we'll essentially grant that. that satellite A, a right lease, and for that period of time, it will be able to accept those rights locally and will converge them to the origin, right? So there'll always be that source of truth, which is the origin, but we could temporarily accept rights from other places.

[01:28:24.04] Jason Vallery :  Jason Vallery- Yeah.

[01:28:24.88] Remote : - And...

[01:28:25.70] Jason Vallery :  Jason Vallery- Today, rights are always flushed through the origin - Correct. - Oh, there'll be like file level or by range level leases that we handed out to the sellers. Yeah.

[01:28:39.59] Remote : Yes, and the thing is that it's like, so when I initially defined this and, you know, worked with the architects and everything, I thought, you know, we'll need read leases right after. you know we release this or whatever. What we found is that I mean most of the use cases are reads right I mean it's rarely that when you have a data set right and it's true for you know media and entertainment you know market data right that never change and no one writes back to the source of the data the and and the global main space is past level granularity and kind of could be one too many but both directions. So I could be a satellite for path A and an origin for path B. So if I need access to this large data set of let's say market data, the results from my processing or the output folder could be outside of that global namespace path so I can write locally but this could also be local global namespace where I'm the origin for that for that output path right and that's what we have you know kind of that's part of the best practice right now and that took us so far. without a lot of customer demand for write pieces. Yeah, I mean, it's a really cool feature. It makes things better. I mean, in some cases, some use cases, you must have write pieces, you know, like on directories and things like that. But the majority of the use cases, it's really... just a data set that you never write back to, or rarely write back to, because it's this big repository. I don't know if that aligns with your experience in AI or whatever, but...

[01:30:37.45] Jason Vallery :  Jason ValleryI think the one case in AI that would be interesting is checkpoint repositories. So, you know, if you've got a fleet of GPUs around the globe all checkpointing to local storage, Often, you'll want to recombine those back into a single global checkpoint repository, and so they would all be checkpointing out on the edge, and then you'd be wanting to re-aggregate them together. But you wouldn't have file collisions. Each GPU would be creating its own file, and there wouldn't be file contention and locking issues. It's a pretty straightforward case, actually.

[01:31:11.66] Remote : - Yeah, yeah, no, okay. Yeah, I can imagine having, I mean, you could have your, I don't know, like this big origin that has various paths in separate global namespaces across the various regions, I mean, every one of the regions will have the read lease for that, like, almost constantly it will have the, I'm sorry, the write lease almost constantly for, because you would keep, it would write the checkpoints locally there. But the owner, the real owner is actually the origin. So data will be consolidated back into this one place. So I mean, we. - Could do that, yeah.

[01:31:55.21] Jason Vallery :  Jason Vallery- Your question, let's imagine you've got read and you've got central storage and you've got N number of satellites. So satellite one is checkpointing and you're now checkpointed local and you're now async copying the bytes back to the central storage. But then like satellite three wants. to read that? Like does it proxy the read back through to the origin and the satellite or does

[01:32:22.82] Remote : it block the read or how does that work? Yeah, this is to be honest with some, I mean there's some detail that I'm really waiting to discuss in Tel Aviv when I'll be there. There is the, some religious conversation you know like it's people are very strongly opinionated on how we we should do those those kind of things whether uh whether we can redirect where we need to go through the through the owner whatever and that's why so i mean some of the details are on rightly are not, I mean a lot of videos are not not confirmed and also the implementation will take a while so the version 5.5 I didn't mention that and you know I mean I was a bit in a selling mode I guess with the timelines but 5.5 is is like a preview it's it's like for early adopters or whatever a lot of the lot of the corner case and everything I mean since they're having to find that it will take a while for us to to develop. So anyway, I don't have

[01:33:37.21] Jason Vallery :  Jason ValleryAnswers. Yeah, I don't want to. No, this would be a good cloud conversation, because ultimately, this is where I see BASC disrupting the hyperscalers. I think this is a huge, huge capability that would unblock a bunch of Azure customers. If you could do this at scale across Azure regions is ultimately what we're really talking about is doing this across Azure regions, plus the new clouds, plus across clouds, right? Like ultimately you get to that world. Cool.

[01:33:59.83] Remote : Yeah, I mean we have customers using it for on Prem. I mean just like really cool use cases where. we have a customer here in Houston, medical, did I mention that? I mentioned that to someone this week, I'm not sure if that was you, but like they have medical devices generating data on, I mean those devices connected to say cluster A over SMB. on, like, one side of a campus, and then on the other side of the campus they have a mini-HVC, right, with a much larger vast cluster and a lot of compute power, and there's a global maintenance between the two, and the HVC speaks NFS and borrows whatever data is needed, right? not doesn't have to store everything and like they're super happy with that it's like and it's i mean they could mount i mean it's same campus right they could mount and that's what they did before right mount the data from um from from that cluster to the clients but but you know you have It's, I mean, it's just works better, and what we've added recently is the ability to use async replication with snapshots for the same dataset between the same source and destination. So between, so the satellite and origin become destination. and source for async replication, right? And then you have, so maybe I need to explain how we do async replication fast, right? Which is similar to NetApp and others, but essentially you take a snapshot, right? You replicate it to the other side. The other side has the same. path as read-only right because it's incoming snaps data is refreshed every time an incoming snap comes in right so it will be the next the next the next snap that is the main and then we have a dot snap directory right the dot snapshot directory within that app a hidden directory that contains the historical snapshots based on your policy, right? So you have read-only domain directory, right? And then read-only historical snaps, right? So that's async replication. Well, now I can add global namespace on top of that, main is not read-only, it's actually read/write. So, because global nasus gave me this access, right, and async replication gives me this point in time that keeps accumulating, right? So if everything is connected and it's fine, I have I have access to data from both sides. right read you know read write active active whatever right so I can leverage let's say if I if I if my site B is dr you know I can I can leverage the compute in that dr side because I have live read write access to the same data set whereas before global namespace I could only protect the data I get a read only copy if I do something on it it split brain right now I eliminate all that um and um it opens up a set of use cases that's what I was alluding to earlier when we envisioned that we envisioned exactly this right this is a new feature was just released with 5.4 um and it was it's the combination it's the protection and add access separately, but together where needed.

[01:37:57.63] Jason Vallery :  Jason Vallery- If I wanted to get my hands on, like I would love to get, can you run an e-box virtually? Like, can I just run a few of the e-boxes?

[01:38:09.72] Remote : - It's, it's a, it's a beast. I mean, you could, we have, we have an OVA. that we hand to partners everything but, or, I mean, it's, it's heavy, you won't be able to run it on a laptop but speak with Andy Persteiner, the Andy Persteiner that I mentioned the office of the CTO, his team he has a couple of people that manage the SC labs and all that's kind of the demo environments or whatever. I mean, he will onboard you and get access to a real live environment.

[01:38:51.68] Jason Vallery :  Jason Vallery- I have, actually here in my house, I do a lot of like tinkering with AI models and just kind of hosting things. So I actually have like a three node cluster here. It's pretty beefy. I was thinking I could just run three virtual machines on it and connect them together as remotely as possible.

[01:39:05.27] Remote : Yeah, yeah, I mean, yeah, if you have, I thought, you know, running on your laptop, but, yeah, so, um, so, you know, I'll, I'll just, there's instruction view and name.

[01:39:18.24] Jason Vallery :  Jason ValleryWhat's the like minimum number of e-boxes to get a cluster forum, is it three?

[01:39:22.14] Remote : No, no. I mean, you don't have to. - Just, if you wanna, what's going on? Like I have, I was presenting to you and I have like 40 messages on Slack.

[01:39:37.92] Jason Vallery :  Jason Vallery- Yeah, that's what happens, right? No, I was, for me, I have a few things. For me, I'll like, I'll move data over into it and, you know, actually try to use it for. real on various things.

[01:39:48.13] Remote : - So speak with, I'm sending you. So Josh is, I just sent you over Slack. Josh works for NDParent Center. I'd say he's in the office of the CTO. He helps on board. partners for them to use our OVA for like integration testing API testing whatever so he can get you to I mean officially there's a process to to get on board I'll get the code get the bits and stuff but just ping him tell him you want to run it and he'll get you set up I have to find time to do it

[01:40:33.86] Jason Vallery :  Jason Valleryit but maybe in the next couple of weeks i'll find a minute to do it so it'll be good yeah but if you

[01:40:37.82] Remote : Just want to tinker click the gooey whatever right i mean just just uh just get on the sc lab yeah i

[01:40:43.38] Jason Vallery :  Jason ValleryMean i i'll do that but for me it's like i run um i have a pretty big synology nas here that i typically use and then like i actually would move data over to it and play around with it and actually try to like you know connect it up to my gpus and see what i can do with it hard to do that in a remote lab is all yeah no i mean yeah you need the data yeah okay well thank you for all your time today happy friday yeah of course and um we'll find a time next week to Yeah, keep going. I appreciate everything.

[01:41:19.14] Remote : Yeah, of course. No, this is fun. I'm glad you're here. You know, a PM to talk to, like a real PM to talk to here at Vast, right? I'm beside my team, but we're so busy with the day-to-day and stuff. So, you know, never have a chance to think about it. about, like, you know, take a step back and improve things at a broader level. - Yeah.

[01:41:47.22] Jason Vallery :  Jason ValleryThat'd be good. We're going to get some maturity around here and a lot of opportunity in the cloud to go in. So I'm excited.

[01:41:55.44] Remote : - I am too. Cool. Okay.

[01:41:59.98] Jason Vallery :  Jason Vallery- Nice to meet you.

[01:42:00.80] Remote : - Happy Friday. I'm going to go ahead and do that and see if it works. So I'm going to go ahead and go to the website, and I'm going to go to Google, and I'm going to go to Google, and I'm going to go to the Google Play Store, and I'm going to go to the Google Play Store, and I'm going to go to the Google Play Store, and I'm going to go to the Google Play Store, and I'm going to go to the Google Play Store, and I'm going to go to the Google Play Store.Music (keyboard clacking) (keyboard clicking) Applause SilenceBLANK
```

<!-- ai:transcript:end -->
