---
type: "group-meeting"
created: { { DATE } }
title: "20251028 1200 Parallels Transcription"
participants: [{ { PARTICIPANTS } }]
ai_extracted: true
transcript_path: "00 Inbox/Transcripts/20251028 1200 Parallels Transcription.txt"
tags: [meeting, group]
---

# 20251028 1200 Parallels Transcription

**Date:** 2025-10-28  
**Participants:** Jason Vallery, Asaf Levy, Eirikur Hrafnsson, Jonsi Stefansson, Kirstin Bordner, Lior Genzel, Ronnie Lazar

## Summary

Cloud BU leadership aligned on a dual-track strategy: (1) ship GCP MVP via marketplace with strong collateral and demos, and (2) pursue hyperscaler-scale opportunities (e.g., MAI 160k GPUs) with a hardware-optimized story and Polaris-managed operations. Urgent collateral gaps persist for GCP launch and supercomputing. Engineering confirmed GCP MVP will use routable IPs and flagged QA/support readiness, deployment validation, and maintenance handling as top risks. Team to adapt the Enscale deck for a Microsoft-led Friday session, with Jason meeting MAI PM lead. Marketplace activation and pricing tuning underway.

## Key facts learned

- GCP MVP is close to launch; demos needed for Supercomputing and CSP booths
- Collateral gap: Product Marketing (Brian) assigned; initial deck exists; battlecards/datasheets pending
- Two-track strategy needed: enterprise/marketplace vs hyperscaler first-party sell-to
- Performance data exists: ~90% theoretical read, ~50‚Äì60% write on GCP v1; no firm KPIs defined
- Need clear, comparable performance/TCO frame across clouds and instance types
- MAI opportunity: ~160,000 GPUs; ask for VAST guidance; Friday follow-up planned
- Adapt Enscale solution/deck; emphasize Kubernetes-led control plane (Project Apollo) and Polaris
- Avoid CoreWeave-style lock-in on any Enscale resale; retain feature/control
- GCP MVP to use routable IPs; customer must provide IP range
- Marketplace: existing blanket private offer expected to avoid new approvals; pricing components must be tuned
- Maintenance/HA: Google scheduled maintenance only; handling via VM migration/serialization; 8 failure domains in GCP; Azure typically 3
- Supercomputing presence: VAST booth + Google and Microsoft booths; ~10 end-user cloud meetings planned
- Key dates cited: Ignite 2025-11-18 to 2025-11-21; re:Invent 2025-12-01 to 2025-12-05; planned Iceland trip 2025-12-08

## Outcomes

- Agreed to run dual strategy: marketplace offer for enterprise bursts and targeted hyperscaler-scale engagements (MAI/OpenAI/UKMET)
- Commitment to deliver MAI storyline, diagram, and deck by Friday using/adjusting Enscale material
- Jason to review positioning and drive a comparable performance/TCO benchmark frame
- GCP MVP networking settled on routable IPs; deployment flow to collect customer IP ranges
- Proceed with marketplace activation leveraging existing private offer; tune pricing components
- Polaris must be included for lifecycle management in both tracks to avoid value abstraction

## Decisions

- Pursue dual-track go-to-market (sell-through marketplace and sell-to hyperscaler-scale deals)
- Use routable IPs for GCP MVP; defer alias IPs/SaaS Runtime until post-launch
- Adapt Enscale solution for Microsoft/MAI with Kubernetes/Polaris emphasis
- Retain contractual control to avoid feature lock-out in any Enscale/CoreWeave-like resale

## Action items

- [x] Share GCP performance Excel and introduce Jason to the performance lead and product marketing owner for the field calculator @Lior Genzel ‚è´ üìÖ 2025-10-29 ‚úÖ 2025-10-28
- [x] Push Product Marketing to deliver customer, internal, and CSP seller decks plus battlecards/datasheets; share latest deck @Brian ‚è´ üìÖ 2025-10-31 ‚úÖ 2025-10-28
- [x] Review decks/positioning and propose a standard performance/TCO benchmark frame across clouds @Jason Vallery ‚è´ üìÖ 2025-10-31 ‚úÖ 2025-10-28
- [x] Schedule and run internal working session with Enscale technical team to refine the MAI storyline and architecture @Asaf Levy ‚è´ üìÖ 2025-10-29 ‚úÖ 2025-10-28
- [x] Deliver MAI presentation (storyline, solution diagram, deck) aligned to Kubernetes-led control plane and Polaris @Cloud BU engineering üî∫ üìÖ 2025-10-31 ‚úÖ 2025-10-28
- [x] Meet with MAI‚Äôs Kushal Datta to align on requirements and next steps @Jason Vallery ‚è´ üìÖ 2025-10-31 ‚úÖ 2025-10-28
- [x] Reconnect Product Marketing with Jason/Yonsi and include Polaris/marketplace content requirements @Lior Genzel ‚è´ üìÖ 2025-10-29 ‚úÖ 2025-10-28
- [x] Implement GCP MVP deployment flow using routable IPs and require customer-provided IP range @Eirikur Hrafnsson ‚è´ ‚úÖ 2025-10-28
- [x] Kick off Salesforce integration for marketplace transaction flow and data sync @Cloud BU ops üîº üìÖ 2025-10-29 ‚úÖ 2025-10-28
- [x] Tune GCP marketplace private offer components and pricing; confirm no new approvals needed @John Downey üîº üìÖ 2025-11-01 ‚úÖ 2025-10-28
- [x] Harden maintenance handling (migration/serialization) and validate across failure domains @Ronnie Lazar ‚è´ ‚úÖ 2025-10-28
- [x] Ramp QA and support playbooks for GCP MVP, including break-glass procedures @Shachar ‚è´ ‚úÖ 2025-10-28
- [x] Produce end-to-end GCP demo video (deploy via Polaris CLI/UI and final state) for booths @Cloud BU engineering ‚è´ ‚úÖ 2025-10-28
- [x] Clarify licensing/packaging for Polaris as VAST-as-a-Service for neoclouds @Cloud BU leadership üîº ‚úÖ 2025-10-28
- [x] Ensure any Nscale resale preserves VAST feature/control exposure and avoids CoreWeave-style lock-in @Jason Vallery ‚è´ ‚úÖ 2025-10-28
- [x] Ask Google about maintenance overlap guarantees across failure domains and document guidance @Eirikur Hrafnsson üîº ‚úÖ 2025-10-28

## Follow-ups

- [x] Report out from Friday‚Äôs Microsoft/MAI session and align next steps @Asaf Levy ‚è´ üìÖ 2025-11-03 ‚úÖ 2025-10-28
- [x] Finalize two collateral tracks (enterprise/marketplace and hyperscaler-scale) and distribute to field @Brian ‚è´ üìÖ 2025-11-04 ‚úÖ 2025-10-28
- [x] Confirm GCP marketplace offer activation and readiness for private offers @John Downey üîº üìÖ 2025-11-05 ‚úÖ 2025-10-28
- [x] Revisit Google SaaS Runtime option for improved preflight/testing post-MVP @Cloud BU engineering üîΩ ‚úÖ 2025-10-28

## Risks

- Collateral not ready (decks, battlecards, datasheets) ahead of Supercomputing and GCP MVP
- QA/support ramp and break-glass procedures may be insufficient before launch
- Customer environment variability (quotas, perms, firewalls) could derail deployments; limited preflight checks
- Marketplace pricing/component tuning may slip and affect offer readiness
- Maintenance/upgrade concurrency across failure domains could impact redundancy/perf if not serialized
- Azure‚Äôs 3 failure domains constrain perf/usable capacity vs GCP
- Lack of standardized performance KPIs complicates field messaging and CSP comparisons
- GPU allocation triage at CSPs can block customer wins despite demand
- Very tight timelines for demos and MVP readiness

## Open questions

- What standardized performance/TCO KPI framework will we use for apples-to-apples cloud comparisons?
- Will Google provide alias IPs or improved networking options post-MVP to simplify deployment?
- Can GCP guarantee non-overlapping maintenance across failure domains for our HA design?
- Who owns delivery of the field sizing calculator and by when will it be available?
- How will Polaris be licensed/packaged when offered as a managed service to neoclouds?
- What exact demo content and deadlines are required for Supercomputing and CSP booths?
- Are there large-scale (multi-petabyte/exabyte) opportunities in GCP/AWS to influence provider hardware SKUs?
- What are the final marketplace pricing components and discount structure for the GCP MVP?

---

## Transcript (auto)

```text
[00:00:00.04]  Jason ValleryI'll try to find the time for us to do a little more formal intro one on one just to get to know each other.

[00:00:18.07]   RemoteBut yeah, it's been a fun first week.

[00:00:22.51]  Jason ValleryInteresting. So where are you? I'm located in Colorado, just outside Boulder. I don't know if you're familiar with the area, but yeah what about you? I'm in Tel Aviv, near Tel Aviv. Yeah. I'll be there, I was just talking with Sahar, I'll be there the 23rd through the 26th so we'll get a chance to meet. in person at that point.

[00:00:47.01]   RemoteThat would be great.

[00:00:49.35]  Jason ValleryYeah. Are you in Orlando today, Jansi, or back in Iceland, or where are you at?

[00:01:01.01]   RemoteI'm in Orlando. I'm handing over for the November 17th and then going back to Iceland yeah I don't know when I'm flying because I have to be at ignite 18th until the 21st yeah and then there is a reinvent December 1st to the 5th and then you and Jeff are coming to Iceland on the 8th, right?

[00:01:36.10]  Jason ValleryThat's the plan. I haven't booked anything yet but that's the plan. Hopefully we get a crop date but yeah, the 8th of December.

[00:01:46.72]   RemoteCool. that we are getting very close to launching our MVP in Google. My worry is that we haven't created a lot of collateral for our field. Just thinking like... Yes, Leor, is Tiffany and Olivia working with product marketing on that one or? There is a guy named Brian, product marketing assigned to it and he's been tasked for the past three weeks to create collateral and I don't think he created much. So it's not up to Tiffany or Olivia to create it. it's a we can we can ask you know brian should be on this call his product marketing okay and he created a v1 of the of the deck that can be shared with customers he started working on a draft of battle cards but uh again it's either he's getting other priorities or it's slow to move one of those things but not much has changed since this is stiff. This is different from like Ronald Cohen and his team and what they want to write a solution. No, no solution product marketing reports up to million to marketing, and the guy named Brian that was assigned to us, and again, very slow. to create collateral, but the request was to start with a deck, basic deck for end users, and then a deck can be used for internal training, and then again a variation of the deck that can be used with the GCP sellers or the CSP sellers, and battle cards, data sheets, just the basic stuff. So we do have slides that we created in the past, but it's like kindergarten. It's not slides that you can really be proud about. I saw some of the collateral content that we had and it basically says nothing. I mean one of the key things that we have to call out is our unique capabilities of global namespace and have everything globally cataloged. So the deck is around that. The deck is really around burst in global namespace. It's not a generic deck about Rust. It's really targeted on that. I will ask Brian to to share the latest and greatest, and I would love to get your help, Yancey, in pushing this agenda. I've been begging for collateral, like I'm getting a request for collateral every day, and what we have is something... I do not want to share, but it's just not good. No, I mean, I think it's really good to get, Jason, your eyes on it as well as the PM for, you know, but the other thing, the other thing I'm worried about, okay, we have to be able to deliver the video demo of. Google, you know, the full process, the deployment, the Polaris CLI, the Polaris UI, you know, the deployment part of it, and the end state, because we are in supercomputing, we are all, we have our own booth where that demo will be. and we have a shared booth. We are also going to be present in the Google booth at Supercomputing. We have a specific presentation at Google booth and at Microsoft booth where we have the opportunity of showing. something that people will care about, and at the same time, they have a presentation in our booth as well. It's supercomputing, it's bidirectional, and we also have about 10 end user meetings with people that want to talk about cloud, and coursing those two NA AWS is where nothing with OCI. So Brian was to talk with you and to create some of the slides were supposed to be the Polaris or the marketplace integration slides yeah so I think you didn't know we talked about those like a month ago or something yeah so it was supposed to reach out last week so I guess it didn't no I was away on Friday, so I don't know, maybe. - Okay, I will reconnect it. Jason, what's your involvement with product marketing or with creating slides? How do you see yourself being a part of it or not being a part of it?

[00:06:39.57]  Jason Vallery- Well, I certainly wanna make sure that I have an opportunity to provide feedback in how we position this. You know, the thing that's top of mind for me is if we're going to go out with this, we need to have the price-to-performance marquee conversation. So what I've started already looking at is how that stacks up. What I don't have is a sense on the performance dimension. Have we done a benchmark of the GCP solution yet? Do we have performance numbers for that?

[00:07:07.68]   RemoteThat launch. So we do. We have performance numbers, and I've reviewed the numbers with a guy Maroon. So there is, we can look at the numbers, right? Best thing, you know, technical guys just look at the numbers. But what we kind of did, and we did the same for UK Met office, and you were on the call with with Nico where he said if you guys can do the performance you claimed you can do, then you won the project, right? Because they took the assumptions we gave them. The assumptions were based on what is the theoretical numbers we can get out of a system or a server or a VM, assuming it has 100 gigabits of network interface or 200 or 400 because that's-- the limiting factor for these configurations. So the GCP numbers are, Ronnie correct me if I'm wrong, but roughly 90% of the theoretical read numbers and roughly 50-60% of the write numbers, which is a decent place to land for v1, for the beginning. So we do have performance numbers from GCP, and I have also asked Maroon to share the numbers. There is another guy, I forgot his name, who is supposed to be sharing the numbers of performance with the field, with the technical field. I can give you his name. So there is a process at VAS of populating performance numbers to the field and enabling a calculation for people to size the system. I can connect you with with the person who is in charge so you can look at the numbers yourselves yeah, and get into the Okay, so but that's that's in play and and we had an email exchange with the software's on the call earlier today Talking about what's the expectations from Azure? What I shared is if we can get to offer the same in percentage, right? Not the same numbers because it depends of what? we get. We get to roughly the same numbers that we go to GCP with Azure, it would be a good place to land. Like again, they can always get better, but we never defined performance KPIs, at least not that I'm aware of, for Boston Cloud and said that's the numbers and we must eat those numbers. What Roni and Tim did is they optimized the numbers, the best way they could, but there was never a definition of a certain KPI. Asaf, Rony, am I wrong here? You are on mute, Rony. Rony, go ahead. I'm walking in the street, so it's a bit hard for me to talk. I was just saying that you're right. Absolutely correct. So Jason, I'll share the Excel sheet of the performance numbers and I will share an intro to the engineering guy who has led the performance who is collecting numbers and the product marketing person who is supposed to be kind of the OCTO person who is supposed to create those numbers as as a calculator for the field to use even when we say that we're ready to go.

[00:10:07.69]  Jason ValleryPeople. So one question I'd like to ask the group is how we want to think about representing performance numbers for these solutions in a way that customers can do apples to apples comparison, and so what I'm getting at is when I think about this from a traditional storage perspective, I would have represented throughput per petabyte, where the throughput scales linearly as data capacity grows. But in this world, that isn't a true statement because really what we're saying is a throughput per instance count almost because the constraint being the NICs and various densities of drives across providers and deployments. So how do we want to send a marquee message of here's the performance you can get with vast on Google, and how does that stack up to. to Google File Store and email and so forth.

[00:10:57.79]   Remote- Let me share my observation first. So when I looked at numbers and when you compare the numbers to Managed Lustre, then I think you don't wanna get to throughput per petabyte because Managed Lustre, including Azure, you can get to a very big number. announced their managed cluster, they said that with a petabyte they can get to a terabyte a second of performance. Now because we're designed differently and we're designed as you know in a scale-out fashion and we had more performance with VMs, on a petabyte we don't get to the same performance they get to, but we can get to much higher numbers when we scale the cluster. So this is a topic for conversation, but I think that compared to to Lustre, just talking about performance, it will be, compared to Lustre on cloud, it will be a mistake to talk about performance per petabyte, because our number will just be much lower. Now the disadvantages with the way that Managed Lustre is designed on cloud. is that it gets to a big number, but the performance cluster doesn't scale to a big capacity. Meaning I think Azure right now, the performance theory scales to two petabytes, and it stops, and we can scale to more than two petabytes. So I like the fact that, you know, we can really be modular and share numbers per VM, and then for customers to build it for their scale, and the second reason why is that most of our projects on cloud burst into cloud are not petabytes, are hundreds of terabytes. So why promote the petabyte number if people are not really going for it? Earlier, to be fair, we have projects like. UKMath and Waze and Two Sigma and Jump Trade and those are all large numbers in the cloud, so why are you saying that it's only the burst and small capacities? Jump Trading is being engaged since July, they did not ask to start the PLC since July, and when they talked about the POC, they never talked about more than a test cluster. Citadel, similar story with Citadel. UKMAT is completely, in my mind, completely of reservations. It's a Microsoft project, it's a Microsoft tenant, and Microsoft wanted a $1.2 billion project to build it for UKMAT. the contract is theirs so they will do whatever they need to do to meet the contract. So UKMET is a one-off. UKMET is almost the same as MAI, the new project now that we have. We'll talk about MAI in a second. So to Sigma, I did not see the start the cloud requirements yet. I've seen the interest of them talking and I've heard that they're talking about about 800 petabytes on-prem, let's see what they ask for on cloud. The only real scale-out projects that right now is asking for high capacity on cloud is NBCU, 'cause they're asking for four petabytes, but we had something similar with ICE. At the beginning, ICE asked for three petabytes on cloud, and then they ended up. Going with three petabytes on-prem because the pricing was just too too much. So I'm still searching for that one customer Again, none of the names you that you showed for me is that customer? Excluding NBCU right now that really wants to buy multiple petabytes. The rest of them are just talking about burst To see bigger than that, but that's what I'm saying. Oil and gas might be bigger I did not see the requirements I don't want to talk about stuff that's you know interest on the sales force. Chevron a year ago when I started here in two months talked about 10 petabytes and Exxon Mobil and not sure about the number, but Chevron ended up going with either managed luster or Weka, where the mix was flesh and non flesh. We are just flesh today. I don't see how somebody puts on 25 cents a gigabyte a month 10 petabytes on flesh. Who is that money? Meaning if it's if it's if it's a UK paying that money they're getting the one pricing it's Microsoft right if and wave by the way wave asked for 50 petbyte they said we got 50 petbytes on Azure if Azure can meet the CoreWeb pricing meaning four cents a gigabyte a month complete solution and when Azure told them we can do 40 cents they moved into CoreWeb So clearly, if you can get to the price points of code on Azure, let's talk big capacities, but they can't. So it just moves out. It moves into new clouds.

[00:15:56.30]  Jason VallerySo what I mean, that's what I'm saying. I just want to cover this, like going back to product marketing and how we position this and how we educate the field. and how we go to market with our customers, there should be some common frame of reference that maybe ultimately is an SLO or SLA in how we measure and track performance, TCO, and scalability, and gives us a mechanism to do apples-to-apples comparisons across cloud providers. across instance types, and having some common frame of reference is that performance benchmark. So that's what I'm pushing on, and then the second piece of that is, to your comments around pepabytes versus terabytes or whatever the units are, the more the better, right? The higher the number we push there, the higher the unit, the higher order of magnitude, that'll tell the story. story about scalability implicitly, and so, you know, I think we should push that petabyte vocabulary over the terabyte vocabulary, but that's my two cents.

[00:17:04.65]   RemoteOkay, so I mean, to your point, Jason, like we can sort of, with that that method show up where cluster, where luster falls off a cliff, we actually continue to scale. So our, our unit of measurements, like what we are comparing to might be, you know, 10 petabytes. You know, just to, just to actually show. When Managed Lustre tries to show that comparison, they look very poorly compared to us. Managed Lustre on Azure can get to 10 petabytes. It's just going to be at the SSD tier, the HDD tier, and then their pricing will be 20% of our pricing. What I'm trying to say Jason is that BAST is all about not the petabyte, we're talking about exabyte scale. Yeah, and then our Salesforce talks about exabyte and they go into an opportunity on cloud and they, you know, the shift and the lift and shift from the product goes into the Salesforce. They will go to a customer and say we can scale to an exabyte. a byte, and it's just not true. You don't scale to an extra byte on the cloud. We're not designed for it on the cloud. We're not priced for it on the cloud. So if you want to promote how good we can be on tens of petabytes, I would argue that it will create a confusion with the sales force that will believe that the same playbook they have on-prem now applies to cloud with Yonsi and Jason and the team joining, where in reality the projects that are going to win are going to be hundreds of terabytes. The question is, what is the addressable market of the solution we have and do we care more about addressing that market or putting a buzz out there? If you want to put a buzz, let's

[00:19:05.39]  Jason ValleryTalk about petabytes and exabytes. like if the sales force shows up with an XMI cloud opportunity that gives us a real arrow in our quiver to go to the hyperscalers and say you need to improve your skew shapes you need to work with us to deploy our OEM hardware like we have a meaningful large opportunity that we need to go tackle but without those opportunities without those leads, there's a little bit of a cart before the horse problem, and so in Azure, we've clearly got that. UKMed, MAI, others are asking for exabyte scale capacity in Azure. But I don't know, and maybe we've got these. Is that showing up in Google? Do we have exabyte scale opportunities we can go push the GCP folks with to improve their densities?

[00:19:52.94]   RemoteTry that. That was a year of wasted time in my mind was following the vast DNA into the cloud. So the lift and shift on business which means what Jeff said, he said let's not look for opportunities to win business. Let's go and convince the biggest customers who demand vast on cloud. Yeah. So we went to and NVIDIA clearly is big and then we went to OCI and said NVIDIA is demanding us and NVIDIA told us we're not demanding you and they didn't talk to OCI so OCI deployed what they want to deploy it and then we had XAI and we went to GCP and to OCI and XAI is demanding us but XAI did not demand us so if our task is to create demand, I'm not saying it's not the task, but if our task is to create demand with the top 20 global giants and once we have that demand we will force that onto the CSPs, then you're right on your thinking process. That's exactly the Jeff thinking process, but BAS has been doing that for the past two and a half years. Instead of building something that works, we were trying to take the biggest customers and use them to force, you know, a solution on the CSPs which is first party, vast hardware, exabyte scale. Now, at this point in time, we don't have that, these customers. of UKMX and as of last week MAI. We don't. I 100% agree with that sort of comparison but we have to us here as the leadership team of cloud, we also have to formulate a strategy what is sell-through and that meaning whether it's first party or marketplace and what is actually going to be our sell-to strategy you know there might be an opportunity in Azure for the AI foundry and see what I promised you Jason that I would forget the name that you talked about. I already forgot about it. Apollo. Apollo. Yeah, see? Already forgot about it. What's Apollo? For the sake of the group, what's Apollo? I saw it on the email. What

[00:22:28.01]  Jason Valleryis it? So, I mean, here's the situation Microsoft finds itself in, is that they're partnering with all of these neo-clouds and... just general data center lease providers for space and power, and when you build up one of these 4K, 8K GPU clusters, maybe even a little bigger, but these end up being single customer, single tenant, and generally they're purposed for training. Microsoft has a real problem. Like they can't make that an Azure region. the effort, the overhead, the control plane, it requires dozens of racks of compute services, like it's just a lot, and then that's fundamentally what it requires for that compute in that data center to look like an Azure VM and for storage to be deployed at all inside of that facility to be local, and so, you know, Microsoft is trying to solve this. They can't continue to make all of these Neo clouds Azure regions, and their effort is internally referred to as Project Apollo, and the goal being to kind of replicate a really thin control plane, really simple networking and data stack that doesn't have, like, like all of the internal dependencies that an Azure region brings along with it in an effort to make something look a lot more like a CoreWeave data center deploy. You know, they're early days, and so, you know, the teams inside of Microsoft haven't even coalesced on a strategy on how they're going to tackle this, and so today they're kind of left with a clear plan forward. What I can say is that, you know, they're likely to be running just bare metal, giving those with a lot of hand-holding over to customers, and then just networking, connecting them back with Microsoft as a product called Extended Zones, which allows them to bring the networking kit in, in a much thinner profile, back to an Azure region, and so that's

[00:24:23.00]   RemoteStarting to happen. One second, this is for customers or this is for Azure first party,

[00:24:27.56]  Jason ValleryMeaning for their own sake? Well, which one? What I would say is in most cases, right now, one of the things to be aware of in Azure is that GPU capacity is largely going to 1p use cases where I include OpenAI in that bucket. So I know the sales guys. that cover the third-party GPU market that you know they're going out and trying to sell the model builders and none of that actually is getting allocated to the third-party business they have a triage team that first probably mostly and says are we really going to go do this deal and what's the higher level value to Microsoft as a company in the platform they're not just purely revenue chasing they're they're pursuing strategic logos they're pursuing places where there's great joint go-to-market or shared business value. So there's a very, very, very small number of customers that aren't Microsoft AI, OpenAI, or apparently now Anthropic, and so these big these deals that are going down are largely to provide training capacity in these third-party sites where they don't need all of the additional Azure services, and then they can build and deploy models into Foundry. I think OpenAI is using a little more fungibly for both training and inferencing, but OpenAI has its own stack around all of this stuff. So our opportunity at VAST is to go into all of these facilities and tell two things. The first story is how our density means that in a limited megawatt facility we can deploy an exabyte of capacity in far fewer miles

[00:25:58.47]   RemoteFor a second going to the facilities going where exactly which facility is going to Microsoft and Google and OCI and convince them or go into the customers while using it the first part meaning MAI and an open AI and the others where are you going with the message the message I mean, that's to my point and what Jason is making, that's why we have to formulate a separate sell to Azure because that relationship and that decision is going to be made by Azure. So it's two different addressable markets, one is defined by a list of 10 to 20 global that are going to drive maybe 80% of the revenue and that's where Vaas wants to win. I get that, and the other is defined by the rest of the market which is where most of Vaas customers are today. So if we're talking about a message to convince Microsoft that MAI should lend with us, it can be the same message to why Chevron should work with us on Azure. It's completely different, right?

[00:27:03.40]  Jason VallerySo I agree with you on that. - What I think is that if we stage this out sequentially, we have to go and convince the key target customers, recipients of these GPUs. So in my mind, that is MAI, that is OpenAI, that is UKMET, that is maybe Anthropic. I mean, we don't have a door in there yet, but. and get them to see the end-to-end value proposition. Because the way they're being sold this capacity is, you know, megawatts and GPUs, and if we can go tell a story that, you know, you can get far better density, far better performance with VAST, and a simplified control plane experience, and they demand VAST, and Microsoft hears that. from enough channels, enough of their customers are saying, you know, the storage solutions you're providing aren't adequate, the topology doesn't make sense. Then what we can unlock is the Azure hardware teams to partner better with us. I appreciate the EGOL conversation, but he's a compute guy, and ultimately what I actually think we need to do, and I mentioned her name, Ronnie Borker. runs Azure hardware all up, there needs to be a storage hardware SKU that is vast optimized, and getting that into their set of certified SKUs, then enables them to go and deploy that into any Azure region, and then make us a first party level offering that has the same level of density from and really be able to prove out what NetApp is, like the same sort of model. That's where I think we have to--

[00:28:37.87]   Remote- So I just want to put a line in the sand. Like if our objective right now is to create messaging for open AI and mission AI, I'm not against it. Then in a way, what Roni does today, almost what Ikki does today, doesn't matter. this call because what they're doing right now is they're building a solution for the enterprise customers or the vast customers. These guys is a completely different story. You just had a call with the team, Yonsi, with 16 people, none of them will benefit from this message. So what I'm asking is, are we giving them any collateral? or not giving them any collateral. Now, right now, we have an opportunity with MAI that started on Friday for 160,000 GPUs. That's my update for the call that we didn't get to yet. So this is clearly more interesting for me and for us than anything else we can discuss. We have a session with them on Friday to present our solution approach. out to us to ask for it. She's also going to say to meet with Renan and you Yonsi at Ignite. This is going to service MAI and it's the same project as the Nscale call that you guys had. I was on the call as well but I had to drop for Zoom. Meaning they've asked, they're sourcing the GPUs back to you, Jason. They're sourcing the GPUs per the contract from Nscale. Nscale suggested our storage. Inflection is using us. So they're very favorite of us as well, and it go to the Microsoft engineering led by Chi right now that reached out and said, "We want to learn about your storage." So it might end with them approving for Nscale storage with the GPUs. Again, this is 130 petabytes to 1.3 exabytes. It depends on what they want to do with it exactly. Or it might end with Microsoft deciding to build it themselves with us, so specialized hardware. So that's the golden opportunity, right? So that's on the table, and UKMet is on the table, and we have a team that you are engaged with, Jason. on already that is trying to get the same kind of product from OpenAI. I'm not against it. I think that's the biggest business. That's the business I want to win. But all I'm saying is let's be fair to the sales force and to ourselves. There are two different tasks. One is to go to those giants and the other one is to prepare a solution for normal customers. are asking to POCs. We need to do both.

[00:31:11.07]  Jason Vallery- I agree too, and I've heard. - Absolutely. - Like what we have to solve for is getting the marketplace offer out there. So we've got the control plane, we've got the revenue model, we've got the ability to transact, and that thing has to converge with a hardware optimized story that we're landing by winning the whales independently, and then we can bring those things together. eventually but for now we have to have both tracks executed no i mean it's it's not an either or

[00:31:38.55]   Remoteit's always both we have to cater for both these uh opportunities and like we are not going to get this opportunity by having it uh uh self-managed by So Polaris needs to be involved in this as well as the full life cycle management, ease of use and management. So that's the reason why Renan asked me to join the NSCALE meeting and meeting with Corvive and all the others, because this is a missing portion. For example, we don't want it to happen that Nscale is basically just managing themselves the storage for their facilities and doing exactly what Corvive does is they completely abstract all the value of Vast away from the end customer. The end customer is clueless. is running. They are not exposing our database. They are not exposing our Tahata engine. They are not exposing anything to the end customer, and that's a real problem. So we have to be involved and like We are all go all of us here are going to be measured on the success of cloud whether it's sell to or sell through and I really don't care where the money comes from because Ultimately we have to solve for these larger use cases But to Leo's point we have to be able to solve for the burden stability that our on-prem customers are already asking for and even in some cases blocking Vast from winning that on-premise deal because of our lack of cloud presence. So we always have to solve for both. It's both sides of the same market. The IN and the entry level into cloud. Yes, and what I was trying to say earlier is that it's probably two different set of decks and storylines Yes, I know if you go with it with an exabyte scale to our sales force while addressing 2,500 customers and all they want to do is use us for tens of terabytes or hundreds of terabytes You're going to miss our sales force again If you go with that message to the 20 huge customers then I'm 100% with you and I would prefer to only deal with the 20 huge customers but right now you know we've as a company we've decided to go into cloud and the opportunity we're getting other than UKMET which is unique in a way it's like UKMET is uh also a first party like it's it's their own it's a first tenant right it's Microsoft other than UKMET everything else is just normal customers asking for a solution and we need to be collateral and we need to build a solution for them and performance numbers when it gets to MAI building that cluster with us it's completely different it's our own hardware it's a full data center it's a different storyline we should do both Yeah. OK, so the process-- let's talk about MAI for a second. She reached out. Alon was on the call. It was a connection from Jovane. Are you familiar with Jovane, Jason?

[00:35:11.52]  Jason ValleryNor is she. Do you know where they sit organizationally? Is she in--

[00:35:15.90]   RemoteShe's a CVP, I will share her LinkedIn in a second. I met with her in Germany eight months ago, together with the staff who was on the call and dropped. She was responsible, or is responsible for Kubernetes. So she was trying to convince us back then to use the Kubernetes, like our dockers integrate of Kubernetes on Microsoft Azure. So when she reached out, I didn't know what for, but then she was on the call with a few more guys and they said, "We heard about VaaS. "We want to understand what's so special about VaaS." Because the inflection team told them that they really want to have VaaS on this new deployment. We're going to build 160,000 GPUs data samples, and we want to get your advice and best practices of how do we approach it. So you're the masters, you know it, give us some idea, and we set up it, that was Friday of last week. So I've set a tentative time Friday of this week, and then I reached out back to Alon to say, what do you suggest? And what Alon said is that the same technical team. that build the solution for Enscale are the team to use. Because in a way, it's our own hardware. It's the same exact requirement. The difference is that Enscale, we're trying to promote it because Enscale, I guess, got the same request from Microsoft to show them how a storage solution looks for that scale. So we don't want to have a different story. It needs to be the same story on the other side of the world. defense. So I have a call with the EnScale technical team tomorrow and put you guys all on the call. The ask is to be ready by Friday with a storyline, with a presentation, a solution diagram, and we have a call, a follow-up call with the same team at Microsoft. Wasn't set yet, it will be set hopefully later today, and based on that we will see how they want to move forward. forward. Now the connection here to EGLE, when we had dinner with EGLE last night, EGLE said, "Why did you promise CHI the usage of LSV-5?" And I told EGLE, "I did not promise CHI anything about LSV-5. They came with the requirement and I promised them nothing." And I said, "Based on my understanding, they're looking to build something now, and your LSV-5‚Ä¶" five is ready in a year, and it might also not be a good fit because the scale is too big and the economics will not make any sense. So Egal said, I wanna be in the loop. I said, okay, you can be in the loop, right? I am not going to block you. So that's the only connection to Egal, but this is going to be our hardware, our approach, our design. on the table right now. That's why I was also pushing on Anand to be able to put Algol in his

[00:37:59.60]  Jason ValleryData center. So let me give you some insight of a conversation I had last night and I would like to be involved in all of the EMEA meetings because I've got continuity there. If I'm reading the tea leaves right, one of the things I can tell you about Project Apollo, what we were just discussing, is that the lead on the program is the Kubernetes team. Because ultimately, what they're trying to accomplish is the control plane pieces of what is it minimum surface area to expose to an organization like MAI. to be able to manage that infrastructure and so it ultimately looks a little bit like managed Kubernetes with becoming the control plane itself. So I have a feeling that she is probably thinking a lot about Apollo. Second point, so there's a gentleman in MAI named Kushal Dada. He has been he's the the buyer it just in a sense like he has been the overall PM owner of supercomputer capacity representing MAI internally within Microsoft back to Azure so you know what I can tell you is I worked with him in my previous role quite extensively because he was coming from along along with Zippin, who I think you guys know. inflection core we've experienced and then he was tasked with okay how do we go and move to Azure managed infrastructure. So what's his name again? Kushal Datta, D-A-T-T-A. So I pinged him yesterday and first of all he had sent me a congratulations message on switching roles and then I pinged him yesterday in follow-up and said I heard I've heard some things are going down with AI, what's going on. He's scheduled with me on Friday. So I have a meeting now scheduled with him this Friday. I can bring folks as it makes sense, and his comment to me was, we're really interested in VaST. This isn't VaST on Azure. This is, let me go and let me find the exact word he said. It's like if you give us his bus is one of these bus exactly, but meaning that it wasn't going into an Azure data set. Uh, it was, and I'll have to find the exact wording he said, but if you get my gist, what's happening is that these facilities will not be Azure regions and then that opens things up because, you know, ultimately to go into an Azure region in addition to all the control plane nonsense. I talked about. You've got a whole networking stack problem where you have to have hardware offloaded networking card, the overlay chip, you have to manage all of that and I'm sure Yancey knows this very well from his NetApp days, and so if what we're talking about, and I think this is the opportunity, is to go deploy bare metal into a non-Azure data center that MAI is getting capacity opens up a world of opportunities in terms of how you solve this.

[00:40:56.69]   Remote- So the way I read it, this is an N scale. This is, again, it sounds like if it's one plus one, usually it's two. If N scale goes to requirement of the same number of GPUs to build a storage solution. Microsoft. So what's going on right now is that the Microsoft. Microsoft team is trying to figure out if a they want to block it be they want to own it See they want to get to give a gain like to any schedule just deploy bus. It's a what I'll tell you

[00:41:25.49]  Jason ValleryBecause your enemy you go wouldn't want this to happen because that'll be losing. No, this is clear. Yes

[00:41:33.45]   RemoteEagle and my niece to chase and boy are going to fight this.

[00:41:40.42]  Jason Vallery- Yes.

[00:41:41.24]   Remote- So I put the link of the lady on the chat, and yes, now when Jason is explaining why the Kubernetes team would want to lead it, so she is the person and the team that we're going to talk with on Friday, and we will have internal sessions tomorrow to take what we've prepared for N scale and to modify it to be better. fit for the Microsoft conversation. Because we prepared it for Nscale, assuming that Nscale will package it and sell it as their own storage to Microsoft, but now with the new feedback it sounds like we should put a different line on the story. It's more about the integration to Kubernetes and why this it makes sense for Microsoft to own the solution, even if it's going to be installed at Nscale. So So this is actively going on right now now I would love to be only with that and I'm just taking it back like conversation wise We're getting to supercomputing. We're launching GCP. We're starting PLCs with Azure We are going to have the same position with AWS and 999 out of 100 customers Isn't going to be MAI so collateral messaging I think we just need to have two sets of messaging one for the huge elephants and one for the rest I agree but remember Lior and Jason on the call with Enscale David was very adamant that having the capabilities of using the data engine and the data spaces back to Azure would severely decrease the friction with Azure if we could actually demonstrate that. You know that's something that we have to be able to demonstrate with Microsoft and that's why To both of your points, we need to have two separate strategies, but ultimately Polaris has to be able to service both. Being able to have a fully managed service where Microsoft doesn't have to be dedicating resources into managing the day-to-day operations of VAST. They are never going to accept that.

[00:43:53.76]  Jason ValleryAnd we have to be careful, we have to be careful with how, if we sell this to Nscale and it goes to Nscale's paper and they're reselling it, we can't repeat the CoreWeave deal where they get control over what features are, we need to still maintain control however that contract gets drafted so that we can't expose it.

[00:44:14.28]   RemoteSo I want to show you an example of Wave. 'cause Wave is kind of in between. Wave is a Microsoft customer that is 40 petabytes. I've been at Wave two weeks ago, 40 petabytes on Azure Globe. Now, they wanted to get GPU capacity, which wasn't available on Microsoft Azure, so they started shopping around, and they ended up making a decision to go to CoWave, which 50 petabytes they want, and they're talking about scaling to an exabyte. So Microsoft lost that deal, right? Now, what they negotiated with CoWiV is a dedicated environment, full function of us. So they told CoWiV, we are not going to be locked. We want to have a dedicated environment to enable us to move the data to wherever we want to move the data. So when I met with Wave, I asked them a simple-- question I said like they must move the data today because they don't have GPUs on Azure simple right that's the big reason and the question was would you prefer to have Azure like vast on Azure or is it okay to have the single source of truth still on Azure globe and the simple answer was we would love to get vast on Azure because if the data stays on globe, then the data is just object storage. It's shipped relatively to bus, but it's just object storage. If you move to data, if we can create 50 petabytes, so it can be the same thing on Friday, right? If we can create 50 petabytes of bus on Azure, then we can get the data collaboration, global namespace, so data replication, use the full feature stack of us. It isn't available on Azure Blob. So if we're looking at opening that door wider, so MAI is one, Wave can be the second one, because they would wanna have 50 petabytes tomorrow, right? And the third one is UKMH, that are talking about 300 petabytes. So big project. wise, we have those free for Microsoft right now, and if I look at the other clouds, we have Nvidia and XAI as two other giants that we're talking about pushing us into cloud. But none of them really made a move. XAI spoke with OCI and decided to drop it, spoke to GCP decided to drop it, meaning they did. and care, and NVIDIA, I'm always hearing that we're so tight with NVIDIA and they want us everywhere, but NVIDIA did not make a move with any of the CSPs to ask for VAST. So right now it's really Microsoft is the focus.

[00:46:45.86]  Jason VallerySo I don't know about the GPU allocation situation or TPU allocation situation as it may be at Google and AWS, but you know the Wave example you cite is a perfect call out of what I've seen and what's been happening at Microsoft, Wave's been a long-standing Azure customer. The actually kind of pre-generative AI boom, you know, I could do autonomous driving research for a decade. That 40 megabyte data set I'm actually very familiar with because what's happened over the years is Microsoft has jerked around Wave and they've said, "Oh well, we're going to give you these GPUs in Sweden and then we're going to give you these GPUs in like Phoenix or something like that. I remember and like we had to move that data set multiple times across Azure regions for them because Microsoft didn't like allocate a full GPU cluster to them and was kind of always last priority to what does Wave get, and then so going back to my earlier point, there's a team now at Microsoft that's qualifying who gets the GPUs, and Wave was and remains fairly far down the priority list because it's not strategic value to Microsoft, and so when you think about this 3B business that we're going to go launch with marketplace offers, who's actually going to get Microsoft's capacity will dictate who can go do cloud projects, and we need to be aligned there. there's a gentleman named Kurt Niebuhr at Microsoft. I actually had a one-on-one with him yesterday. His team is making the recommendations. So as these opportunities come forward for GPUs requests in Azure, he's the one that's assigning a value score on should we do this deal or not based on the available GPU capacity that Microsoft has. I'd assume this situation is probably true at Google and probably true at Amazon.

[00:48:25.66]   RemoteAmazon that there's a team that it is this. So I have an emergency on another call where Microsoft is trying to collaborate with us and I told him that I'm not going to join the call because they've set it on this time and now they're saying you should join the call. I would love, Jason and Yonsi, I would love to split the strategy to two. there is a very clear action item right now with Mission AI, we can extend that action item to WAVE because WAVE are really close to us right now, right, and we can push with them and work with them if that makes sense, Jason, and at the same time I would love to understand what you guys see as the strategy for the Salesforce education outside of those 20 giants, and how do we message the right... data to everyone and if we decide that we want a message to everyone that we are exabyte scale on cloud you know what I will message that we are doing that as well I just want to have one message with you guys I don't want to sound like an idiot messaging something that is a smaller game plan with a Salesforce and you come with a different message I want to ask to have the same message after we agree but we need to have a follow-up on this call it so I need I need to jump on the other side, I apologize. Yeah, absolutely. I mean, I agree, let's do that dual strategy, but I want to spend like the last 10 minutes on the engineering side. Is there anything that is giving you guys, Ron and Eiki, heartburns for the GCP loans? any particular thing more than one thing yeah I mean just like so I mean it's no it's true I mean there's there's a there are there are definitely still a few unknowns I mean obviously we're running as fast as we can and and I think that should hopefully make the things faster for us because there is a problem with you know us having to integrate Salesforce as well to get all the information so hopefully that kickoff tomorrow will help. There is there seems conclusion yet on the WIPs on alias IPs versus routable IPs I think that's probably the biggest sort of infrastructure piece that's still not clear to me at least but I haven't I wasn't here for the launch for the MVP Oh, no, we will not have change. We'll not be able to make any changes because Google don't know what to do with it. I mean, maybe they'll give us a better in the future. I hope so. Yeah. Not for for the lunch. That's what we have. We have world based IPs. So unfortunately, I wasn't in the I wasn't in the call today. So, that was the conclusion. Yeah, that's the conclusion, that we need to revisit this. All right, I mean, that's something, it just, yeah. So, basically, what we need to do now, since we are going with the routable IPs, We have to take that into account in the deployment where the customer has to supply us a specific IP range Specific IPs. IPs. Not even an IP range. Oh, it can be a range Okay Okay, yeah So we have so that's that's that's at least that's good good to know that we have a definite answer there the I think the sort of third biggest or or you know top three risks probably will be the timing of the approval if there are any additional approvals needed from the Google site and we'll push on that with Billy once we have... So John Downey is the one that is our sort of bridge to the Marketplace team and he called me last night and he said that... he will make sure that there is no approval because we already have an existing blanket private offer that carries no entitlements so he said that there should according to his discussion there shouldn't be any approval times because it's already I hope so too but we do have to tune that somewhat or tackle it's basically the the component and pricing that part needs to be tuned somewhat so it works um okay what else um what about the main what about the main aspects like unscheduled maintenance or even scheduled maintenance. Is that something that gives you guys any heartburn? So there is, according to Google, there's no such thing as unscheduled maintenance. It's always scheduled. Right. are saying I mean if it's if it's unscheduled I guess it's a failure it's not the maintenance right you can name it anyway you want we have a solution I think that we need more testing I'm not sure that so my biggest worry is the the time that we have for testing, which is not what we wanted. We wanted two months, and we came late, and the integration with you is late, and I'm not sure that we're gonna have enough traction in QA for everything, and support. I know that Shachar said he will take care of QA. getting ramping up support. I maybe it's happening in the background. I haven't heard of it. So it will surprise me if it's happening. I assume that need input for me, and I don't think that support people know enough how to handle this thing. Yeah, I think that's also testing QA like outside of our own testing QA and then the uplink the uplink capability to support testing and or to support support I mean and Yeah, making sure that the break glass support capabilities work correctly. I think we're ahead of that still, but that also needs to be tested. I mean, another thing that also, I'm not sure. if you're worried about it, if you're not, if you've done anything on that, is the validation in the user's environment that everything is set up correctly. - Yeah. - There are so many things that can go wrong. We don't know all of them because we had only one customer actually try. and so we don't know everything, quotas, permissions, firewall rules, we have our checker, I don't know if you're using, you're planning to use the checker that we have? Yeah, we have, it doesn't test everything, and I think that that's going to be the biggest challenge. biggest hurdle in beginning because yeah we said Polaris and we want to have a seamless experience unless we can test everything we will not have a seamless experience yeah and I do want to like one of the things that Google is you know wants us to do is to use the the SAS runtime part that potentially has better ways to do like pre-testing pre-flight dry runs but honestly like right now we we don't really have time to to switch gears and go to that rather than Vastraat so we'll maybe revisit that you know in uh in the next week or in a couple of weeks but it's uh yeah it's tight everything is very tight

[00:57:31.91]  Jason ValleryYeah can i ask you a question about the maintenance failure topic um i don't know how this works in google but in azure a given instance has this thing called an instance metadata service where it can query and identify on a node basis which fault domain and upgrade domain that node is physically placed. So then you can build on top of that and distribute data such that Microsoft makes a commitment that a fault domain means any sort of power outage, networking equipment failure. assuming the nodes are in different full domains, that no outage would impact both nodes. Similarly, upgrade domains, like when they do platform updates, they walk the upgrade domain such that they're not upgrading more than one upgrade domain at a time, and so then you can take that information and use that when you build an HA design to ensure that you know what you're doing. nodes could have correlated failures? First of all, are we looking at that? And second,

[00:58:36.97]   RemoteDoes Google do that? Yeah. So the failure domains we're using in Google, we have eight failure domains and we're spreading our data over them, and we're spreading it in a way that if an entire failure domain falls, we don't lose the data. Azure is problematic because it has all only three failure domains. In order to split our data between only three failure domains, we're gonna have the performance will be impacted and the capacity, the usable capacity will be heavily impacted.

[00:59:07.32]  Jason Vallery- Yeah.

[00:59:08.14]   Remote- So those are two issues that I know Asaf told me that he spoke with Azure guys on that, and they have all kind of ideas that they're going to make changes. It will not be for the POC or the near future. That's something that I'm not sure that we have a good story around having three failure domains.

[00:59:32.28]  Jason ValleryJust to be clear, domains, you're not referring to zones, availability zones. You're actually just talking about instance metrics.

[00:59:38.81]   Remote- Yeah, what you said that, I don't, you had a different name for it, but no, different racks that say, okay, an error will not propagate between racks again, unless it's entire availability zone that's failed, right? Regarding upgrade domains, I didn't know about that. It sounds to me more, similar to the maintenance windows in Google, but Google does not have this concept of upgrade domain. So they're just saying, we're giving you a week's heads up and you do whatever you need to do. So we have a process around that. bringing up new VMs and moving the data from one VM, from a VM that needs to be put into maintenance and moving it to another VM. Failure, if we have upgrade domains, at least what I understand from what you're saying, it can help us by making sure that we don't being upgraded in a way that will cause us to lose data, but it does - so in GCP we're trying not only not to lose data, which is a given, but we also don't want to lose redundancy due to the maintenance. Is there a chance that you ask Google about this? Is there a chance that we'll have simultaneous maintenance events? Because they are in different failure domains. Could we have maintenance events in two failure domains on the same date? - There is a way to configure it. But we didn't go into it because we don't want to rely on that anyway, anyhow. Because an event that you lose the entire failure domain is a very expensive event. I mean, it's going to impact the user with lots of data to recover, and it's going to long time and reduce bandwidth so it's not something that we want to I mean we want to avoid it so we need to just that you know when I'm reading the metadata we can we see certainly that there are two failure domains that have the same maintenance on that. So I'm not sure if it's according to failure domains. We can check. But anyhow, we have a process to avoid this altogether because we're going to we'll start by trying to migrate the data to new VMs, and if that doesn't work because we can't allocate new VMs, then we're going to serialize. the maintenance so and actually we can decide that maybe we're gonna serialize not one by one but serialize it according to failure domain so we can say okay the same domain I don't care about putting down two VMs or three VMs

[01:02:56.53]  Jason ValleryThat that's something to consider do we have the ability to dynamically set the scheme and placement on a per customer deployment or tenant basis? Is that a fixed thing we're doing by cloud or can we tweak that for a given customer? What kind of capabilities do we have

[01:03:14.99]   RemoteAround that? What do you mean EC? Oh the erasure code? Yeah. So we don't configure it directly. directly by customer, but it's calculated automatically according to how many VMs you have and if you have failure domains. So there's a calculation around that.

[01:03:42.08]  Jason Vallery- I can just imagine certain customers may have different risk profiles and might desire us to configure.

[01:03:46.56]   RemoteDifferent no that's not something that that we have it's internal to us it's not something that we do one thing we do have is that so the more than 80% of the data oh sorry 85% of the data is using eraser your erasure coding and that's calculated internally it's not something that can be controlled usually the other part the other five percent which is metadata is not using erasure coding it's using mirroring and then we can decide per installation if we're using if we have two mirrors for each part or three mirrors, through triplication or mirroring, and to enhance the redundancy. In the cloud, we're not using triplication right now because the performance, it impacts performance too much. - Yeah. I gotta jump. one other one topic for next time maybe maybe we should discuss like if if we're going to and you mentioned this earlier like if we're going to package polaris as like vast as a service for the neoclouds and if that's a licensing deal or if that's just something that we are giving away or Yeah, this is basically just coming from Chaka and Rene, but we have to, we have to.

[01:06:18.57]  Jason ValleryHello, hello.
```
