---
type: projects
title: AI caching strategy for MAI
created: '2026-01-03'
last_updated: ''
status: active
auto_created: true
tags:
- type/projects
- needs-review
- status/active
last_contact: '2026-01-01'
---

# AI caching strategy for MAI

## Status

| Field | Value |
|-------|-------|
| **Status** | active |
| **Owner** | Jai Menon |

## Overview

Define and execute a unified, pluggable caching strategy for MAI at extreme scale, prioritizing training cache requirements first and adding inference/KB caching later; evaluate OpenAI cache/IP and alternatives (BlobFuse/Blockfuse, AC Store, Alluxio/DAX).

## Open Tasks

```dataview
TASK
FROM this.file.folder
WHERE !completed
SORT due ASC
```

## Recent Context

- 2026-01-01: Terika Dilworth invited Jason Vallery to an urgent internal training on KV Cache storage, timed to N...
## Key Facts

- The training content is intended to prepare AI sellers to lead customer conversations about KV Cache and the impact of NVIDIA's Context Memory Extension (CME) announcement.

## Topics

- KV Cache storage category and customer positioning

- NVIDIA KVCache Storage announcements at CES

- NVIDIA Context Memory Extension (CME) and inference infrastructure implications

- Internal enablement for AI sellers (live training and recording)