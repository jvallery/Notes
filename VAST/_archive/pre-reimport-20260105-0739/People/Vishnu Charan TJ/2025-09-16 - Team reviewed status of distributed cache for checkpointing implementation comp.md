---
type: "1-1"
created: { { DATE } }
status: "done"
counterpart: "[[Vishnu]]"
role: ""
team: ""
company: ""
series: "1-1/Vishnu"
cadence: "Weekly"
meeting_mode: "Video"
location_or_link: ""
calendar_url: ""
start_time: ""
duration_min: "30"
privacy: "internal"
ai_extracted: true
transcript_path: "00 Inbox/Transcripts/20250916 0936 Teams Meeting (Parallels) Transcription.txt"
tags: [meeting, "1-1"]
---

# 1:1 â€” Vishnu â€” 2025-09-16

> [!info] Purpose
> Build trust, surface and remove blockers, align on priorities, coach & grow.

## Summary

Team reviewed status of distributed cache for checkpointing: implementation complete, scale and resilience testing ongoing. AKS integration will use Linux mounts for now due to CSI/containers team concerns. Private preview is targeted by end of September with Figure AI; MAI is deferred until a data-driven case (throughput reduction, scalability, reliability) is established. Several metrics, documentation, and integration follow-ups were identified.

## Key facts learned

- Distributed cache for checkpointing (write/read) is implemented; node up/down scenarios under test.
- MVP does not support model or dataset loading and has no prefetch; no cross-node model refill on node loss.
- Two checkpoint modes exist: cache-only and cache with lazy writeback to storage.
- AKS integration will proceed via Linux mount instead of CSI PV for now.
- Scale testing planned at 100â€“200 nodes; goal to extrapolate implications for 10kâ€“100k nodes.
- Need to quantify network throughput reduction to Blob with and without cache.
- Measure TPS per node to metadata/blob and extrapolate to large clusters.
- Figure AI is the initial preview candidate (uses VMSS, not AKS).
- MAI currently not adopting BlobFuse; will revisit with data-driven narrative.
- CoreAI requested SSD-only KV cache offload without cloud tier; feasibility unclear.
- Resilience and performance improvements are in progress.
- AWS examples (PyTorch S3 storage writer, prefix layout, S3 Express) are reference points.

## Outcomes

- Confirmed MVP scope limited to checkpointing; model/dataset loading out of scope for preview.
- Agreed to collect throughput and TPS metrics during scale tests.
- Converged on AKS Linux mount approach for initial validation and demo.
- Plan to hand off build and setup steps to Vishnu for validation ahead of preview.

## Decisions

- Proceed toward a private preview by end of September with Figure AI; defer MAI until metrics and scale narrative are ready.
- Use Linux mount for AKS integration initially; CSI route remains under evaluation.
- Focus preview on checkpointing use cases; exclude model/dataset loading and prefetch for MVP.
- Include measurement of network throughput reduction and TPS to metadata/blob as part of scale testing.

## Action items (for Vishnu)

- [x] Run 100â€“200 node scale tests for distributed cache and record performance/resilience results @Akanksha â« âœ… 2025-10-26
- [x] Capture network throughput reduction to Blob with and without cache during scale tests @Akanksha â« âœ… 2025-10-26
- [x] Measure TPS per node to metadata/blob and extrapolate scalability to 10kâ€“100k nodes @Tomer â« âœ… 2025-10-26
- [x] Finalize AKS Linux mount integration steps and prepare demo @Amit â« âœ… 2025-10-26
- [x] Hand off AKS setup and build to Vishnu for validation @Amit ðŸ”¼ âœ… 2025-10-26
- [x] Prepare private preview plan (customers, docs, enablement) and align stakeholders @Vishnu â« âœ… 2025-10-26
- [x] Document supported scenarios and explicit limitations for MVP @Vishnu â« âœ… 2025-10-26
- [x] Investigate feasibility of SSD-only KV cache offload without cloud tier for CoreAI @Amit ðŸ”¼ âœ… 2025-10-26
- [x] Continue resilience and performance tuning for distributed cache @Sourav ðŸ”¼ âœ… 2025-10-26
- [x] Review AWS PyTorch S3 storage writer and evaluate applicable prefix/naming learnings @Amit ðŸ”½ âœ… 2025-10-26
- [x] Evaluate long-term approach: contribute caching into Python/open-source tools vs kernel-mode driver @BlobFuse Engineering Team ðŸ”½ âœ… 2025-10-26
- [x] Reassess AKS CSI driver path with containers team and document concerns/timeline @AKS Containers Team ðŸ”¼ âœ… 2025-10-26
- [x] Assemble data-driven narrative for MAI (scale targets, throughput reduction, reliability) @Amit ðŸ”¼ âœ… 2025-10-26

## Follow-ups

- [x] Schedule and run deep-dive to address Vishnuâ€™s requirements list (use BlobFuse meeting if possible) @Vishnu ðŸ”¼ âœ… 2025-10-26
- [x] Confirm Figure AI preview timing and environment details (VMSS vs AKS) and next steps @Vishnu â« âœ… 2025-10-26
- [x] Share AKS Linux mount setup instructions with stakeholders for trial @Amit ðŸ”¼ âœ… 2025-10-26
- [x] Align on measurement methodology and tooling for throughput and TPS metrics @Akanksha â« âœ… 2025-10-26
- [x] Re-engage MAI after metrics and scalability narrative are ready @Jason Vallery ðŸ”¼ âœ… 2025-10-26

## Risks

- AKS Linux mount may be suboptimal versus a CSI-based approach.
- Unknown scalability at 10kâ€“100k nodes; potential metadata/blob TPS bottlenecks.
- Current limitations (no model/dataset loading, no prefetch) may limit customer adoption.
- MAI disinterest until strong data proof; risk of missed internal adoption.
- Potential schedule slippage due to competing priorities (e.g., hackathon).
- Framework checkpoint sharding/naming may conflict with proposed storage writer patterns.
- CoreAIâ€™s SSD-only KV cache requirement may not be supported by current design.

## Open questions

- Will results from 200-node tests extrapolate reliably to 10kâ€“100k nodes?
- What TPS thresholds to metadata/blob are acceptable at MAI scale?
- Can the design support SSD-only KV cache offload (no cloud tier) for CoreAI?
- What is the path and timeline to support model/dataset loading and prefetch?
- Will AKS Linux mount meet performance and stability needs versus a CSI-based approach?
- Should we invest in a kernel-mode driver or a reusable user-mode library for broad adoption?
- How to align checkpoint naming/sharding with framework expectations if adopting storage-writer-like conventions?
- Which additional customers beyond Figure AI should be included in the first preview cohort?
- What concrete metrics thresholds will convince MAI to adopt this solution?

> Next meeting (if any): (none)

---

<!-- ai:transcript:start -->

## Transcript (auto)

```text
[00:00:00.04] Remote : That IP and all that discussion also happened to see if we can get hold of that also see what is happening on that side. Some of those things came up.

[00:00:14.24] Jabra Speak 710 :  Jason ValleryOn that, we haven't quite got access yet, but we've got all the right stakeholders lined up and figured out how things to Pete, and now actually that's also in JSPORT.

[00:00:22.51] Remote : is to make some approvals happen, but we're almost there. - I see, got it, got it, got it. Yeah, sure. Hey Vishnu, you are here, let's see. - Hey folks. - Hey Vishnu, I'm trying to see if Vikas and others can join quick update since Jason is also here on where we are on the overall status on the received cache. Yeah, I had a bunch of things to line up as well Amit on that because our due date is coming up on the ADO item as well and and I don't know if you're on track. I just wanted to make sure they are ready for preview before the end of September as what we would, that's what we're tracking right now. - Yeah, I discussed about that. Correct. I discussed about that also Vishnu with Vikas and team. So I'll tell you what is pending and-- - Sorry, just a second, Amit. Maybe I'll record this for-- I was hoping Vikas can join because he would have the up to date status. Yeah. I can give you some high level. Yeah. Yeah. Go ahead. Yeah. So, uh, I think for Jason's benefit, so Jason, we, uh, so we are in that state where, uh, a distribute cache for checkpointing, that particular use case we were talking about, writing checkpoints into a distribute cache and reading from them. That use case, we actually completed the implementation of already. We were doing a bunch of testing and a bunch of around. scenarios like node up node down those kind of but in general the that use case we the implementation has been done now there is another thing that we were looking at on distribute cache which is the AKS integration that's where there's a bunch of other discussions happening as to how do we integrate this with AKS I guess so that I think Vishnu is also involved in that discussion and we've been talking about blob cs integration via blob csi and you know linux mount and a bunch of options we're looking at but I think the larger picture also there is that there's also discussion about I think how does this, how does distribute cache works well with the local CSI driver for easy store? But there are also this discussions about how they, both of them gel together, and the other discussion is about also how do these fit into the integration with frameworks like Ray and whatnot, right? So some of those. here side of discussions are happening and I will let Vishnu pitch in if he wants to add anything because he's driving some of them. But that's another thing that we're looking at. But from the engineering perspective, I think the implementation is done. So the next step for us right now are two things for preview. We want to do a preview I think by end of September has been talking to Festive AI, I believe, as one of the potential-- - No, Figure AI. - Figure AI, sorry, Figure AI, and then I think MAI is off the table, if I'm not mistaken. But there may be a few more customers. But so our plan is to do a private preview. Hopefully, by end of this month. and this is what is pending I think from from what I understand. I think Sourav is here also he can maybe update a few more things if needed but so one we want to do scale testing at least on 100 or 200 odd nodes this this has been going on for some time this cycle load we are able to spin up the VMs and the testing was ongoing going i think there was last week we got slowed down for some other reasons but uh uh that is one thing that we intend to finish uh next week this week's also hackathons will be giving some space to folks uh but next week we plan to finish the scale testing on at least 200 nodes uh that is one that is pending the other thing for aks i think the Figure.ai is AKS, they're using AKS, so based on our conversation with the azstore team right now what we're thinking is to, there's an option to mount, to use this Linux mount on AKS rather than doing this PV, going by the PV route, the blobcsi route, so right now azstore team is more comfortable. with us exploring the Linux mode. So that testing also is almost done. So there will be some steps that the customers would have to perform to set it up in AKS via Linux mode. So that's another thing that I think we have to do a demo and hopefully we will get that done also next week, and then our plan is to give it to the hands of Vishnu so that he can try this out and and maybe so if any of you are interested and see if this works and uh and then if all looks good we will go to preview. So that's our plan um so I will take a pause if anybody has questions or sort of you want to add anything please feel free to do that. Yeah, yeah, so yeah, we are also working on like the resilience and performance parts to make it as much resilient and stable and also like Yeah, achieve like performance. So we are also working on that And just to update on the customer So, we spoke to two customers specifically for these scenarios, one figure, we're trying to figure out when we can go to them and we've asked them whether they use the CSI driver because we had a big back and forth on the CSI driver because we don't integrate into the any of the AKBs. tools or any Kubernetes tools rather in terms of operators which was brought up by the AKS team, the containers team as a big concern because we use internal APIs sorry internal IPs to communicate between nodes. They were not sure how that would work and they wanted to have a larger evaluation of how we use. the inter-node communications and how do we balance and how do we get signals and all of that. So because that is going to take slightly longer, they suggested that if you think it's going to work fine, it might work fine just with Linux mounts and that's what we're trying right now. I it's probably not optimal but. That's the best option that we have from an AKS perspective. Then we tried AKS primarily because we said MEI is using AKS for these scenarios and hence we wanna try that. From a figure AI perspective, we understand that they don't use AKS, they use VMSS nodes for their training. So we don't need to do anything else apart from whatever. have already tried with cyclic cloud we can just go ahead and ask them to use it directly on the nodes and once we have the testing ready we can go ahead and you know ask them to try out the one other team that we've been talking to is in coreai they wanted to try a solution a local solution that can use for kvcache offloading now they did not want to connect to the cloud storage tier, so they asked for a solution if we have one that can connect only to the local SSDs and offload to the local SSDs, but not to the cloud storage. We still haven't figured out if the current solution can offer that. We need to understand that, I need to understand that better, because we have the code ready, we'll be able to figure out where it can integrate with, let's say, some orchestrator that can take care of offloading or eviction, basically bringing things back and how does the balancing even work in that scale? All of that needs to be understood clearly. The two scenarios that we can actually go talk to customers is the first one is figured one is of course the MEI, the KVCache one still needs a little bit more clarity there. So that's on the customer line mindset to say how do we go validate this option that we have today.

[00:09:15.81] Jabra Speak 710 :  Jason ValleryWhat happened to Black Forest Labs? I know they were using BlockFuse for checkpointing in the past, are they still using it?

[00:09:22.96] Remote : They are still using it. They're still using it. We haven't gone to them. We haven't thrown the broader net yet to go to more customers because we want to validate, first of all, if these are not-- so there are obviously a few limitations in this solution today, and that's something that I want to validate, and we said we'll start with Figure because they had a very good-- conversation. We had a very good conversation. They also use BlockFace a lot now and they had been wanting to test ahead of time for their future workloads that are expected to have larger footprint in terms of storage. So we said we'll start with Figure AI and MAI, understand if this set of features that we've built actually meets the requirements and then go broader. the reason why I'm I want to make sure we have a validation right now is today the solution works for checkpoint and restore it does not work for model loading or data set loading and no pre-fetch so which means if if they are okay to use it for local checkpoints uh and then bringing it back and also lazily tear some of the checkpoints to lops it will work this is the solution for them but if they also want to make sure they can do model loading which can do distributed replicas across loads or if let's say you lose a node and bring a node back and then you want to get that node filled with the model using the data from another node that's not going to happen today. So that's why I want to make sure this this solution is still good enough as an MVP to even go and ask other customers to try out. We need to validate that, Jason.

[00:11:06.57] Jabra Speak 710 :  Jason ValleryOne of the things that came up in Pete's MAI call earlier this week was a desire from the nation and Jay and I completely agree with this that we need to be able to quantify in these scenarios

[00:11:19.23] Remote : What the network throughput reduction actually is.

[00:11:21.70] Jabra Speak 710 :  Jason VallerySo without the cache and with the cache, how much throughput back to blob is actually moved from those flows. So as we look into the scale testing that we're planning for next week or the week after, we should be trying to see if we can quantify that in the scenarios we're running.

[00:11:38.03] Remote : Through the test paths. - Yeah, I think there were some numbers previously, like really long back, initial tests, but there were like very small set of variables that I saw, but yes, I think sort of, I don't know if Saurav's involved in this, but. Vikas and I think it was Akanksha. Yes, it is Akanksha who's doing those tests and she was capturing some of these, but we will capture that as well, Jason. That's a fair point because we need to be able to say how much of an offload it is on the storage systems when we're using this cache, especially for checkpointing. We can actually do both the comparisons because. we have both the modes where you write to the cache only or you write to the cache and allow lazily to write back to the storage environment. So we have both the options and it should be easy for us to test with both of them and see how long it takes to actually complete a checkpoint.

[00:12:36.95] Jabra Speak 710 :  Jason ValleryYeah and then the other thing I would point out is you know I think if we're going to go back to MAI and try to convince them to use BlockView which, I mean, we can take a run at that at any time. It requires us to have the confidence that the solution we're proposing would meet their requirements and scale to their requirements, and so, you know, doing 100 or 200 node validation, I totally get it. You're not going to go get access to 100,000 nodes in Azure to do a scale test, but we have to have a convicted opinion that the 200 node test is representative of if we were to try and do something like this at full scale with MAI on a 100,000 node cluster or whatever crazy kind of scale targets they have over the next 24 months. So I don't know if you're looking at it from an architecture lens and from a scalability lens on what kinds of friction we might run into as the node count increases, but having some kind of view on that

[00:13:36.43] Remote : That we can review would be helpful. - Yeah, fair enough, Jason. That's the part that Akransha is testing and Tomer is also involved in making sure we are able to scale. But yeah, that is really the testing that has been going on for a while. Yeah, I think the, so Vishnu, I think Jason's point is that we can test on 200, but the question is that will we scale linearly to the number of nodes, let's say 10,000 nodes and all that. It's, I think from what I hear from Tomar, he says that, he's saying that there's not, there's not known architectural bottlenecks, but unfortunately we wouldn't be able to test be very very sure. But I mean I can bring this up again to Tomas to see if we at least when we do the 200 node and at least look at if there are any bottlenecks that may surface that could potentially be a bigger problem when we scale to 10,000 nodes and things like that right. So but I don't think so there are any architectural gap, Jason, that I am aware of that would basically stall us or not make us work.

[00:14:45.98] Jabra Speak 710 :  Jason ValleryOne thing to look at is sort of like the TPS per node that get generated back to the metadata store and blob and they kind of extrapolate that out. Yeah, yeah. 200 nodes, it's driving x TPS to metadata. If you scale that out to 10,000 or 100,000 nodes. You're going to be driving Y-TPS and then you're going to overload blob, kind of figuring out where we're going to sit in that ratio and when, frankly, the index in blob tips

[00:15:09.94] Remote : Over. Shankar? Yeah, I think that's fair. That's fair. We can definitely look at measure TPS and whatnot. Yeah, sure. Shankar, you have a question? I'm sorry, I joined a little late, but I know you might have discussed it. So Jason, you brought it up in an indirect way. So what is MAI, as Pete also mentioned it, I think, yesterday. So are we, they are on a path that is not using, block use and and is that what the plan what the what the what we are hearing

[00:15:52.89] Jabra Speak 710 :  Jason Valleryi mean that's what i've heard pete do you want to take that

[00:15:56.43] Remote : Yeah uh i mean vision has done a great job uh going in front of mai multiple times we've talked to them about it they just live and breathe in python and wanna So for them, it just doesn't fit their use case. I think the CSI driver down the road might have some interest to them once we're a little farther along with it, especially if we can point to specific performance improvements and show them that this has a meaningful impact on their. checkpoint times or their data load times, but right now they just don't have a need for it,

[00:16:37.24] Jabra Speak 710 :  Jason Valleryin their opinion. Okay. I'll say, you know, Jay and I were talking about this yesterday, and while we totally respect that opinion coming out of MAI at the moment, they've got a lot of other priorities, they've got a solution that's working for them, this is a distraction. you know, our opportunity to win with blob views is to convince MAI that they should use it. But, you know, I don't think it's worth our time to go back and continue to push that agenda until we can show up with data on, we know it's going to hit your scale targets, we know it's going to reduce your network throughput, and we know it's going to be reliable and resilient for your use case. If we can come with with a data-driven narrative that tells that story, then I'm sure they'll revisit if Blob uses the right solution for them. So for me, our tasks in front of us is to build that data-driven narrative so that we can take another run at MAI. I don't think we should completely discount that they wouldn't use this in the future

[00:17:34.08] Remote : if we can accomplish that. sense, and the part that came about was about cash, right? I think, you know, they could, you know, they could use something with, you know, the Python and blob file or whatever, without BlobFuse, but they may not, they won't get cash, and if they don't get cash, I think the another compelling argument or a data driven. You know approach that we could take is that hey, when you start getting to that scale, you need cash and we have cash working on bluff, so that is one another way we could look at it, but only other the question I had was. If. You know, is there a path to getting a cash tomorrow without bluff use in the picture That that we can think of or that we want to think of I don't know. See, but basically the point is that, you know, saying that you know Python interfaces and I hear the blob file or whatever that they're using. What if they hit? What if they are continuing in their path, but they get bottlenecked on on scale and you know and and the network and. throughput to storage, and at that point in time it becomes clear to everybody that they need a client-side caching solution for this to scale, and we have the client-side caching solution using, you know, on Blobfuse interface. At that point, would we expect them to take a look at our Blobfuse-based solution to, leapfrog that limitation? Or will there be an expectation to get the cash in, you know, natively on, you know, non-Blobfuse-based access? They're not singling out Blobfuse either. So I put an example in there of how they've built out a new That's called data link. It's a distributed file copy engine that sits on top of not easy copy not object replication But actually sits on top of put blob from URL or put block from URL depending on the file size So it's definitely not anything against blob views in particular. It's just another example of how they're investing directly in either building their own tools in Python on our primitives, or they're using open source tools that are out there like blob file, and I'm not here to tell you what we should do, but one of the things that I can carry back to this team from EMEA. is that, you know, rather than trying to fight against this, one of the alternative strategies we could look at is kind of going what our competitors do today, which is to actually invest directly into these open source tools and actually kind of lean into them rather than trying to go against them. a look to embrace this and invest and take our tech that we've already built in BlobFuse or that we're building now it's distributed caching and look to invest and actually do PRs and contribute directly to these open source tools and sort of end up guiding them and making them the vehicle for our tech to go into these customers who want to live and work in their native environment like Python, and there's precedent for that already with our PyTorch connector for Blob. So this is an area that we could steer into as well. I'll stop there. Yeah, I think from a longer term perspective, it does make sense. The real reason why we wanted to go with Blobviews to begin with was for us to quickly validate if the solution works, and it's the fastest that we can develop a solution in today's Azure environment, right? Because we have all the necessary caching capabilities, and we had to stitch it together to get the distribution parts so it made sense for us to do it go with this now one is Python today I don't know if there's going to be something that's going to be rust base tomorrow it's going to look for a cache and the other thing is I think we are also looking at frameworks that have their own sort of distribution going on whether in. layers and different forms of maturity. So in that context, yes, it does make sense to build a way for our tools to connect with different nodes and also downstream or upstream towards either the cloud storage or towards let's say any application that would like to connect to Lofi's. any uh or whatever the model that we have so from that perspective i think we should definitely go evaluate that uh as an option uh we should definitely consider how to build caching capabilities on some of these um although some of the frameworks provide like limited options the the real question that i am uh yet to find the answer for is all of these teams they build their own tools like OpenAI has the cluster cache and they and MAI is now just trying to figure out how to just define and most of the folks start with making sure they can copy files they almost inevitably always start with easy copy and then move the data to their nodes whenever they need it and then just work with that data right. a generic way that says we can actually connect with every single tool that the customer might be using when I say every single tool most of them by developing a module that can talk to any of these tools is do you think like that if I were to simplify it if I develop something for this Python log file to have a caching capability? Can I reuse that to any other tool that somebody else might develop in this space? Can you think of a way in which we can actually do that? - I don't know if you'd want to, and I'm not saying Python will be here forever. As the standard, but it is the language of the data scientist, it is the language of the frameworks for better or for worse. It's got some performance overhead. Yeah, but I don't know if you're going to be able to change that anytime soon so even if you could, you know, the question is, is why. Now, that being said, building out libraries that could be reused in different tool sets out there. Am I a fan of that? Yeah, definitely. But at the end of the day, neither MAI or OpenAI or CoreAI, CoreAI less so, they don't have really the software development. Expertise to do this on their own. I mean, I've been AI they don't want to be in the storage tool Business development business. They built all of this stuff because they had no choice Yeah, I didn't have anything that met their needs. So they essentially went around us because they they couldn't wait for us so that's Concerning, but it also creates an opportunity in that what they have built, we have access to the IP, and if we don't have access to the IP, that's because, well, it's open source, so you have access to the IP. So I think we try and meet them where they're going, not necessarily trying to push our tools onto them. That's One thing that both have been consistent about is kind of pushing back on that. We just haven't been successful in pushing our tools onto them. They're more interested in taking the primitives. So I like your idea. I just don't know if the build at once, reuse many can technically be done.

[00:26:10.87] Jabra Speak 710 :  Jason Vallery- I think the root of that-

[00:26:12.27] Remote : - It's a beat.

[00:26:13.51] Jabra Speak 710 :  Jason Vallery- Shankar's point of biosystems. interspaces are preferred and I completely agree with in a world where everything is a mount point and it can be accessed from any tool written in any language because it is a standard that's ideal but the problem with that we're aware of is that you're going from user mode application written in whatever language to kernel mode back to fuse into user mode, and these transitions just come with complexity and performance

[00:26:41.21] Remote : Impacts.

[00:26:42.13] Jabra Speak 710 :  Jason ValleryAnd so I think if you're talking about, I'm going to build some sort of library that I can take a dependency on from my Python app, or my Rust app, or my Go app, that all of that code stays in user mode and never has to make that transition, you get those performance benefits. So ultimately, one of the other questions talking about is do we have to go into moving away from fuse and looking at is there something we need to do in kernel mode and actually have some sort of driver that we ship that thing could potentially even leverage underlying hardware optimizations and is that the North Star we should be moving towards or is it some sort of library module that I think Vishnu is describing that we can be reusing across different implementations of client tools.

[00:27:30.10] Remote : Yeah, I mean I'm also curious is that if we are saying Python is a language of choice for data and ideas, how so Pete are you aware how is this this word evolving in AWS today? They also have mount point, is that normally being used for, let's say, AI training and things like that? Are they positioning this differently? I think they're at the same sort of juncture that we're at right now, where I think mount for S3 has done very well for a lot of use cases outside of AIML, and they've tried to enhance it and make it work, and I think it's been mildly successful. But honestly, every day, right now, today, what is today? September 16th. So September 16th, you had about 800 to 1,200 that woke up today to go sell storage for AI and ML, and the storage sales pitch is the same for all of those, let's say, thousand people when they wake up every day. Today, every presentation to every customer for ML storage starts with a combination of Lustre and S3. it's always two in a box, and if the customer bulks at the price of Lustre, the fallback is mount point for S3. So I agree with you that file interface, so the way AWS just has a different sales model than we do, they start with Lustre. That is the de facto default for all HPC and ML storage for HPE. high-performance training, checkpointing, all starts with Lustre. It's never Lustre by itself though, it's always Lustre with S3. Lustre is the front end, S3 is the durable, economic storage back end, and then they have a very elegant sync process between the two to make it look seamless to the customer, and that's how it's sold. the time today, and if the customer blocks the price, only then is an alternative brought in, something like Mount Point or just S3 straight. Does that help answer your question? Yeah, but I'm still wondering, because you made that comment that a lot of these researchers use Python and I was trying to understand. How would that play out if they have to use Lustre or MountPind? Well, so the first PyTorch connector was actually built by S3, so that launched over a year ago. So for those customers who do want a Pythonic interface with S3, that's what the connector is there to do, and they've launched that. That was one of the things we... launched it at the end of 2023. So I think this is definitely an interesting discussion on how do we want to take this forward. Because if we would want to tap into this market and we think that we probably would have to either. with this open source projects like blob file or whatnot and extend them and add the capabilities that we have built or and or we have to essentially go build a library or something that customers can just use. I think those are things we could definitely look forward to and see how we would want to do it. our story around distributed cash for Blobfuse itself. I'm assuming that we're still aligned on taking that forward in the same path, right?

[00:31:18.53] Jabra Speak 710 :  Jason Vallery- Yeah, I mean, I certainly want to see--

[00:31:22.65] Remote : - We may not force on this to MAI, but there are other customers we could still try this, right?

[00:31:30.75] Jabra Speak 710 :  Jason ValleryYeah, I'm still convicted that we need to take this forward and test it and figure out if it's useful for customers. I mean, you guys have done a lot of work here, and I want to see that over the milestone.

[00:31:42.12] Remote : I'm anxious to get my hands on it, and let's see if customers find value in what you have. - Amit, I don't need to draw, but I have a long list of things that I want to validate in terms of our current solution versus the requirements. So if it's okay, I might set up another time. I don't know if you said, you mentioned that folks are busy with the hackathon this week, but-- - We can have a discussion, Vishnu. discussion is not a problem. Yeah I have like a long list of things that I need to get an answer for because if you're going to we can't start the documentation after the scale testing is done so I think I need to understand what are the scenarios that we can actually work with and what are the things that we can actually talk to customers about and of course all the limits and how should we look at enabling those limits. limits? Yes also, we can talk about that Vishnu. I think this whole other thing about we were talking about model loading and pre-loading and stuff like that, I think those use cases also Vishnu we are looking at it but we are already working on them. It's just that we I think for MVP we still think that we should go. ahead with checkpointing because that first of that was at least one of the yeah no that's fine uh that that i agree i but when we are asking for a preview we have to be specific about what we offer and what we don't offer that's that's the real differentiation that needs to go through yeah we can we can discuss that whatever questions you have yeah we can we can attempt other maybe we can use we can reuse uh you know tomorrow's uh meeting for that, the BlobFuse meeting. - Sure, I mean, hopefully Vikas can join. I will try to join. I usually have conflict with it. But okay, we'll see. Yeah, sure. - Okay. - Sounds good. - All right. - Okay, Jason, anything else for us? - Yeah, you should definitely look at the new S3. storage writer that I copied into the chat there for the PyTorch connector. One of the things that it does specifically for checkpoints is it actually defines out what the prefix structure is going to look like, and this is, I think, pretty clever in that it takes some of the mental model some of from the customer to have to figure out how to lay out their structures. It actually ends up sort of creating the naming conventions correctly for you to have high entropy early in the prefix path, in the keyname path, so that you end up basically having a lower probability of 503 errors due to how long it takes them to split the partition. partitions and load balance on the S3 side. I would just take a look at that. I think it's an interesting new little twist, specifically that storage writer feature. Yeah, I quickly looked at that bit, but it looks like it might conflict some of the frameworks in terms of how they do the checkpoint sharding. that's been considered here. We'll definitely take a look at that. Thank you. Yeah they actually go and integrate directly with some of the frameworks so then the frameworks have a mode like lightning for example PyTorch lightning have a mode that they know that they're now working with the S3 storage writer so then they turn that responsibility then over to the S3 storage writer. That's interesting, okay, that makes sense. - Cool, all right, thanks. - Yeah, we'll take a look, Pete. Interesting that they also had this mention of distributed cache there too. So yeah, we'll take a look. - Oh, the distributed cache is just S3 Express one zone. It's, that's their distributed cache. They just, actually it's more of a centralized cache than distributed. them. They do not have what you're building. >> DIRECTOR DEWOLF: I understand. This is on the summer itself. OK. Got it. >> DIRECTOR DEWOLF: Sounds good. Yeah Pete we'll take a look. >> DIRECTOR PINKHAM: OK. >> DIRECTOR DEWOLF: Thank you folks.
```

<!-- ai:transcript:end -->
