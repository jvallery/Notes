---
type: "group-meeting"
created: { { DATE } }
title: "20251030 0748 Parallels Transcription"
participants: [{ { PARTICIPANTS } }]
ai_extracted: true
transcript_path: "00 Inbox/Transcripts/20251030 0748 Parallels Transcription.txt"
tags: [meeting, group]
---

# 20251030 0748 Parallels Transcription

**Date:** 2025-10-30  
**Participants:** Jason Vallery, Paul, Ray

## Summary

Team aligned on presenting two exabyte-scale storage options for MAI: (1) an NVIDIA NCPâ€“aligned high-performance pod sized for ~41k GPUs as the baseline building block, and (2) a high-density Maverick 5400 capacity-optimized variant. Materials (rack layouts, metrics) will be produced by EOD for a call with MAI tomorrow. Narrative emphasizes AKS-led standalone control plane, local storage for checkpointing despite WAN outages, Kubernetes (CSI/COSI), file+object, and single-pane monitoring/logging.

## Key facts learned

- Target environment: 160k GPUs likely split across ~3 sites; propose sizing per ~40k GPU training clusters.
- Baseline option: NVIDIA NCP pod supporting 41,472 GPUs with ~450 C-nodes and 180 D-nodes (~215 PB usable per pod).
- Baseline perf (per pod): ~11 TB/s read and ~4â€“5 TB/s write.
- Capacity option: Maverick 5400 at 5.4 PB per 2U; exabyte-class build is ~210 Mavericks usable after EC.
- Capacity option sizing: typical C:D near 2:1; e.g., ~420 C-nodes for performance, or ~105 C-nodes minimal for capacity (lower perf).
- Maverick rack density: approx. 30 racks for ~1 EB; 1350-series needs 4â€“5x more racks.
- Azure Blob reference: ~2.5 Tbps (read/write) per 100 PB; ~25 Tbps per 1 EB aggregate.
- AKS driving thin, standalone control plane for leased/neo-cloud sites (reduced dependency on Azure region).
- Extended Zones introduce WAN dependency; local storage needed for checkpointing and pre-staging to avoid GPU idling.
- Data movement today: OpenAI uses PutBlobFromURL pipelines; MAI uses azcopy with scripting; Sync Engine is optional/needs fit.
- Interfaces: CSI/COSI; file and object likely required (MAI prefers file, moving to object).
- Single-pane monitoring/logging in VaST is valuable; Azure Log Analytics/Kusto typically unavailable in these sites.

## Outcomes

- Agreed to present an NVIDIA NCPâ€“aligned 41k-GPU pod as the baseline design and extrapolate to 1 EB.
- Agreed to present a second, high-density Maverick 5400 capacity-optimized 1 EB option with perf trade-offs.
- Will keep proposal messaging consistent with NScale and avoid pricing; focus on architecture and benefits.
- Materials to include rack diagrams and a side-by-side metrics table (racks, read/write perf, capacity, power).
- Deck will highlight AKS standalone control plane fit, Kubernetes integration (CSI/COSI), file+object, monitoring/logging, and offline resiliency.
- Jason will create a one-slide with Azure Blob comparison for a 1:1 with MAI and socialize broader comparisons later.

## Decisions

- Use NVIDIA NCP reference architecture as the lead option for MAI (~41,472 GPU pod, ~215 PB usable).
- Include a capacity-optimized Maverick 5400 exabyte option as an alternative focused on density and power.
- Reference 1 EB as the comparison baseline across options.
- Defer direct Azure Blob comparison in tomorrowâ€™s call; prepare it for follow-on stakeholder discussions.
- Do not emphasize Azure Lifter/RPaaS APIs; prioritize thin, standalone control-plane compatibility.

## Action items

- [x] Draft NCP-aligned design slides and rack diagrams for a 41,472-GPU pod (â‰ˆ450 C / 180 D, â‰ˆ215 PB usable) and extrapolate to 1 EB. @Paul ðŸ”º ðŸ“… 2025-10-30 âœ… 2025-11-08
- [x] Compile performance, capacity, and power metrics for the NCP pod; produce side-by-side metrics table. @Ray ðŸ”º ðŸ“… 2025-10-30 âœ… 2025-11-08
- [x] Design capacity-optimized Maverick 5400 exabyte option and rack layout; include C:D ratio variants (e.g., 420:210 and 105:210). @Paul ðŸ”º ðŸ“… 2025-10-30 âœ… 2025-11-08
- [x] Model read/write performance and power for the Maverick option at chosen C:D ratios; validate backend media limits. @Ray â« ðŸ“… 2025-10-30 âœ… 2025-11-08
- [x] Add bullets on Kubernetes (CSI/COSI), file+object support, single-pane monitoring/logging, and offline auth/DNS resiliency to the deck. @Paul â« ðŸ“… 2025-10-30 âœ… 2025-11-08
- [x] Create 1-slide/Excel view adding Azure Blob comparison columns for the 1 EB reference. @Jason Vallery â« ðŸ“… 2025-10-31 âœ… 2025-11-08
- [x] Join MAI call to present the architecture and answer technical questions. @Paul â« ðŸ“… 2025-10-31 âœ… 2025-11-08
- [x] Join MAI call to present performance/power modeling and NCP alignment. @Ray â« ðŸ“… 2025-10-31 âœ… 2025-11-08
- [x] Confirm MAI checkpointing method and sustained write throughput targets during 1:1. @Jason Vallery ðŸ”¼ ðŸ“… 2025-10-31 âœ… 2025-11-08

## Follow-ups

- [x] Validate distribution of 160k GPUs across sites/pods and max non-blocking cluster size with MAI. @Jason Vallery ðŸ”¼ ðŸ“… 2025-10-31 âœ… 2025-11-08
- [x] Clarify MAI data movement standard (azcopy vs PutBlobFromURL pipelines vs Sync Engine) and implications for design. @Jason Vallery ðŸ”¼ ðŸ“… 2025-11-05 âœ… 2025-11-08
- [x] Confirm NVIDIA acceptance/certification posture for Maverick-based capacity-optimized design. @Ray ðŸ”¼ ðŸ“… 2025-11-05 âœ… 2025-11-08

## Risks

- Architecture decision within Microsoft (Extended Zones vs standalone control plane) is unresolved and political.
- Maverick 5400 not part of NVIDIA NCP certification may face acceptance challenges.
- Unclear MAI checkpoint cadence and sustained write requirements could mis-size performance.
- Data movement approach (azcopy vs custom vs Sync Engine) is unsettled.
- Potential messaging inconsistency between NScale and direct-to-Microsoft paths.
- Backend media throughput limits may cap gains from additional C-nodes if C:D ratios are imbalanced.
- WAN/DNS/AAD outages require robust offline auth/DNS; requirements need validation.

## Open questions

- What sustained write throughput per pod is required to meet MAI checkpoint cadence (e.g., every 60s vs 5â€“15 minutes)?
- Will MAI accept a non-NCP, Maverick-based capacity-optimized option for exabyte deployments?
- How exactly are the 160k GPUs distributed across sites and training clusters?
- Which data movement approach will MAI standardize on (azcopy, PutBlobFromURL-based pipeline, custom, or VaST Sync Engine)?
- What identity/DNS offline requirements must be met for JWT claims caching and RBAC during WAN loss?
- Do they require both file and object interfaces at launch, or object-only?
- What are the rack power and tile placement constraints per site that affect density choices?
- Are dedupe/compression expectations applicable, or size strictly on no-reduction performance?

---

## Transcript (auto)

```text
[00:00:00.00]   Remote>> Hey, folks. >> Excuse me. All the amount work. There's Jason joining.

[00:00:05.66]  Jason Vallery>> Hey, sorry about that.

[00:00:06.38]   Remote>> So that's okay. So let's do a quick intro. So Jason, you go first.

[00:00:13.09]  Jason Vallery>> Yeah. Hey, guys. Jason Valleri. I joined BAST a little over a week ago. I am running product management for the cloud and reporting to Jeff. I most recently was at Microsoft, so I've got a lot of context here. I actually was product management leadership and in the object storage team for the last 13 years and ran a lot of things, including hardware. So, you know, this is a timely conversation to look at how we can get a proposal in front of, not all of Microsoft really, but initially just the InScale team on what it would look like to use OEM hardware and how that stacks up against Azure storage offerings and yeah, so that's what we're trying to get out of this call or what I'm trying to get out of this call.

[00:00:56.66]   RemoteYeah, but just one correction, not the InScale team, the Microsoft MAI team. So everything he said is correct now. You guys are working with InScale and that's why Alon suggested to work with them. you and you are suggesting kind of the same approach as to Nscale, so Nscale will position it with Microsoft. So the same team at Microsoft that is communicating with Nscale has reached out directly to us, and now we need to put a solution in front of them, and again, logically, all I'm saying is that they have three options. They probably have more than three, but three logical is that they decide that whatever they did to date on storage they will continue to do moving forward, which is remote usage of Azure Blob. Right? JSON, that's what they're doing today, and the other option is that they will decide to give Enscale the business, not just the GPUs, but also the storage, and then we win. You guys are going to sell to Enscale. Enscale is committed to Rust, and the third option is that they decide... that they're selecting us but they're building it, which means we win Microsoft, not Enscale. Maybe Enscale will not be pleased, right, but overall it's a bigger opportunity for us as a company. So the call tomorrow is not a call to put a quote in pricing, it's to put down our suggested approach to how does a vast solution looks like. for 160,000 GPUs and the key lines of what's the benefit, you know, what's what are the design concepts. So when I spoke with Elon, he talked about, you know, when we talk about stuff like that, performance per GPU or capacity per GPU depends on the type of work that they're doing, video or non-video and so on and so forth, and he just said, work with the N-Skill. team and modify what they did to have something ready to show Microsoft and it's also important for us to be consistent because they're going to get kind of the same offer on both sides of the fence and we just want them to choose Vaast or Vaast and we don't want to confuse them that Vaast is a different story, and what else? And I think Jason, you also really wanted you on the the fact that you can add because you said something about kubernetes and why is she leading

[00:03:04.37]  Jason ValleryThis program maybe yeah well let me give like a couple of pieces of context um one of the big challenges that microsoft has is that um they're putting all these gpus not just in in scale but in many of these neo clouds they're leasing data center capacity and the way they're selling and you know, the customer set that's primarily getting them is either OpenAI, Microsoft AI, and there's a small amount of Azure Foundry, but I don't even know that they're really getting anything meaningful, and rumors may be Anthropic is getting some, but that's even news to me. But regardless, the scenarios in terms of workloads that can go into these clouds, are these Neo clouds and leased facilities is kind of limited because these aren't actually Azure data centers and they don't have the breadth of services, and so, when you think about this, what they're getting is 4k, 8k, 16k GPU clusters, and the way that those then get managed and delivered is today they're using something called Azure Extended Zones, and what Azure Extended Zones represents is a networking product that connects whatever facility we're talking about back to the nearest mainline Azure region, where all of the Azure services exist, and so that would include all of the, you know, authentication services, the DNS services. the networking plane, the control plane, and everything that you really need to manage, you know, full Azure deployment, and because all of that isn't local to the GPUs, that network link becomes a failure point, and the, if that network link were to drop, you then would have a bunch of idle GPUs, and one of the things Microsoft is looking at right now is, how do you make these sites usable without taking a dependency on the adjacent Azure region or turning one of these sites into a full-blown Azure region itself? Because both of those paths are fairly heavyweight. Like, to turn it into an actual-- region itself and bring all of those services into that facility. It's, you know, 60, 80 racks of compute and a whole bunch of engineering work to turn it online. So our opportunity and what I understand to be happening is that the AKS team has taken the lead on building, you know, we can call it new Azure or whatever you want, but it is a very, very thin control plane that becomes standalone that would run inside of this facility and then expose to the end customer a just Kubernetes like interface for managing that infrastructure that has no real dependencies on the adjacent Azure region. So that's why the AKS team is really interesting because you know I think they're the ones that are going to look at this and see why VaST is And so, why is that important? Well, today Azure Storage takes a ton of dependencies on all of those services that you don't necessarily need to just run a training job or inferencing, and without it being a full, either extended zone or Azure region, you can't even deploy Azure Storage racks, and so, one of the things that Microsoft is doing been looking at is what is our storage solution for these locations and you know I can just tell you they're thinking about how do they go engineering this themselves but I came from that team that's a multi-year project they need a short-term solution and that's something where we can come in and help them and you know it is really important for them to have some local storage in each one of these sites because that enables them to do checkpointing and pre-staging. staging the training data despite network connectivity drops or if the WAN isn't big enough back to wherever the nearby blob region is. So that's kind of the general end-to-end story. They want to be able to use these things fungibly for training, for inferencing, and we can step in and help them provide a very dense deployment, so an interesting stat. I want to index on a 1 exabyte deployment. You know, 1 exabyte is somewhat arbitrary here, but it's a nice big round number, and I know that's the kind of scale that my eye is interested in. You know, to deploy 1 exabyte of blob storage on hard drive, that is about, give or take, 220 racks of equipment, and it's about five times as much. as I believe vast to be from one of the other proposals I saw, and so when you think about that 5x reduction in power, that's probably the most important thing, although tile placement is also constrained in some of these facilities. That's a lot of GPUs that they can cram into that same location compared to block. So there's a clear narrative here that we're much, much more efficient. efficient in terms of space utilization, power utilization. There's a clear narrative that the performance is way better. Blob gives you about two and a half terabits per second, terabits is the measure they use, of both read and write, it's the same for read and write throughput, for every 100 petabytes of capacity you deploy, and so, you know, a one exabyte deployment, It's hard to get all that in a single namespace, but they could pull it off, they could pull it off, and the rights keeps.

[00:08:22.72]   RemoteJason, pause for a second. You have too much information to share. Let go. Thank you. Pause for a second. I have a question to you though. If they do all that, do you still envision Microsoft having the data in Azure Blob and then trying to move it from Azure Blob into those? new data centers or it's going to be the data center will have data that will

[00:08:42.81]  Jason ValleryNot need to live on Azure blob at the same time. Certainly OpenAI and MAI are going to look at the GPU cluster adjacent storage as kind of a glorified cache to survive network availability. issues and to provide high-performance local capacity, but the systems of record will continue to be in Azure in block, primarily for a few reasons, like that's where all the CPU cores are, so when they're doing their data processing pipelines, data ingestion, Spark jobs, Databricks jobs, that's all going to be in Azure, they're not even deploying CPUs anymore. into these Neo clouds, and because they're likely to have multiple GPU clusters around the globe, they have a central data repository that they sort of pull data out of to be adjacent to the GPUs. The GPUs generate the checkpoints and then they bring those back into their central data repository. So that's kind of the general workflow that OpenAI have and in AI increasingly so. they just don't have the maturity to do it.

[00:09:48.29]   Remote- Okay, and one last question and then moving to the team. How do you see them moving data? Is that a storyline that we want to talk about, like Sync Engine and stuff like that, or under the assumption that they will have their own ways to move the data from Azure Blob into the one exabyte cache that they will have with us?

[00:10:07.67]  Jason ValleryI mean, we can talk about Sync Engine, I think. I mean, I think the answer here is it's going to be customer-dependent. I know very well how OpenAI do it. They leverage Microsoft has a API on-blog storage called PutBlogFromURL, which allows service-to-service data movement, and OpenAI have built their own effectively Sync Engine. over top of block storage using that API. So OpenAI have their own first-party solution for this. MAI is less mature and they use Microsoft's got a command-line tool called azcopy that manages some of this for you and they pull that today out of the vast cluster. So MAI have a vast cluster in Portland CoreWeave that they're using azcopy against to move that data. it from there and the block storage via some scripting they built out. Sync engine is interesting here, I just we have to think about how that works with

[00:11:01.55]   RemoteTheir scenarios and workloads. Okay, so let me try and summarize Paul and Ray kind of what we need help with. They said 160k GPUs, I think Jason is right, like one exabyte of storage as the reference point so we can show them how an exabyte looks and then also compare the exabyte to Azure Blob. Not for the call tomorrow, the comparison, but the comparison is important for other calls and other meetings that Jason is arranging with people around it. For tomorrow, we don't need to tell them that their baby is ugly. It's more about explaining our baby, and with everything that Ray shared yesterday... and all the work that you guys did with N scale, question is, how do we, how do we design as vast? Like I don't have the experience. How do we design a solution at the next about scale for this number of GPUs and promote it? - Well, it's also a question of a, are we going to follow the guidance that NVIDIA, for example, provide? architectures that they want us to adhere to, to feed their GPUs. I think that if that's what we're doing with Neo clouds, and that's the way we operate, then the logical answer to me is yes, we don't want to again, I really think and again, this is thinking out loud. This is an open conversation. Nobody has the truth, right? Is that we want to be Consistent on both sides of the fence, but if they ask and it for a configuration should be the same configuration give or take It's just of doing business and I don't care if they buy it from n scale or buy it from us You know me is lost. We just want to win it. So if we're doing something completely different It's it's out to explain um, NCP for, for the archive storage, for the low, you know, uh, there's not a separate, um, cause our RAs are, you know, the NCPs we've been following so far with, with N scale up in the HPS high performance storage. Right, and this is, as I understand it, this isn't that right. right? This is high performance storage, just in your question, Paul, are you asking if we need to position an high performance tier and next to it vast that is an archive tier? Is that what you're trying to ask? Well, that's that's what I think. Because next about, you know, we haven't been dealing with anything nearly on this scale within scale so far. storage that we're putting in you know the high-performance storage tier for a

[00:13:35.53]  Jason ValleryVery very large 200,000 GPU cluster. Well let me ask a couple of leading questions around this because I think that there's probably two then comparison points we want what I would love to see is the highest density option you can provide so you know how do we get to the fewest number of racks, the fewest number of kilowatt megawatts, right? And that's option A. I think that's an important one. But then I think from the point of view that,

[00:14:06.19]   RemoteYou know, we have a very high density, we're just about to bring out the 5400 Maverick, which is 5.4 petabytes per 2U. That's the densest product. we have but that product is not all part of the nvidia reference architecture ncp yeah yeah we're gonna have to go with 1350 series which would you know mean we would have a lot more boxes in order to get to the xy scale so is that because we didn't get to certify with nvidia because it is well we nvidia of chuck you know they've really the for the performance we need to adhere to Maverick is much more of a an archive solution for for for vast so we've only certified the series platform which is for high performance most of the ones we're selling you know we'll be using the series 338 so the smaller box with the 15 terabyte drives but we have a series 1350 which uses 60 terabyte drives which is you know what a petabyte per one you basically now we are planning to bring out of series 2700 fairly ish soon right yeah we hope but but the but the drive sizes for series because they use the ruler drives they come out a bit later than standard to an office what's what's the NVIDIA reference let's just talk about that what what if we're working under the NVIDIA guidelines what are the size of the drives so so the site that well both the 1350 and the three three a series you can you use which one you want, depending how many GPUs you're sizing for, you need to have a certain amount of performance to adhere to NCP. The capacity is kind of independent of that, so you know, the capacity, the performance is independent of the capacity with the two different platforms. So you switch drive sizes based on the capacity the customer would need. So if we're talking. We're probably going to be using the 1350s rather than the 338. So yeah, a 338 series cluster at an exabyte is a lot of nodes. So what's the 1380? You explain what it means for a second, double click on that. So the series 338 is the raw capacity of the chassis. So 338 terabytes. 1,350 is 1,350 terabytes, so it's 1,3,5 exabytes per 1u, so that's that's what that that means. All about all about b-boxes, the last you know the number at the end of the description is the terabytes of raw data. So the the Maverick 5400. is a 5.5, 5.4 bitabyte raw you know so you have to take a bit of bit of capacity off that for erasure coding and that's what you'd get usable so so yeah the 1350 series is the is the high capacity series the 338 is the low capacity you know that you would use to design a you know a higher performance solution. because you would need more boxes per terabyte to get. - Oh, does NVIDIA reference work on how many, how many C nodes do you need per GPU or? - That's defined tightly in the reference architecture. So we have this concept of scalable units, they call it. So, you know, you have like a 1K, 2K, 4K, 16K. 32k, uh, you know, GPUs, and then you size based on that. So it's, it kind of multiplies up, but. You know, they don't have a standard NCP reference picture for 160,000. What's the size of the, of Microsoft's really deployment when they deploy GPUs? What's the size of one cluster of GPUs?

[00:17:58.07]  Jason ValleryWell, they suit the, so the GPUs are. deployed in those sort of same 4k tranches, but then they're cross-connected in a non-blocking network to get this like 160k scale. But what I can tell you is almost, I'll find this out tomorrow when I talk to Gushal, the plan of record when I worked with MAI before I left was those 160,000 GPUs were not in a single non-blocking cluster. They're actually... spread across three different sites and they're not multiple namespaces for that amount of GPU. Yeah so like it's you know likely they're only going to have like you know like 40,000 in a single training cluster or something in that magnitude. I mean that's still a pretty impressive cluster but this isn't 160,000

[00:18:45.34]   RemoteOne side one cluster. So the biggest reference for NVIDIA is 32,000 GPUs as a single? 41,472 GPUs. Where is that number coming from? Coming from NVIDIA or coming from something related? NVIDIA. It comes from NVIDIA. So they said 160,000 I would say it's a big number. that it would be four times that let's go with the 40 000 and and build the best solution we know how to build for 40 000 and explain it and they can do x4 to to make it x4 times bigger so is that the 180 by 72 essentially right for that no so that is the 450 by 180 okay this is a bigger one that i haven't modeled yet yeah yeah um so most of the reference architectures from nvidia used to stop at about 16 and they've gone beyond that now so the biggest one that we have is a 450 by 180 which would give you approximately 250 14 petabyte per cluster, and 450 by what, sorry? 180. 180, okay. The 180 is based on the 1350, based on the capacity? Yeah. Okay, and 450 C-nodes, and this will support 41, what was the exact number, 41 what? 4 1 4 7 2 gpus yeah okay and that's about 215 petabytes per pod right correct yeah okay so i would need like what to get to to get to Multiply up on the amount of GPUs that supports to get to the x. Yeah, it's just to where Jason wanted us to get to which I think is a good place to get us. How do we design an exabyte? An exabyte is basically four times. Well, it would be just over four, wouldn't it, 215? Okay, you know an exabyte, you know, if we are shy by 10% it's still an exabyte. Like we're following, it's a statement, we're following the NVIDIA best practice design, and we're getting to an exabyte on the other end of it. Now, Jason shared numbers a second, you know, like 25 terabit for an exabyte. What are the performance numbers for a cluster like this? >> For this, it's going to be â€‘â€‘ >> 11. terabytes a second close to yeah four terabytes a second five terabytes a

[00:21:29.29]  Jason VallerySecond of right but that's for the 250 by five but you wouldn't get that

[00:21:38.53]   RemotePerformance from you know that you know a group of GPUs attaching would only be into a pod not the whole thing right how does about parameters per second how many use is that for 51 by 180 that's five six hundred and fifty rack units three six at 630 not taking into account any switching though I've got 757 under dog yeah 758 you across 30 racks switches or devices I think that's got some switches in it as well yeah I'm doing this in the sizes so it's assuming switches okay So how do we? JSON again, this to me sounds like the right approach. It's keeping it simple. They ask for guidelines. We're following the NVIDIA best practices. We're putting down a design for a sizable cluster. NVIDIA doesn't have a design for a bigger cluster, right? And a bigger cluster doesn't make sense on the same scale of a global, of a single namespace. We can prep all the way it looks, the way it operates, the performance it gives, and then we can extrapolate the numbers to show how does an exabyte look, and that can be a stepping stone for the other conversations you want to have tomorrow with other people, comparing it to Azure Blob, correct?

[00:23:07.92]  Jason ValleryYeah, I like it. I still would like to also have, as an additional option, this archive SKU you talked about. You know, when you get to this scale, the kinds of performance you're describing is actually far more than is needed, and so, I appreciate the reference architecture and certainly for N scales purposes, I think that's the way you lead. But I'd also like to be able to have another column in the table of, and if the preference is density, where you're really just trying to look at an exobotic capacity, but you don't performances that would yield using the NVIDIA, how dense can we go and what is the corresponding performance characteristics of that option?

[00:23:47.64]   RemoteShould we optimize that for capacity not performance?

[00:23:52.06]  Jason ValleryRight. Yeah, okay, but that is the whole problem of that is how

[00:23:59.31]   RemoteYou know NVIDIA are going to... that because they define very tightly the gigabits per second per GPU which we then multiply up to the but there is a but let's just say we use the same number of c nodes and we're just going to use d nodes which are much higher capacity supporting 40 000 gpus just you have much more capacity no no so it's not just a factor of c nodes, right? So C nodes add performance up to the limit of what the I/O, the back-end disk can provide. So you get to a tipping point where actually adding more C nodes you've tapped out on the performance of the back-end disk. Okay. So we talk about that and you listen to Jeff and you know the marketing is like add C nodes for performance, add D nodes for capacity. which is true up to a point because at some point you tap out on the throughput that the drives can actually deliver. Let's play the game though. Let's just be real and play the game. If we want to play the game and we use the same number of CNOTs for the sake of the argument and we want to go with capacity optimized not performance optimized then on the same scale how much capacity do we get which books are we using and how much capacity do we get well if you want ultimate ultimate capacity you would use the the maverick 5400 which gives you like five

[00:25:19.47]  Jason ValleryPetabytes of capacity per to you yeah and then i mean presumably you can work out then what the throughput result is and at an exabyte scale. So if we build one-

[00:25:33.38]   Remote- Yeah, I haven't got you in there. We could work that through, yeah. - So that's actually moving us from the same number of C nodes, and again, I wanna see a balanced system. I get your point about the fan-in, fan-out ratio between C and D, but assuming that we get the balance with 5.4 petabytes instead of 1.3 petabytes. bytes then we're actually building an exabyte system one system that can scale to an exabyte just for the sake of the argument again and we're not saying they're buying it we just want to show them the room of possibilities like to put two systems side by side and to kind of explain the design the question is how many c nodes do we need to put in front of those Mavericks for 5400. For it to make sense yeah. Yeah well we need to find out how many Mavericks we need to get to that exabyte scale and then it's like a two and a bit to one ratio of C's to D's. It's a quarter of 180 it's simple because the capacity is four times right. Well actually with Maverick right we tap out the performance of a Maverick with two C nodes. don't we, Ray? - Yeah. - Okay. - So, that makes the maps easier, let me... - Here is your system, then it's 40 Mavericks and 80 C-nodes, that's it. 80 on 40. - Let me... - 80 on 40 will get you an extra byte, and just different performance numbers, it's a capacity tier. I like Jason's idea here. - Well, so also this in video speak, this would be the data lake storage. - Okay. - Not the high performance storage.

[00:27:10.52]  Jason Vallery- It's still higher performance than anything Microsoft has, even if you're gonna call it the archived here. I'm gonna get, I'm interested to learn the terabytes.

[00:27:18.51]   Remote- If we go down to 80, so we're going down from 40 and 50. C nodes to 80 C nodes, then we're, let's just assume that the numbers are going to be cut by, it will be two terabytes a second read and a terabyte a second write. So it's actually lower than the numbers you shared earlier, 25 terabytes. So it's about the same, but it's not, again, it's very decent performance, almost the same. Same as the exabyte azure. Yeah, but it's it's slightly less not slightly more and

[00:27:50.26]  Jason ValleryBut I'm assuming he's much better. I mean since it's all flash. The latency is gonna be a lot better. Oh

[00:27:56.61]   RemoteYeah So that's gonna be roughly 210 Mavericks 210 Mavericks, that's the number 10 mavericks gives you pretty much exactly a petabyte and then we would need like what 420 c nodes in front of that to deliver the performance. Okay take it back it would be a monster again.

[00:28:20.55]  Jason ValleryAnd there's no reduction in capacity not the exabyte in my mind that was my mistake sorry. Let me clarify the numbers you're sharing. our raw capacity or is that after erasure coding to make some reductions after?

[00:28:34.92]   RemoteThat's usable capacity after erasure coding and then there's possible benefits for data reduction on top of that. However, with the NCP reference architectures, they typically will size it on the performance of no data reduction or similarity because it's like it's about. performance it's not about you're not selling on data reduction you're just selling on getting the performance to the cluster yeah so also taken to into account if we design this as a data lake storage it's going to be capacity optimized so we can reduce the cnode count even further to about uh uh uh what's 210 divided by well yeah so the minimum ratio is one to two right so we could go with yes 105 yeah 105 but that would kill that would seriously ramp down the performance right i i well i would actually like not to call it data data lake We're just giving them fruits for thought, right, the Microsoft team, and the fruits for thought is that this is the NVIDIA reference design, and this is the room of possibilities with the hardware, and I don't want to call it data lake. We're going to show the numbers, the performance, and everything, and if they think it's good enough, they can go and argue with the NVIDIA if it's good or not, right? trying to minimize it it's I would prefer and we'll show one that is an exabyte right the maverick 5400 and the other one we're building a smaller system but we'll in the same temple we show how that extrapolation of how does that get to an exabyte as well just to have apples-to-apples comparison so looking at the deck and the stack stuff you guys created for Enscale. How much time will it take us, take you, and willing to support it to any way I can to put the same type of data on those two configurations, table side by side, so I can work on the before and after slides and have a story ready for tomorrow's meeting with them, and I would love you guys to be on the call tomorrow. and explain it. I'm not the architect that is going to explain how it works. It needs to be one of you guys that will go through it. So Paul, we can basically take that PowerPoint you had for the Portugal one and just update it. Yeah I've got all, I've got in Lucid, I've got all of the sort of template racks for series and in the 15x9 kind of cell design and we can just draw up a rack configuration fairly easy that shows it. But yeah, it looks like we can squeeze the Maverick into 30 racks for the whole exabyte whereas we're looking at four or five times as many racks of that to do with 1350. so answer is yes we can get something done by this evening good because I committed to Jeff that I will share everything before end of day today guys it's not every day you get an opportunity to talk about 160,000 GPUs in an Exabyte store, it's all fun, because if you create side-by-side comparison to Azure Blob, will it be easier for you if this all goes straight to an Excel sheet or spreadsheet?

[00:32:06.65]  Jason ValleryYeah, I mean, I'm going to turn this into, so I have a one-on-one with Kishal tomorrow from MAI, we go back, like I helped them build. Inside Context, they have three exabytes of Blob in Phoenix right now, and I'm going to talk to him about this off the record, and I'll show him the comparison with Blob, so I'll take what you guys produce and put it into a one slide view that adds Blob as extra columns and share with him tomorrow, but yeah, or even if it's in Canal, I can just show him the exact same thing. for now, but ultimately, we need a polished walking deck that we can take and socialize to a bunch of different stakeholders in Microsoft about clear density and power benefits. I mean, in terms of data points that I want to see in this, it is number of racks, performance characteristics on read/write, total capacity, total power consumption. so we can draw the comparison across all of the dimensions is there any

[00:33:07.31]   RemoteSpecific metric that they are really focused on for example petabyte per gigabyte per second per watt or something like that I mean

[00:33:24.71]  Jason ValleryWhen you get to these big aggregates, the main point is being able to catch the checkpoint. So it's right throughput in aggregate that really matters. You know, the way OpenAI is a little less sophisticated than OpenAI, if this was going to OpenAI, they actually just stage all their checkpoints on local NVMe and then they async copy them. actually worried about ensuring you have enough throughput to sustain the frequency of checkpoint that you're making. Right? So if your freq points are every 60 seconds versus every five minutes versus every 15 minutes, like it's the sustained throughput model. Um, yeah, I is trying to get to that and maybe they're there by now, but they're experienced. with Vast in their Condor cluster meant that they didn't really need to invest in that, and so they were checkpointing straight off to Vast, and the throughput was adequate. So, you know, they may still be in that world, but I doubt it because they've been forced to, they already have some of this capacity online, like I mentioned, they're already trying to make this work with Blob, and they're hitting pain points. I assume they've already tried to solve async checkpointing, but I don't know that for sure and I can quote Kushal on that tomorrow. You don't need the architecture to try and hit the throughput numbers to be successful if you're doing it.

[00:34:42.01]   RemoteI'm just wondering because if these are going to be effectively Azure. It's an Azure in a box, and it's going to be sitting somewhere. and self-contained and it can operate in independent of what's happening in the back-end Azure like the DNS issues yesterday. Would that mean the checkpoints are going to be accumulating and at some point needs to be flushed

[00:35:10.97]  Jason ValleryOut to the blob? That's what they do yeah and so but in reality you don't need every checkpoint. So yeah, like a 3/5 checkpoint, every 10th checkpoint or something like that, they'll look at centralized, um, you know, just to have an offsite copy of it so that they can restore, and then, you know, often what happens is if they're doing like a research project, you know, they may switch jobs in which cluster is working on which job at what point they do like rollbacks. So, you know, They'll checkpoint to a certain epoch, and then they'll decide they want to change something about the way they're doing the training run. They'll roll back to a previous state, change some parameters, train forward, and then compare two checkpoints to see what was the better outcome, and so they're always kind of doing those tweaks, and so that's the primary scenario for these, and then they're collecting every so many checkpoints. storage for long-term retention and then their garbage collecting the local ones.

[00:36:06.67]   RemoteIn the Azure blob you're not, I'm assuming there's not going to be much deduplication.

[00:36:13.67]  Jason ValleryBlob has inline compression, but it does not many dedupe. You know there's, well I could get into some API semantics about how. our customer could achieve effective dedupe, but there's no inherent dedupe in the platform.

[00:36:27.42]   Remote- Hmm, okay. - So Jason, just to summarize it, other than putting it side by side with all the parameters you talked about and showing the solution diagram of how does it look like and the ability to explain what those boxes represent, right, do you think we should prepare anything else ties the story to Kubernetes or ties the story to Chi in a better way for the call tomorrow?

[00:36:50.88]  Jason ValleryI mean, we should call out the CSI driver and the maturity there. I'm actually not super familiar with the details on what's been implemented, but having Indian CSI support for volume provisioning and access and so forth is valuable.

[00:37:08.19]   RemoteWell, seeing as you've got the CSI, you've got the COSI, we can do block, file and object provisioning. Okay. But is that of interest for them to block, file and object for this type of environment

[00:37:24.92]  Jason Valleryor they only care about object? File and object. Well, I mean, MAI will care about file more than OpenAI will. um, eyes all object, MAI is file and being drug and kicking and screaming the object. Um, so I think, you know, calling out file and objects is interesting. I think, you know, single pane of glass for monitoring, logging, capacity views, all of that within the vast system, like one of the big. the biggest challenge that is happening is that Microsoft standardized on Microsoft Log Analytics, which is a shit product, if we're honest, for managing logs. There's an internal system called Kusto for all of the telemetry and everything, and that's not getting deployed into these facilities. So having VaST be able to provide a single... solution for monitoring metrics, logging, all of that is a huge win because Azure Storage doesn't have any of that. It depends on Kusto and Log Analytics and all of those other services.

[00:38:26.78]   RemoteWhat else? Continue. Spill it out. What else?

[00:38:30.22]  Jason ValleryOne of the things that's important, and I don't know that Chi will understand this, but certainly OpenAI and MAI understand this, is how the Bearer tokens, JSON Web Tokens claims cache against identity providers and how that end-to-end RBAC kind of model works and the capabilities that we have there, and then how that would enable this network isolation case, where if it's disconnected from Azure Active Directory, things will continue to work. Let me actually just up-level the bullet point because then, you know, to your point about DNS yesterday, having, you know, and I'm still learning all the vast ins and outs, but I understand it has its own sort of DNS routing kind of structure for object. The entire point is resiliency during network automation. if that fiber path is cut to the internet, that VaST can still continue to authenticate, authorize and route traffic from the GPUs.

[00:39:34.66]   Remote- Okay. - Okay. - Ray and Paul, like you are the VaST technical experts, anything that you just said that, you know, click well or anything else that you think we should emphasize, based on your experience? We don't normally mention much about working in an offline state. It's possible. Apart from that, what else? We do have a single... pane of glass for our own systems that we monitor. We've got the the provisioning systems. Actually on Azure, do they, besides the big open AIs of the world and the truly large customers, do they open up the API deeply to the storage layer or not really?

[00:40:31.87]  Jason Valleryby opening up the API.

[00:40:34.14]   RemoteSo as in initiating snapshots, initiating bucket creations, and like administrative-type tasks. So that's something else, Jason. So we are a Lifter program partner, and we are working with Microsoft APIs. How much emphasis do we want to put on that

[00:40:51.78]  Jason ValleryWhen you talk with 2G? Actually, I think it's the opposite, right? Because what's happening is like all of those APIs for the lifter program and all of those APIs that enable you to become a resource provider in Azure are the things that they deploy into these environments. So the whole point is that they need a very, very, very thin control plane that doesn't bring all crap into these facilities and still be successful. The lifter program, RPaaS, all of that shit that's great when you're in one of the big hero Azure regions, when you're running it in scale, you don't have any of it.

[00:41:24.19]   RemoteFair enough. How deeply is NIDI in this process of decision in your understanding, and who makes the decision here? She is researching, right? But who makes the decision of how to move forward?

[00:41:38.35]  Jason ValleryWell, I mean, this is the Project Apollo thing I've referred to. So Microsoft has to decide on the architecture for these sites going forward, and there are competing egos around what the correct solution is. The networking team believes the solution to be this thing called Extended. zones that I've described earlier, and so they're making their play to own this problem. She is, and this is me reading tea leaves, I don't know this for certain, but she is making the position that it should be an isolated standalone control plane, which is the side that I would come down on if I was to advocate, and you know, Nini is the stakeholder because it's her GPUs that they need to keep functioning and, you know, the owner of the AI story for OpenAI and Microsoft AI. So Azure Storage has an opinion about this, and they would certainly come down on the Extended Zone side because that's their only path to even be able to put any capacity into one of these sites. bunch of reasons why that still doesn't even really work very well. I imagine ultimately going to be a Scott Guthrie level decision who runs you know Azure and Cloud AI. Yeah so I don't I don't know how far along they've made that decision but I know there is a project in flight to make a decision around that.

[00:43:05.22]   RemoteSo we need to share the story. in mind that we are going to help them isolate the control plane so we can enable that type of solution with everything we offer and that we can also sustain the right operation even without this connection that is the live connection all the time to to Azure meaning it doesn't have to be an extended zone for it to work okay So Paul and Ray, thank you for prioritizing it this afternoon, and whenever you guys are ready to talk about what you prepared, just send an invite, buzz me, and let's jump on another Zoom to review it. Every time it is, between now and whenever, and it's something you're doing that I would love to-- get the details. If you want to engage more and you want to share any other questions or stuff, just reach out. I would consider anything else and jump on it. It's priority one. So I'm looking forward to this photo. Cool. We'll get going on it. Okay. All the best. Have a nice day. Bye bye. See you guys later. (keyboard clicking)
```
