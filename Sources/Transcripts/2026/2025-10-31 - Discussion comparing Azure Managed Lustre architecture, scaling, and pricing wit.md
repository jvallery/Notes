---
entities:
  customers:
  - '[[Microsoft]]'
type: transcript
source_type: unknown
date: '2025-10-31'
---

# 20251031 0802 Parallels Transcription

**Date:** 2025-10-31  
**Participants:** Jason Vallery, Lior, Wolfgang

## Summary

Discussion comparing Azure Managed Lustre architecture, scaling, and pricing with VASTâ€™s planned Azure offering. Covered Lustre tiers, persistence on Premium Disks, async Blob offload via HSM, scalability, AZ pinning, lack of ephemeral mode, RDMA roadmap, resiliency concerns for AI workloads, and VAST rollout plans (POC Terraform early Dec, Marketplace GA Feb, future Azure-native/Lifter SaaS).

## Key facts learned

- Azure Managed Lustre tiers: 500/250/125/40 MB/s per TB; 500 is bandwidth-optimized, 40 is capacity-optimized
- Provisioning increments provide ~2 GB/s per increment; tier differences are in attached disk counts per OSS
- Backed by Premium LRS Managed Disks (v1 today); no ephemeral deployment
- Availability Zone is mandatory; VMs and disks pinned to a single AZ
- Scales today up to ~12.5 PB on MLS 40; higher targets planned
- Blob integration via Lustre HSM is asynchronous; preserves directory hierarchy in Blob
- Strong consistency (write-through to Blob) not supported; HSM state can be checked per file
- Loss of MDS/MGS makes FS unavailable; OSS loss degrades performance but I/O continues
- Managed Lustre often 30â€“40% cheaper than VAST modeled on L96 v4 (license excluded); L192/Lsv5 pricing unknown
- Guest RDMA between VMs not yet broadly available; future enablement under discussion (Boost/Overlake teams)
- AWS offers an ephemeral tier; Azure Managed Lustre does not
- VAST initial Azure approach uses available VMs; performance bounded by NIC (e.g., 40 Gbps)

## Outcomes

- Shared detailed overview of Azure Managed Lustre architecture, scaling, and pricing rationale
- Aligned on value of Blob offload for cost and namespace scale despite async behavior
- Agreed to continue coordination on RDMA timelines and VMSS Flex/fault-domain changes
- Plan to connect at Supercomputing; Jason to join existing meeting with Lior

## Decisions

- VAST will proceed using currently available VM SKUs for the initial Azure release even if limited to 40 Gbps
- Target POC/Terraform deliverable in early December
- Marketplace GA target is February; Azure-native/Lifter path targeted later (tentatively Q3)
- VAST SaaS control plane to run in a VAST-owned tenant exposing endpoints into customer tenants
- Azure Managed Lustre remains persistence-only (no ephemeral), Premium Disk v1, AZ-pinned; Blob offload stays async

## Action items

- [x] Deliver Terraform scripts for VAST Azure POC @VAST Engineering â« âœ… 2025-11-08
- [x] Define backup/recovery strategy for VAST on Azure (RPO/RTO, snapshot/offload, rehydration) @VAST Engineering ðŸ”º âœ… 2025-11-08
- [x] Confirm if Azure Managed Lustre supports Premium Disk v2 (Direct Drive) and share timelines @Azure Managed Lustre Engineering ðŸ”¼ âœ… 2025-11-08
- [x] Coordinate RDMA enablement timeline (guest RDMA between VMs) and implications for Lustre/VAST @Azure Boost/Overlake teams â« âœ… 2025-11-08
- [x] Hold roadmap discussion with VMSS Flex/Jerry Steeleâ€™s team on fault domains and Lsv5 readiness @Jason Vallery ðŸ”¼ âœ… 2025-11-08
- [x] Share SKU pricing/availability guidance for L192/Lsv5 to inform cost comparisons @Azure Compute ðŸ”¼ âœ… 2025-11-08
- [x] Schedule Supercomputing meetup and add Jason to existing session @Lior ðŸ”½ âœ… 2025-11-08
- [x] Document current AZ pinning/locality guarantees for Managed Disks and Lustre VMs @Azure Managed Lustre Engineering ðŸ”½ âœ… 2025-11-08
- [x] Assess feasibility to expose HSM state/automation hooks for users waiting on Blob archive completion @Azure Managed Lustre Engineering ðŸ”¼ âœ… 2025-11-08
- [x] Advance Marketplace listing work toward February GA @VAST Product Management ðŸ”¼ âœ… 2025-11-08

## Follow-ups

- [x] Share detailed tier/increment documentation and performance tuning notes with Jason @Wolfgang ðŸ”½ âœ… 2025-11-08
- [x] Provide update on Blob HSM enhancements preserving directory hierarchy and any roadmap changes @Azure Managed Lustre Engineering ðŸ”½ âœ… 2025-11-08
- [x] Confirm dates/logistics for Supercomputing booth and meetings @Wolfgang ðŸ”½ âœ… 2025-11-08
- [x] Track Lifter/Azure-native VAST milestones and SaaS architecture decisions @VAST Product Management ðŸ”½ âœ… 2025-11-08

## Risks

- Async Blob offload can expose data loss window before archive completes
- No strong consistency option between Lustre and Blob
- High downtime cost for AI workloads; recovery/RPO/RTO plan not finalized for VAST on Azure
- Fault-domain limits (e.g., 3) force less efficient EC schemes
- Guest RDMA availability and timing uncertain, impacting performance roadmap
- Managed Disk provisioning/attachment complexity at scale

## Open questions

- Will Azure Managed Lustre support Premium Disk v2 (Direct Drive) and when?
- What is the concrete timeline for guest RDMA between VMs for Lustre clients (GPU Direct)?
- Can any tunable consistency mode be offered for Blob offload, or only async with HSM state checks?
- What backup/snapshot/rehydration mechanism will VAST use on Azure and what are the target RPO/RTO?
- Is there any additional placement/locality control beyond AZ to reduce Managed Disk network path latency?
- When will pricing/availability for L192/Lsv5 be published?
- What EC scheme will VAST use given 3 fault domains, and what capacity overhead will result?
- Could pass-through reads from Blob ever be supported, or is Lustre always required to rehydrate first?

---

## Transcript (auto)

```text
[00:00:00.04]   Remote(mouse clicking) (keyboard clacking) I apologize. I'll be switching over to my phone. 15 minutes in, I've got to head off and for a surgical consult to see if, see if that's okay with you. I'm going to switch over to my phone. I apologize. I'll be switching over to my phone. I'm going to switch over to my phone. 15 minutes in, I've got to head off and for a surgical consult to see if, see if that's okay with you. I'm going to switch over to my phone. necessary or not, so I apologize in advance. I'll be switching to my phone.

[00:01:11.93]  Jason ValleryOh, well, I hope you're nothing serious.

[00:01:13.84]   RemoteYeah, I mean, you know, I've had a history of back issues, right? And I had a really nasty flare-up about 10 days ago when dealing with some pain and ongoing weakness and numbness. in my right leg. So hopefully it'll just be a cortisone injection and not, you know, not going under the knife. I do not want to do that again.

[00:01:40.54]  Jason ValleryYeah. My wife, the cortisone stopped working for her after, I don't know, a few years. I feel like they would do it every month or something like that, and then it just ended up no longer doing it. So she had...

[00:01:52.53]   RemoteThis is for her knee, but they ended up replacing it. Yeah, I gotta tell you, I've never, I mean, I've had it done a few times before, nothing like the frequency that your wife went through, but it's, I haven't, same thing, I haven't had lock, it just, it just doesn't seem to work. Everybody's constitution. different, but yeah, it didn't, it hasn't helped me in the past. Ah, crazy. So what's up, how can we help?

[00:02:23.51]  Jason ValleryWell, I mean, we were, we were, uh, you know, chatting about a few different things and, you know, he thought it would be good to connect in here a little bit more about the managed luster offerings and the comparison and how that all stacks. up, so I think that's what Leroy's goal was. I wonder where he's at though.

[00:02:43.66]   RemoteOkay, no worries. Oh, he's usually running a little bit behind, call to call, so there he is. Perfect, let me let him in. I think you and I, Lior, are in a, in a, a fierce competition over who's late because their prior call. If it's not you, it's me. Did you guys start?

[00:03:30.03]  Jason ValleryWe haven't. No, we didn't. Setting kind of a frame of what we were wanting to accomplish. I mean, Leora and I were chatting and he thought it would be good to look a bit at managed luster and architecture and that kind of dimension and what kind of performance numbers you get

[00:03:45.06]   RemoteEarlier or did you have other things we wanted to cover? No, no it's again we're looking at our own design of our solution moving forward and we just want to see like learn from your best practices and Wolfgang last time we spoke you had some kind of you explained the way Managed Duster works and the way it scales and the performance tier and the capacity tier and the usage of different VMs, and we're exploring usage of persistent storage on Azure as a part of our solution, and I think for Jason, who just started and he's now getting familiarity with the way Vaas does stuff on Azure. It can give him some good advice as the product manager of what does he want to suggest moving forward on the next generation of our offering. So anything you can share about. Manage luster and what worked, what doesn't work, how it works, how performance is tuned. We also talked about pricing. So maybe if you can retouch the pricing aspect of that, that would be great. No, no, sure. So yeah, no, let's see, and first of all, nice to meet you, Justin. We never met while you were in Microsoft. It's a pleasure to meet you.

[00:04:51.08]  Jason ValleryI saw your name. I knew some of the work you're doing. I think you were supporting Vishnu at one point and some other things. So yeah, awesome.

[00:04:57.87]   Remote- No, no, no, and yeah, I know of course all what you are doing here. So no, no. We know each other, but we don't know each other. So let's say that just as a preliminary thing, I will give you at the moment, a general review of the service. What are the information that are published? or whatever they are basically written between the landed documentation for whatever that is a little bit more. Maybe confidential. Just to be clear, because I don't know how much we are deemed that confidential. We can also follow up with our engineering manager eventually, so that is not a problem if I have any doubt that we are in a gray zone. Maybe we will cross validate and check, but let's say so. Luster today has four tiers which are basically just with a number more specifically is 500, 250, 125 and 40 and those number means the throughput in megabyte per second in mega capital B per second that you get for each terabyte of provision. at the file system and let's say that if you look at the pricing of that the idea is the following that by the way if you go it's very close also what AWS does with FSXL which is basically the there is the highest year the MLS 500 which is the cheapest way you can achieve a specific bandwidth target While there is the smallest tier, MLS 40, which is the cheapest way where you can reach a certain capacity target. Of course, with less bandwidth, and of course, in the middle, there are all the mix and match, let's say customer that has, I don't know, I want X petabyte with that bandwidth, maybe they can find a sweet spot in 250 or 125. So in the middle. So that is more or less the rationale and yeah, this is something of course that if you deploy an instance, you can see pretty easily also for after you mount that from the common line. It's that, let's say the difference between the tiers is only the configuration of the objects to our servers of Azure Manager cluster, meaning that, let's say. In our configuration, when you deploy, you can deploy in incremental units that are different in terms of capacity between the different tiers. So let's say that MLFS 500, you can deploy four terabyte steps and each four terabyte steps provide two gigabytes per second in each step. MLFS 40 you're deploying in steps of four. Terabyte and provides you the same bandwidth over. Let's see each increment. So it's still increment of 2 gigabyte per second. This because what changes between the theory is that there is the same bandwidth that you can get for each increment slash objects to a server. Of course, what what changes is the amount of data that are better. or disk that are attached to the object storage server. This is basically how it is configured and this can be seen also if you deploy an instance and you do lfsdf min-h you will see the osts with all the configuration, and by definition let's say of course and here is the part where I don't know how much you will find it useful the two solution let's say know vast technology. Let's say at a level, say medium level, not for sure at a very deep level, but let's say high medium level. I know that I don't know how much you will find valuable that. But as you know, luster as open source project doesn't ever Azure coding, so doesn't have any possibility. Let's say by definition of having a between the server, meaning that if you lost an object or a server and basically you're not able to recover the disk for any reason to attach that to another host. Of course, the data don't have a redundancy built in, so there is no. There are no stripes or basically any form of redundancy of the same data on is a preliminary project that is in the open source community creating file level redundancy, which is meant to create redundancy at the file level in different OSD. That will basically start-- it will start generating that, but it's very preliminary. So as is written in our documentation today, the way it works is that we need to run on premium LRS disk, Meaning that they are managed disk and in case of course there is a failure of the host, there is basically the same or they also will make the VM will migrate to another host and the disk will come up visible and the file system operation will continue. So the degradation is very in term of performance. The duration of a degradation is relatively small. Because, of course, you were on Azure, you know basically what is the time for a NOS migration in case something happens and let's say that during I don't know how much you know of luster in general software system. Of course, when you have an object storage that goes down, different system remains totally available. You have only basically a degradation in performance and potential. there is somebody that tries to access the stripes on that object storage, will have an I/O hang, but will not be disruptive. So, let's assume that in that moment you have a checkpointing event. What happens is that the checkpoint will hang, maybe instead of lasting 10 seconds, that event will last three minutes. That, of course, is not great, but it will not break the I/O. So the only condition where basically we go in let's say not available state is the situation in which basically we lose the metadata servers and the management service. That is basically when the first system becomes a bit unavailable, but also that, of course, it's in generally in generally very, very limited. resiliency is built on that and in reality we don't have at the moment, at least with the current Lustre technology, any choice because Lustre as a system doesn't have any replication mechanism to be resilient. However, at the same time, we are, let's say, investing pretty heavily in also on the integration of luster with blob as you can see of course from the documentation and also there unfortunately we are paying a little bit of the history of the luster project as it is because we are building this against the component of luster which is called luster hsm which is basically the part of luster that allows you to Basically, replicate the data at a file level on a different storage back-end, and eventually even to leave only the metadata on Blaster and basically totally releasing the objects part so that basically you free up space, and what is basically, of course, the challenges in the implementation, and this is something that there are also people presenting at the conference about the complexities that that mechanism was meant to work with tape or basically with very low performance file systems meaning that what was doing was creating on the back-end files with just an FID. So basically there was not a hierarchical structure in the storage back-end. What we are doing for BLOB is kind of different because we want to keep exactly the same hierarchical structure of the files on BLOB, meaning that if somebody wants to access BLOB, let's say with BLOBFuse or with EasyCopy or with whatever way in which he can hit the REST APIs, even BLOB over NFS, we want them to have the same data structure in an accessible form. format. In a transparent way, because in that way, and this is by the way, so it's what the AWS is doing, meaning that in that way you can potentially. More work away blob only when you need basically a specific bandwidth density on a part of the namespace. You put an Azure Manager luster on top, you accelerate that part, but you're able to iterate data from blob, and vice versa. So for us, rather than being a resiliency mechanism, being basically able to offload data with blob is more of. Integration with Azure Ecosystems story, meaning that it is more about having a copy of the data. I have this year that can be offloaded on blob so that they become available to. that can basically take this data with a REST API call. Let's say, for example, that completed your training, just making an example, you want to dump your checkpoints on Blob, and then you want to bring up an Azure ML environment, and you want to host your model for inferencing. Of course, it's easier to basically have Azure ML communicate to REST API with a Blob storage account. Rather than being VNet integrated with luster because of course Azure manager luster doesn't have rest API way of accessing the data. So yeah, this is more or less this situation and. Of course. Up to when luster will not have any technology that. Will will allow basically to. lose a certain amount of host without having data compromise, for us, everything that is an ephemeral storage is of course not viable, which of course is way different for you. So yeah, this is more or less the thing and in terms of pricing, let's say we made this exercise also with Leo, what we see today and let's say we, at the moment, the only thing that I can say is, let's say, if we consider L96v4, which is basically, at least if we consider what is available, if you go now on the portal, let's say, Luster is way cheaper, and let's say it can be 30 to 40% cheaper as compared basically to Vasta, even if even if we consider we don't consider license cost of course if the l192 before and eventually of course the v5 as it's your expectation will be better in terms of balancing between storage network bandwidth and so on of course the the situation and the modeling may change i don't know if you got any pricing i personally didn't about for example, L192, but that is also another element that of course will be very important for you, and we were discussing this with Carol offline, about as much as you can accelerate in having a good integration with Blob, will eventually. Thanks to your technology, you will be able to basically offload probably a good part of the cost of storage in cases of, let's say, a customer has 100 petabytes, but that's only 10 petabytes in warm, hot, having the other 80s residing only on blob, of course, for you will be an unfair advantage, and I don't know if today you are integrating with other object storage on Prem or in other clouds. Even let's say I and I don't know if you are tied to doing that at the file level or you are able to do that at the block level, but I think that would be probably one of the high. If I let's say if I was leading the product, but you are leading probably my view. I see that one of the top priority, at least from an Azure perspective.

[00:16:32.20]  Jason Vallery- Let me ask some questions. So going back to solution A, the one in market leveraging premium disks, how scalable is that? I mean, I know premium disks and the provisioning model and some of the challenges there and like how far can you take that solution in terms of total namespace?

[00:16:53.09]   RemoteOkay, so let's say that we today we are able to go up to 12.5 petabyte already today so if you go to commercial you can deploy on MLS 40 12.5 petabyte. We are planning to go higher in the next. months. I don't know if there would be something coming out to supercomputing, but let's say we are talking about the number of petabytes that starts to be pretty considerable, so already 12.5 I see that a good number, but it can become more. I see that very, very scalable. The only thing is that, of course, you need look at, from a pricing model perspective, what is the bandwidth density you're looking at? Because, of course, if you look basically at the bandwidth density that you can have with the premium disk, as soon as you start basically requiring to have higher bandwidth in lower space, you need to start combining multiple premium disks to get in RAID to be basically get to there. So if you look basically at the bandwidth cost and capacity, you will see basically that it starts to be challenging and challenging if you want to get super bandwidth density. So if you want to get two gigabyte per second per terabyte, starts to be complex, but that is probably about the technology. itself, which compared to local NVMe on the host, is, of course, totally different from a-- let's say, from a working perspective, a protocol perspective, and transfer between the VM and the remote storage, I mean.

[00:18:38.36]  Jason ValleryAnd today, is it just Premium v1? Do you support Direct Drive Premium v2?

[00:18:46.16]   RemoteI can ask to the engineering manager, but I think we are on v1. Yeah, I'm pretty confident it's only v1 today.

[00:18:57.90]  Jason ValleryYeah, and then when you think about network pads for disks compared to the L-series VMs that you're attaching to, is there like some notion of of pinning to ensure that the VM running Lustre is in, is adjacent to the XIO cluster?

[00:19:15.10]   Remote- There is only an availability zone concept. So there is not nothing at a lower level. So of course you have both the VM and so first of all, this is something I should have told you at the beginning, when you deploy an Azure manager. cluster, there is the mandatory parameter, which is easy. So you need to specify the availability zone you are deploying to. Of course, if the region doesn't have multiple availability zone, of course, it goes without saying in that availability zone. But if you deploy, you need to pick the availability zone, and in that case, both the VM and the disk will be pinned to that availability zone. zone. This doesn't mean, and let's say, as you know, same data center in some cases, but in many cases, hopefully, it should be the same data center.

[00:20:02.47]  Jason ValleryYeah. Why offload the blog? Well, I mean, I have a good answer here myself, but were there issues with this architecture that caused? were running into that really necessitated the push to blob?

[00:20:20.16]   Remote- No, it's more, it's more due to the fact that in that way we can achieve basically, this Luster started from the HPC world. So many HPC customer, as you know, work with. a working data set which is relatively the 10, in some cases even 5 percent of the overall data set, having the possibility to say my working data set that I have really on luster is smaller than the overall data set I could potentially still see and access is very valuable for them. So let's say for example who is doing seismic processing? Who is doing seismic processing? if we go in HPC domain, what he's doing basically is probably doing projects where in the single project they have one or two petabytes of data, but then the overall archive data can be petabytes and petabytes, so it can be 100 petabytes or more, having the possibility potentially, let's say to access that namespace. If you need the basically. or basically having a direct and transparent solution where you can say, OK, I've worked with this data and done. I released that to blob. But if next week I need basically to just recover them. I don't need to do anything special. I will just access the file. I will have some delay because they're coming up from blob. But basically you have that seamless integration that is very useful. We have seen that also in automation. automotive industry, people that do projects, then in six months they need to start a very old project, and having that seamless access, let's say, I just copy the folder I start from there, and I'm able to recover that from something that has not been occupying space for the last six months, is very valuable. In a similar way, if you go in the iDomain, it's also kind of similar if you want. First of all, let's take a cases where your training data that you have available for your training are basically way higher than what you consume in a single training job or in multiple training jungle over one week. Having that type of layered approach between a very big core storage layer and accelerator layer is very valuable for the customer. for checkpoint, I want basically you can have an automatic way to say I keep only the last 100 checkpoints on the Azure Manager cluster and then starting from the oldest I will start offloading to blob. All of that is very valuable because if not the only way that the customer has is to or delete the data. Or eventually to manually move to another to another solution. I know vast. Of course, you can support data movement between similar vast architecture. So, but also there, of course, depends on what is the customer configuration and if really. That let's say they can manage that all with us so. at the cloud let's say you're working with that let's say for years and also at a scale that I've never seen you know the lowest price point per gigabyte for data storage is blocked there is nothing nothing else better than that. Yeah and in a nutshell Jason I mean you know the few of these opportunities that I've been involved in like NERSC and Oak Ridge these are these are hobbyists to the extreme, right? You know, they're not buying luster solutions off the shelf. They're, you know, they're building these. It's incredibly cost effective, right? Like, they're okay with a science experiment and putting together the system themselves. So cost is a huge driver for all of these research institutes, and, and it's it's economics right i mean blob is blob is how we, we put something together that is that's cost competitive.

[00:24:10.45]  Jason VallerySo it's got to be a part of it, where I'm pushing is just to understand, like, where the pain points associated with using managed disks for an architecture like this come. and what the real trade-offs in value are, and so when I think about it hierarchically, you've got NVMe, you've got attached managed disks, and you've got offloaded log. Is there actually value in the attached managed disks? Because in my mind, what I see is mostly a bunch of pain.

[00:24:38.05]   Remote- Yeah, yeah, it's just the persistence for Lustre. Because Lustre doesn't have that concept of erasure. It's purely for the persistence and the three replicas. - No, exactly. Let's say that in terms of latency, so if you take an LSV3 and you measure the latency that you have on the attached NVMe, which is on the host, is of course way, way better, order of magnitude better than what you can get from a managed disk, because by definition it's a... remote storage. There is caching, there are basically other mechanisms that can help mitigate that, but let's say of course it depends also now the storage solution reacts to that. Let's say that in our case, Lustre Architecture by definition, the object storage also themselves, the file system itself is able to do a lot of caching. risking a lot of the latency in some scenario. But for example, if you start doing 4K random rates, of course, that is something that could have a huge difference if your OSD is on a local NVMe or on an attached disk. So, that is something for sure to take into consideration.

[00:25:49.10]  Jason Vallery- But, so from an architecture perspective, if you had And maybe Premium Blob is the answer here. If you have good performance out of Premium Blob and you could-- and I'm assuming this is about cluster spin up and spin down and all the metadata that Lustre keeps within its own state. Is that what's being paged in and out of disks today? And can you replace that with Premium Blob such that you wouldn't have to use managed disks at all?

[00:26:15.48]   RemoteThat the so first of all the underlying hardware is not so different right that

[00:26:20.32]  Jason ValleryYou can confirm that to me yeah from our about like the problem of managing attaching disks and detaching disks okay failing and so you mean that between

[00:26:30.66]   RemoteOkay between being a block storage or an object storage I mean yeah no the challenge there is that last Luster is meant to work on a block device. So let's say it's a Luster is. When you basically create an object or a server, you format the block device on that server with Luster FS. So there is a common line which is makefs.lusterfs and you format that so. By definition, it expects to have a block device attached.

[00:27:00.72]  Jason VallerySo I guess going directly to yeah, if you've got local envy me you're using that as the block of X, right? What what I I maybe I'm just not familiar enough cluster to understand the right question to ask here But you've got local envy me for the high-performance tier and presumably lots of also store. It's metadata And then you've got these attached disks and in my mind those were to extend the capacity of what Lustre had, and so what then is the purpose of them in a world where you're using object for that capacity extension?

[00:27:35.00]   Remote- So for Lustre architecture, when you access file and when you access the data, for how the architecture is built in the project, you are accessing them from the Lustre file system. You are never reading directly from the backend storage. So every time you access a file, the process is the following. File on blob gets back on Lustre and is striped on the OSS. So basically, let's say that you have one terabyte file, it gets striped on all the 20 or 100 OSS, and only then the operation can. can start reading. So there is no way to do a full pass-through unfortunately. Yeah and these are these are the limits of you know open source luster and while the team is contributing right like well we've got experts there in Pittsburgh who are working on it it's we're still you know limited by the overall open source project right. >> Yeah, I don't know exactly.

[00:28:32.52]  Jason Vallery>> Well, what is cluster spin-up and spin-down actually look like? Actually, let me back up. If you haven't attached any managed disks, because that is a deployment option, you can deploy Lustre purely on the ephemeral NVMe with Managed Lustre?

[00:28:44.60]   Remote>> No, you don't. You can only do persistence.

[00:28:48.31]  Jason Vallery>> Oh, you have to have managed disks. Because that's what you're using for. So you don't have a redundancy option within the cluster itself.

[00:28:56.59]   RemoteNo, no, exactly you. We let's say there is AWS. That is something that in general. Honestly, I'm very curious about to understand how they're doing that AWS on top of the tier that I mentioned to you as also a ephemeral tier. Where they're calling out explicitly that if there is another. But I don't know how your money they are managing that in a communication with a customer. So this is open on their documentation idea. I'm not sharing nothing that is not there, so I'm really curious. So I don't know if they have an option where they say to the customer. Yeah, you can deploy an ephemeral, but don't get to complain with us if at a certain point doesn't work anymore, and I don't know how this can even work in a cloud service.

[00:29:38.12]  Jason ValleryBut yeah, well, that's so that was actually a good segment of my next question, which was consistency models. Is there an opportunity to configure with the blob offload scenario, you know, like a strong consistency where it writes our DAC until they're in blob or or is it always? Is there a data loss risk?

[00:29:55.04]   RemoteYou mean writing directly to blob, but without going through luster, you mean?

[00:30:00.99]  Jason ValleryGoing through Luster, but ensuring that it's been offloaded to Blob before acting back to the client. Meaning, getting to strong consistency through Blob, but making sure...

[00:30:12.58]   RemoteBy definition, it's asynchronous.

[00:30:15.55]  Jason ValleryYeah, it's async, so you have... Meaning.., and there's no tunability in that? You can't tune any kind of consistency models between the... local and blob capacity?

[00:30:25.28]   Remote- So let's say if you want a consistency at the level that you don't close the POSIX, you don't get the POSIX close up to when data are on blob, that is not something for sure out of the box. The only thing, of course, you can be sure about, there is in Lustre, there is the possibility on each file to basically do an HSM state, and we'll tell you exactly if it is archived or it's not archived yet. If it is archived, you're sure that is consistently on blob. So you have a way to check that, but it's asynchronous, and again, this is from the history because when Lustre was meant to use this HSM integration was meant to be with tape system, with very slow system. So when you trigger an archive of the file, don't want the I/O to hang in any mean, waiting basically for data to slowly move to the back end. So it's again, part of the tradition somehow, and again, let's say we, you know that when I'm sharing this, I'm not saying that the Lustre is the right technology. status of the art is what we have implemented today is what basically is available as a managed service. But of course, when it came into the cloud, you see all these things that we're talking about are indeed kind of elements that came from the history of the project.

[00:31:48.64]  Jason ValleryYeah. If you were starting Managed Lustre over today, what would be different?

[00:31:53.10]   RemoteLet's say, you know, I think that are interesting or there are other open source project also that are very interesting. You know, what GCP has done with Deos, for example, that they went in the Deos direction to have a manager service in that direction. Deos, for example, has already many of the features that were not from day zero in Lustre. you know, they have a recording, they have the possibility to run on NVMe, and they have their resilience towards failure. So, for example, that is for sure something that if I have the possibility to start another project, and of course, this is something that we are discussing here, and we never talk about that, is maybe looking at technologies like that would be... more efficient if, of course, you want to go in the direction of something not properly there, an open source in the same line of luster. But restarting from scratch, of course, I see that very hardly to be done differently with the current infrastructure that there is on Azure and with the and with the luster behavior. Let's say that you have been. So of course if you go in the direction of having basically something dedicated, of course, that becomes another story. But then comes another problem which. I should change it a lot. You know we deploy resources and this is something of course you can notice also from our page. But first there were like 3 or 4 regions that had a great store offering, so this is something that you can notice also from our page. which had all the specialty SKU. So basically there was West Europe, Sweden, East US, South Asia, that were the aero region. The landing of specialty SKU started being much more fragmented, meaning that now it's very easy basically to have 20s or 30s of region where you need to have basically physically a 9.9. performance storage and a very specialized storage like luster and in that scenario, if you went in the direction of dedicated hardware rather than basically relying on the overall Azure infrastructure, of course, you would have basically been in a very bad situation because at a certain point you should start deploying harder with all the overhead that can have in all the different data center. So. Let's see if you would have. two years ago probably I would have said yeah maybe we could have evaluated dedicated hardware but seeing how basically also the deployment has changed and now specialized computing is basically has spread I would say that I'm not anymore sure that would be the best and I don't know Carly if you have any opinion on that also. Yeah it's kind of interesting you know obviously Neither Wolfgang or I were asked at the time, but you know, the need being for a high throughput parallel file system, you know, the other options should have been evaluated, right? I mean, obviously, Deus has really come a long way and we expect to see that even further with with HPE. taking control of Daos. I mean, God, old IBM has done some really compelling things with GPFS and storage scale. Obviously vast. I mean, there's some parallel capabilities with PNFS work that that the team has done there, but... there would have been other ways for us to do something with a platform that could have made better use of what we have at hand, right? In the data centers at scale.

[00:35:30.64]  Jason Vallery- Has the team started playing on with guest RDMA and using that?

[00:35:38.15]   RemoteI don't think we did that because I I I don't think that there is still the possibility to do that. But of course this is something that we are actively looking into because after you know this has been announced that the build I think and there was also a demo with the next generation larger booster should a demo. Let me. that we are actively looking in, meaning that the luster out-of-the-box supports mount over RDMA, and that of course will allow us to get to have nice things like GPU direct and so on that AWS already has, thanks to Elastic Fabric. So we didn't start as far as I know, but of course this is something that it's very interesting. by the way let's say I guess that I don't know if on LSB5 you have any insights on that because

[00:36:29.24]  Jason ValleryProbably you know more than LSB5 than I do. I mean it should come for LSB4 so it should be

[00:36:36.44]   RemoteEven on existing once it's rolled out. Yeah there's a lot of debate right now with the with the Boost team, with the Overlake folks. There's a huge camp inside of that team that doesn't wanna bring RDMA between VMs to the masses until Overlake 3 is released. So that's one of the reasons I wanna have that call that... that we tried to have yesterday, Leore, is to make sure that we're tracking all of these items because there are so many groups involved outside of storage, outside of EGLE's team. We've got to get Mutu and Nico Pamblocas and his team involved to make sure that we're in lockstep. on when we're gonna have access to RDMA. - And the only thing is that one thing that, and again, please don't get me, don't get this as something that I want to say to create any pressure or any concern on your side. The other part is that I think for you having. So first of all, let me ask you a question. So let's assume today you are deployed. Let's say in Sweden central easy one and let's say you have a alpha petabyte file system and There is basically a rack failure at a dimension that basically you get a number of host Dying that doesn't allow you to recover operation just with a raju coding. What is the recovery plan in that scenario? And this is just for me to understand. I'm not saying, I don't want to check if you have a recovery plan. I want to understand to share

[00:38:26.82]  Jason ValleryWith you what are my thoughts on that. Well, I mean, it seems looking at this, the problem is the number of fault domains that get exposed to the instances. So what it means is a far less and EC scheme will have to be used. I was talking to the team about this the other day. I think there's only three fault domains that we get exposed, and so they're planning to EC at a level that we can recovery assuming the loss of that entire fault domain, which means, I don't know exactly what scheme they're gonna use, but something like 663 or something, and then you're in a state where you're far less capacity efficient.

[00:39:00.71]   RemoteAnd in that case, let's assume the worst happens. So something that should not happen, but let's say, let's assume that you get in a situation where you need to restart from a snapshot or from a backup. What is the process potentially?

[00:39:15.32]  Jason Vallery- We have to decide, we don't have a solution. Like be able to- - Oh, okay, okay. - Obviously- - No, no, I didn't want to- - Like the question becomes like, do we, fully offloaded blob and depend on being able to rehydrate, or do we have some sort of like snapshot backup drain to blob restore cluster capability. There are different levels of engineering work and they're sort of orthogonal in implementation, so you know the better path here is a full offload implementation of blob with a tunable consistency model in my mind, but

[00:39:45.72]   RemoteYeah, there's going to be a lot changing with with the work that's being done with VMS skill sets flex. There's a yeah, I mean, obviously, we still have a rack as a potential failure domain, but fault domains in terms of, you know, how they're referred to how they're exposed to there's a lot that's changing. So we've gotta have some near-term roadmap conversations with Jerry Steele and his team, and make sure that we're positioning you guys for that LSV-5 release in a way that gives you the ability to amount of overhead. Right. There's a lot of work that needs to be done, a lot of discussions to be had, but with the LSV4 release, hey, what we've got on the truck is what we've got, and you know the warts there, but the good news is there's near-term changes coming. Yeah, no, and this is just to say that, let's say, first of all, First of all, let's say, and I talked about this to Leo and also to Karl, I don't see, let's say, and I don't perceive that we should have a relationship with competition between the products. So I get what I'm saying just to think, the thing that I suggest to prioritize is that your storage, if basically you're coming in to be an AI storage. and this is your positioning and is exactly basically what you are doing outside the challenge and you what you I think is very important to be aware that you are connected to critical infrastructures that an hour down time is super expensive and that is the part that let's say it's very very risky for you and I think it's something that having the project of a fluoride into blob and rate rating. Having that type of scenario is super critical to be sure that there is a way to recover in case the worst happens. Just because, let's say, as I was, if you would be running on, attached to an HPC storage, as an HPC storage, that pressure, if you look basically HPC node like an HBV4 and HBV3 would have been high, but would not have been so high as compared basically to, of course, an AI node. That is the part where really, let's say, and I'm keen to support also in the conversation with Karen to help with whatever experience I can share, but I think that is extremely important having, I don't, of course, because of where we are today with Azure and where it is vast with the blob integration. There there there cannot be tomorrow. I guess a solution will say so. OK, we have zero downtime. We are kill of 10 second. Of course that that is not possible, but I think what is very important is to have a part if a customer deploys to say OK, if this happens, these are the steps you need to take to come back online with. Basically, a recovery window of the data of, let's say, one hour or two hours, whatever they need, meaning that you lost maybe one hour of data or three hours of data or 12 hours of data. I see that as a priority, but no, sorry, this is the only thing I really wanted to share when I was thinking in the last weeks about about vast that would be the only thing that from a product point of view management perspective, I already prioritize on Azure at least. Of course, I know on-prem and in other scenarios, we don't have any issue at all on this. Our initial release on cloud is, again, for the first positioning, we need the customer's burst. So the data is on-prem, and we're using global namespace to move a small piece of the data in. the cloud. So if the worst comes to worst and the cluster becomes unavailable and they need to rebuild it, I guess the GPU resources are on demand. They can shut it down, move the data again and start it up. So they don't need to keep it up and running and waste so much money. But again, it's in phases, like we're not there at the point where we can sell to our customers persistent storage liability and resiliency on Azure. Now, that's based on the VM approach. We have other approaches which are not for the VM, other programs that will give us a better solution that can address the use case completely. But that's more with specialized hardware. It's not with the VMs. >> Yeah, sure. Again, I'm not saying it's a vast limit. is the combination of fast and now Azure works today that is generating today that limit. You're correct, you're correct. That's why we're not promoting the VMs as a persistent storage solution that will always be up and running without moving data to global, without creating something else that doesn't exist today. I'm with you, you're right, and again, I'm not saying this to say Lustre or Rust is better, no, I'm just saying that That is probably-- I just want to anticipate that if you get in a customer that has that bias that the storage is persistent and something happens, it will be a very bad conversation because, let's say, with Lustre, let's say we are under a tremendous pressure, even for minutes of downtime. So that is the problem. which I just wanted to share as an experience but not to generate any

[00:45:05.14]  Jason ValleryPressure and you don't get me wrong really. Stop with mine, it's definitely something else.

[00:45:09.82]   RemoteI know, I can imagine. Yeah, not a well-kept secret yeah I apologize guys I'm about to walk in to the doctor's office so i'm gonna drop but uh yeah yeah thank you guys i appreciate it and good good to talk to everybody we'll catch up lately or we got a bunch of stuff to catch up on um most mostly good okay i'll reach out to you we'll schedule some time take care uh okay my questions

[00:45:42.04]  Jason ValleryAnswered i don't know if there's anything else that i didn't think to ask otherwise i think i'm good

[00:45:45.83]   RemoteWill you be at Supercomputing? You, Leo, yes. You, Jason?

[00:45:49.83]  Jason ValleryYeah, yeah, I'll be there. We should say goodbye.

[00:45:52.84]   RemoteYeah, we can meet. I mean, let's say we can meet, of course. If there are other questions or a more informal chat, we can have it. Yeah. That would be good. Sounds good. Yeah, we have already some scheduled time with Leo, but yeah, of course. If not, I will come to your booth. are also closing I received the layout yesterday by the way Jason can join the same meeting that is on the calendar with me oh that's perfect no thank you guys for the time was a pleasure to connect and yeah as you know I'm super pressing on your to get at the terraform script as soon as it's ready because I want to play with your solution because, again, I'm a totally honest, mentally honest, and a philosophically honest guy. Of course, Vasta is a technology born in the future as compared to Lustre. You have multiple unfair advantage, so I'm really curious to test that and to get it on Azure. - Awesome, yeah. - Okay, first week of December, that's the plan. if that changes I will let you know. Okay, okay, no problem. But is it due to your development or to L192 availability? Our development. We have made a decision to use the VMs which are available and again it's as good as the hardware we run on. So if we end up running on VMs with this is 40 gig of network, then the performance will be buttoned by that. it's more about just getting something functional out, and UK Met team, the first customer for this software, they said that they don't care. They're just going to plug the numbers into Excel sheets and to figure out what's 40 does, and then they can extrapolate the numbers to understand what 200, 400, 800 can do. So it's really up to our cycles, and based on our VP of engineering, only the first week of December. Yeah, no, I'd say when you have a new VM, yeah, of course there may be challenges on if there is anything outstanding on the network driver, on the NVMe interfaces, but in general it should be small. So it's more about the project timeline to begin with. Our plan was to be generally available on the marketplace in February, and with that, being ready for POC was planned for December. So I've tried to expedite it, but we have so many other tasks on our side that I just couldn't expedite it. So it's not about the new VM. It's just the first time we're going to support BaaS on Azure, right? So it's a lot of work that needs to be done for the first time, and then once we're there, I think supporting other VMs will be an easier task. in supporting the first VM. I know for sure, I mean, and but the other part, and sorry, this is my question tag. It's a there has been a big conversation. You remember in the last meeting when there was also your Israel account team involved about Azure native roster. That is something after, right? Is in the lifter program and so on. OK, yeah, so yeah. That's in the plans, and right now we're talking about Marketplace Q1 of next year, and Lifter is supposed to be generally available, which is again Marketplace at the customer tenant sometimes around Q3 of next year, and we're already working on plans of building our own SaaS service, but that's, again, JSON will know about it more than me. That's still under construction. We're still building it up and finalizing the details. We're not ready yet. I would say the best case scenario it will be ready at the end of next year. - Yeah. - So, yeah. - It would be something like Azure. So, because there are two deployment model usually for this type of things is it would be something like, I don't know, Azure Native Cumulo today, or would it be something more like you have your. and then somebody can say, OK, I want to enroll my subscription and you basically deploying the subscription. So we look like more on as an Azure service or as a platform that can deploy in an Azure tenant.

[00:49:42.88]  Jason ValleryIt's not service would run in and fast owned tenant. We would expose endpoints into the customer.

[00:49:49.51]   RemoteOK, so like not, I think it should be, yeah. Yeah, okay Okay, thank you for your time today, thank you guys have a great weekend. Yeah
```
