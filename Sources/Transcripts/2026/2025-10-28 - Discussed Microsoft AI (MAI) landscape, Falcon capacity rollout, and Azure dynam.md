---
entities:
  people:
  - '[[Alon Horev]]'
type: transcript
source_type: unknown
date: '2025-10-28'
---

# 1:1 â€” Alon â€” 2025-10-28

> [!info] Purpose
> Build trust, surface and remove blockers, align on priorities, coach & grow.

## Summary

Discussed Microsoft AI (MAI) landscape, Falcon capacity rollout, and Azure dynamics to align before Jasonâ€™s Friday meeting with Kushal. Explored VASTâ€™s path via Project Apollo (AKS-led slim control plane) and potential as standard storage in Apollo deployments, plus longer-term hardware qualification in Azure. Agreed Jason will meet Kushal Friday, send notes, and Alon will follow up with Vipin. Strategy: use MAI success as wedge; position VAST as software-first, open to ODM hardware and liquid-cooled options; consider Blob compatibility carefully but focus on GPU-utilizing performance wins now.

## Key facts learned

- Kushal (ex-Inflection, now MAI under Mustafa) reconnected with Jason and set a Friday meeting about using VAST outside Azure data centers.
- MAI Falcon plan: Phoenix, Dallas, Richmond; ~40k GPUs per site connected by AI WAN; initial tranche has ~3 EB Blob.
- MAI currently struggles to use Falcon capacity due to control plane fragility and GPU issues.
- OpenAI GPT-4.5 training (multi-island, ~9 months, up to ~100k H100s) was disappointing, shifting philosophy away from ever-bigger clusters.
- MAI exploring online RL continuous learning with trainers (Phoenix) and generators elsewhere, tight loop (~60s).
- Vipin presented VAST internally; values global namespace, quotas, capacity estimation, QoS; acknowledges Blob canâ€™t match VAST performance.
- Microsoft reportedly closed a large contract with Nscale; end customer not disclosed.
- AKS-led Project Apollo aims for a slim control plane for single-tenant GPU sites (lease power/space) without full Azure region overhead.
- Azure Storage lacks a deployable solution for Apollo-like sites today; AKS is exploring thin VAST control plane/topology.
- Marketplace VM offers (e.g., Lsv4/v5) are not price-performance competitive for VAST at scale.
- Azure Compute leadership is incentivized to keep workloads on Azure first-party; potential internal resistance.
- Hardware qualification path likely runs via Azure Hardware (Ronnie Borker); multi-year effort for first-party SKUs.
- Liquid-cooled storage SKUs could help with DC cooling fungibility and late-binding storage vs GPU rack decisions.
- Blob API is a Microsoft-specific legacy; S3 compatibility remains attractive; multi-protocol head (Blob + S3) could broaden appeal but faces control plane integration hurdles.

## Outcomes

- Jason will meet Kushal on Friday to discuss a potential VAST deployment for MAI outside Azure DCs.
- Jason will send Alon notes post-meeting to inform next steps with Vipin.
- Alon will follow up with Vipin after Friday to align and probe AKS/Apollo angles.
- Maintain close collaboration with Lior and Tiffany to position VAST for Project Apollo.
- Align on a software-first approach, remaining flexible on ODM hardware and liquid-cooled options.

## Decisions

- Wait until Fridayâ€™s Kushal meeting before Alon follows up with Vipin.
- Prioritize Project Apollo as the first entry path over Azure marketplace SKUs.
- Use MAI success as the wedge to influence broader Azure storage strategy and hardware qualification.
- Treat Blob compatibility as exploratory; near-term emphasis remains on performance to keep GPUs utilized.

## Action items (for Alon)

- [x] Meet Kushal (MAI) to discuss VAST deployment options outside Azure DCs @Jason Vallery â« ðŸ“… 2025-10-31 âœ… 2025-10-28
- [x] Send meeting notes to Alon after Kushal call @Jason Vallery ðŸ”¼ ðŸ“… 2025-10-31 âœ… 2025-10-28
- [x] Follow up with Vipin after receiving notes to align on path and any AKS conversations @Alon ðŸ”¼ âœ… 2025-10-28
- [x] Stay close with Lior and Tiffany to advance Apollo storage integration option @Jason Vallery ðŸ”¼ âœ… 2025-10-28
- [x] Map Azure stakeholders (AKS/Apollo, Storage, Compute, Hardware) and their priorities @Jason Vallery ðŸ”¼ âœ… 2025-10-28
- [x] Engage Azure Hardware (Ronnie Borkerâ€™s team) on qualifying a VAST-friendly storage-optimized SKU @Jason Vallery ðŸ”½ âœ… 2025-10-28
- [x] Explore liquid-cooled storage SKU options with ODMs to leverage DC cooling fungibility @Jason Vallery ðŸ”½ âœ… 2025-10-28
- [x] Evaluate Blob compatibility and multi-protocol (Blob + S3) head; test interest with Azure contacts @Jason Vallery ðŸ”½ âœ… 2025-10-28

## Follow-ups

- [x] Clarify with Kushal the target site and hardware profile for VAST (non-Azure DC) @Jason Vallery â« ðŸ“… 2025-10-31 âœ… 2025-10-28
- [x] Verify with Vipin whether he spoke with Anson and Keek (AKS) and gather feedback @Alon ðŸ”¼ âœ… 2025-10-28
- [x] Confirm whether the Microsoftâ€“Nscale contract maps to MAI and any implications for VAST @Jason Vallery ðŸ”½ âœ… 2025-10-28

## Risks

- Azure internal politics and P&L alignment (Compute vs Storage) may resist third-party storage adoption.
- Control plane and infrastructure fragility at MAI could delay showcasing VASTâ€™s value.
- First-party control plane integration limits hinder rapid adoption across Azure managed services.
- Hardware qualification timelines in Azure are long (multi-year), slowing first-party presence.
- Unclear ownership/alignment within MAI (Vipin vs Kushal) could stall decisions.
- Blob APIâ€™s limited strategic appeal across Azure could reduce interest in Blob compatibility work.

## Open questions

- Is the Microsoftâ€“Nscale contract intended for MAI, and can it host a VAST deployment?
- What exact deployment scope is Kushal exploring (site, scale, timelines, hardware)?
- How aligned are Vipin and Kushal on adopting VAST for MAI?
- Will Project Apollo adopt VAST as standard storage for Apollo sites, and what is the pilot path?
- Is pursuing Blob API compatibility strategically valuable to Azure beyond Manishâ€™s org, relative to S3 or multi-protocol?
- What are Azureâ€™s timelines and openness to liquid-cooled storage SKUs?
- Who definitively owns decisions across Azure Compute and Hardware for this path, and how to secure their buy-in?

> Next meeting (if any): 2025-10-31

---

<!-- ai:transcript:start -->

## Transcript (auto)

```text
[00:00:00.00]   Remote(keyboard clicking) (keyboard clicking) You Okay. All right. I'll be back. I'm doing good.

[00:01:34.02]  Jason ValleryI'm, what, a week and a day in and enjoying it every minute, so I'm having a great time meeting folks and trying to add value where I can and hit the ground running, so it's been fun.

[00:01:52.70]   RemoteWho's this Kushal guy you're talking to?

[00:01:57.31]  Jason Vallery>> Yeah. I've got a meeting with him on Friday. I wanted to give you some backstory about him, Vipin, what I know about MAI, and see if we can make sure we have our stories aligned and work together on this. Kushal has been with Microsoft pre and inflection acquisition. I think he's been Microsoft five, six years, came from NVIDIA. After the inflection acquisition, he got moved under Mustafa to be part of MAI, and I first met him in the context of their move on to Azure, and so, you know, the way this kind of unfolded is they were obviously running on CoreWeave in the cluster there with VaST very successfully as inflection, and then once they became part of Microsoft, you know, inside of Microsoft, the culture was really clear. All GPUs went to OpenAI. I was very involved in this world. What was happening is that as new GPU capacity came online, it was almost fully allocated to OpenAI. Nobody else got any. third-party Azure business at this point, and Mustafa shows up and says, "Well, I want a supercomputer." He had a little bit of FOMO, because OpenAI was getting these gigantic supercomputers, and MAI is running on this tiny CoreWeave cluster that they brought along with them from inflection, and so, there were a lot of, I mean, this is all politics at the top level of Microsoft, but a lot of conversations around upcoming data center capacity allocations, and at this moment, this was like December of last year, Microsoft wanted to play a little bit of hardball with OpenAI. My two cents here is that Microsoft AI was always a hedge against OpenAI relationships. turning south because they obviously have all the IP coming out of OpenAI, so they benefited from that as their research frontier. If you actually rewind the clock further, it all started when Sam Altman walked out on OpenAI and left the organization, and there was a whole Board of Directors coup thing. I don't know if you were tracking news back then. a month after that, and that was basically Microsoft saying, "Oh shit, we better have our own first-party strategy for how we build frontier models." So going back to the table, December of last year, Microsoft AI finally got enough momentum inside of Microsoft to convince it to give some GPUs their way. Microsoft is obviously the of a deteriorating relationship with OpenAI, and so Microsoft made this decision to say, "MAI, we're gonna build you a supercomputer." And at this moment in time, I have to also share with you, some of this is keep it between us kind of thing. GPD 4.5 had just finished training, and so everybody was in no shape, right? Because up until that point, training run being finished with GPD 4.5, there was this view that that next iteration and model scaling in terms of parameter count was going to yield a far more capable model, and what I can also share during that time is the the training was painful. It was like a nine-month training run running across multi-islands. and at peak, like 100,000 H100s, and was the outcome of it was very disappointing. Like the whole thing was like arguably a big failure, and this goes into the contract negotiations with Microsoft in December, where suddenly OpenAI is changing their specification for what they want in terms of next generation capacity. Before it was always bigger is better, you know, let's build the biggest supercomputer on the planet, and, you know, then suddenly the paradigm shifted, and like GPT-5, it was trained on 4,000 GPUs. It wasn't trained on the supercomputer, and so, MAI is sitting here trying to come up with, well, what is their frontier model development planner going to look like? What do they want? Falcon was in you know to my current knowledge my current knowledge is stale as of like July. That was last time I was in a conversation around this. >> DIRECTOR DEWOLF: Can I say something real quick.

[00:06:10.78]   RemoteI'm sorry my kid is calling me.

[00:06:12.78]  Jason Vallery>> DIRECTOR DEWOLF: Sure.

[00:06:14.32]   Remote>> DIRECTOR HAMPSON: Thank you.

[00:06:33.28]  Jason ValleryYeah, so Falcon, the Falcon project started around that timeline and what, what the team sort of identified from the Azure perspective is that we could sacrifice. a data center in Phoenix, a data center in Dallas, and a data center in Richmond, Virginia, each one capable of handling about 40,000 GPUs, and that those would be connected together via, Microsoft's got this thing they're calling the AI WAN, but it is petabits of fiber point-to-point between these locations, and so, you know, I got introduced to MAI about that time, and specifically Kushal. Kushal is, or was, the delegate from MAI to represent all of MAI's interests with Azure, and so he was the end-to-end capacity owner, leadership point of contact for anything coming out of MAI. the Azure organization, and so I worked with him closely to plan Falcon, which meant like how much storage was going to go into these three sites, construction timelines, you know, what that meant in terms of requirements. Many meetings with him and then he brought in Vipin a few times. So I've met Gushal many, many times. I met Vipin, I think, two or three times. three times, to kind of just talk about MAI's experience on fast core weave, pain points that they see in Azure, where it's going to work, where it's not going to work. There was a lot, like what I would just describe is, MAI were clearly being drug on to Azure kicking and screaming because they were very happy with the experience they had, and then they've got me showing up talking about storage where you know to get the kinds of throughput they wanted you're literally deploying exabytes and exabytes of blob racks like, you know, and I'm stealing all their megawatts to go and Put those racks meaning that the GPU counts were dropping You know, there was a lot of conversations at that point about like model architecture and what the GPT-4 five learnings meant, and how I might change their training runs. One of the things that they were heavily looking at is a model of online RL continuous learning where what they had described, and frankly OpenAI has done this and I suspect XAI is doing this, but where they have training. and generators running in a reinforcement learning feedback loop where the trainers are running at like full fp16 precision and then those distill down real-time at each checkpoint till I guess before then those deploy on to the generators and then the generators run test time compute and then the output of that feeds back into the training loop and they're talking about doing that like every 60 seconds, a full loop of that, and so the idea being that like the trainers would all run in Phoenix, and then all of these other locations where I mean, I was going to get capacity would primarily be generators was kind of the plan they had at that point. You know what I can tell you, and this is actually my my post Microsoft knowledge, but I still know some of the books over there is that the first tranche of Falcon capacity did come online with 3 exabytes of blob, but they're really struggling to use it because of a whole bunch of control plane issues, fragility in the infrastructure, they haven't even really been able to fully exercise the blob capacity because of the GPU issues they're facing. So independently, Prushal and I became good friends over that time and he pinged me kind of out of the blue a couple of days ago based on my LinkedIn post saying, "Hey, I'm joining VAST." He congratulated me on the role. We started a conversation, and, you know, what he shared with me yesterday is they're very seriously looking at VAST I think there's a whole bunch of conversations we should have around what hardware in Azure looks like and how we can approach that long-term, but what he said is that this would not be in an Azure data center, whatever that's worth, and he's like, "Hint, hint," if you know what I mean, and he's scheduled, he sent me an invite to meet on Friday to talk about this. So I've got a meeting with Kushal separate Friday. That's up to speed everything I know about me in a nutshell What do you know

[00:11:00.93]   RemoteWe know that Microsoft close a big contract with N scale, right Yeah, so And skill doesn't know who the end customer is whether it's an AI or for open AI or whatever whatever it is. So I think that could help connect some of those dots. When I spoke to Vipin, he wasn't aware of like a concrete plan to bring VAST on board. I would expect him to know, right? He did say that folks have been asking about us, have been curious about us, that he's... team has showed VAS to folks in Microsoft.

[00:11:41.94]  Jason Vallery- I was in the meeting with Vipin where he presented VAS, yeah, exactly.

[00:11:47.40]   Remote- Yeah, he said that he is not seeing a way for Blob to be competitive with what they're getting. So he acknowledged that, but I was kind of... disappointed to hear like no confirmation from him that there is like a real thing being looked at unless Vipin told Kushal like, you know, go do it. Like, I would be happy to get that working or maybe Kushal is looking at what's happening with the Azure stack and thinking to drive this himself. So Vipin said, you know, - Nathan said, first of all, I was like transparent and honest with the team. I said that we also had a bit of a bumpy road initially with you guys. We had like, we had two outages with Condor. That's the code name with Microsoft AI in the early days.

[00:12:40.25]  Jason Vallery- Still is, they still refer to it as the Condor cluster.

[00:12:43.62]   Remote- Yeah, and, but that was a pretty long time ago, and, you know, through that process, we also got closer because Vipin was curious and he wanted to understand what's going on and how things work. So he told me he was honest with the team, but the trend is definitely good. He definitely likes, you know, using VAST. He really appreciates all of the extra functionality that we bring to the table. global namespace that he even started even using he already sees the potential value you know user quotas capacity estimations like a lot of features that he's using QoS he said that you know those are like valuable features for him um and um and he said that he's going to talk to Anson and Keek um that he's like he asked who did I meet so I gave him the names, so they're on the AKS team, let me go talk to them to understand where

[00:13:35.51]  Jason ValleryThis is coming from and see how it can help. So I can give some context on that that isn't MAI specific. I talked to Lior a bit about this, he told me this was coming. The AKS team has, I think I shared with you some of the project context, but just, you know, the high level is that when you look at these facilities like Enscale, CoralWeave, Nebvius, etc., Ad Nauseam, those different providers, Microsoft is trying to treat as just least space and power. But then the way that gets deployed is like single customer, single tenant. It's not an Azure deployment because it's not designed to be multi-tenant. Deploying Azure is incredibly taxing just in terms of the resource, the amount of racks, the amount of maintenance, the services, it's just awful, and all of that needs to be in place in order for it to be considered Azure and have all the Azure control plane goodness and to have Azure storage deployed and to have Azure networking deployed, and so there's this thing inside of Microsoft called. Project Apollo, which is how do they build a new Azure that is a much simpler control stack, control plane, and topology, and that is primarily being driven out of the AKS team, and the reason is because the idea is that, you know, the control plane they really need is a very slimmed down Kubernetes interface, right? Like you really just want to say kubectl deploy my training infrastructure to the those thousand nodes over there and be done with it, right? You don't need a heavyweight control plane if it's a very small set of use cases, and so the AKS team generally owns Project Apollo. Microsoft, or sorry, Azure Storage, like doesn't have a solution here today. Like you can't deploy Azure Storage without all of that other. It comes along with it being an Azure region, just key management, log management, services, just so many things, and so, what I suspect is that, you know, connecting some tea leaves here, the AKS team wants to know how we could bring in a thin, vast control plane, vast topology into Project Apollo, and ideally, and this would be my big win and I would love to see this happen become the standard storage

[00:15:48.60]   RemoteDeployment model for Apollo data centers understood okay so I would you know I would stay close with the Leo and Tiffany help them through your connections and whenever you feel that I can be helpful and pulled into conversations with with Azure to to explain and sell what it is that we've built on the technical side happy to do that. I think it makes sense that I follow up with Vipin sometimes by like towards the end of the week to ask him if he got to speak to people.

[00:16:30.56]  Jason ValleryMaybe wait till Friday. I'll send you notes. I'm meeting with Kushal I think noon or something on Friday and I'll send you notes from that and you'll have that context before you reach out to Phippen.

[00:16:38.62]   RemoteYeah, that would be good. That would definitely be good. Yeah, sounds like a great opportunity.

[00:16:47.29]  Jason ValleryYeah, I think it's exciting. I mean for me just up leveling at Beyond MAI. it's great to go get a exabyte or whatever we can get from in the eye. For me it's the arrow in the quiver to Microsoft at a higher level on why they need a better storage strategy than they have today and hopefully it can be something that will help us unlock getting our hardware qualified or an equivalent kind of hardware qualified to go. into the Azure mainstream fleet. I think that's the only, like, honestly, we're not going to be successful with the marketplace offers here. It doesn't matter if it's LSV4 or V5, it just isn't price performance competitive. The opportunity is to use this to motivate Microsoft to qualify a rack that we can specify, you know, for additional organization. I don't know how much you played in this space, but this guy, EGOL, that Lior is talking to, first of all, he's our enemy in this, right, because he owns Azure Compute, and so he doesn't want any of this to go away from Azure because it's not his business unit, and he doesn't want us to have a storage-optimized SKU because it wouldn't line up to the compute business in there at the end. his P&L. So there's a woman named Ronnie Borker who runs Azure hardware, and what I would ultimately see as our strategy would be to take, you know, whatever we can specify from OEM, ODM kind of perspective, and drive that through Ronnie's team to get qualified in a way that they can then make that a standard hardware SKU they deploy, and then we can and that's kind of like what NetApp has to a certain extent today, but they're really the only option outside of the traditional active storage hardware SKUs that go out. Additional point I'd make and actually be curious on your take on this, because I know nothing about this space, but another constraint Microsoft has that we should play to our advantage. is fungibility of data center cooling. I saw some press from Soledigm that they've got liquid cooled flash now. One of the things that I was pushed a lot on is that Microsoft right now is making a bunch of design commitments on data centers that they're going to construct, and when you think about that, like these are multi-year projects and the design of the data center, you know, effectively needing to specify percent air-cooled, percent liquid-cooled, of these facilities now, and when you look at the hardware going into these data centers, it's GPUs that are liquid-cooled, or it's storage that's air-cooled, and some amount of networking kit, and so, one of the things that the data center team at Microsoft was pushing hard on the team was give us a liquid-cooled SKU and Microsoft like store Azure storage was like making no inroads there like they don't like that isn't even on their plan and I could tell you I mean they're gonna be forced to change timelines but historically speaking from inception of a project to design a new storage SKU from a hardware footprint perspective to, you know, first. deployments in Azure is like a five-year project. Like the amount of effort that they put into qualifying the hardware, validating the software stack on the hardware, the firmware, the chain of trust, the like, you know, it's ridiculous, and so, you know, today the most dense hard drives are like 30 terabytes and the most dense SSDs, I think are like 16 or 18 terabytes. like those were projects that were put in flight a few years ago finally now making it out into production scale you know that's what they call it growth so even if Microsoft kicks off a liquid cool design project right now I think it's you know three years before they have their first liquid cooled storage racks in the data another area we could potentially play to our advantage is bringing fungibility to Microsoft of it's all liquid pool data centers and you can late bind the decision of how many storage racks you're going to deploy because we can slot into the same rack positions that the GPUs go into.

[00:20:47.45]   RemoteI'll tell you what, we don't give a shit about hardware, we give a shit about the outcome right in terms of did you have the right amount of CPU memory and flash to build a system that is cost-competitive right we don't care about the details and it's one of our strengths I mean when it comes down to to I mean if I'm if I'm putting my in the shoes of folks, different folks in Azure, I would feel that I may bring VAST in because Microsoft AI needs to be successful and they're telling us this is what they need, right? But if we ever are going to have Microsoft want to position VAST, it needs to be integrated on the network layer, on the observability, hardware. like you need to have alignment on everything. Is it possible for us to sit down at the table and explore options that were, you know, maybe there's an ODM that they work with and we don't, that can build a storage SKU, that can somehow you know come out faster. Because, you know, like qualifying something arbitrary like the AIC boxes that we have, that's arbitrary. and I don't think they support liquid cooling too. But I would say that positioning VAST as a software project that we can go over different options and explore the best route to production, be it some specific VMs or some skew that we can come up together with an ODM, that's great. My assumption is that for Azure to be proud of that, it needs to show, like the product needs to be integrated. It needs to be very consistent with the experience. So while I feel that we have a lot of, you know, a lot of work that we need to do in the ONSYS team in terms of control plane integration, party offering that actually looks really good I think it's gonna take time for people to to not feel that blob I mean a it sounds like blob is not getting there anytime soon in terms like of like a low latency high throughput that high density option right so I would also assume it's gonna take time for for people to feel comfortable and it would basically require Microsoft AI to be successful with VAST inside of Azure, right, to have other people feeling comfortable in terms of, you know, supporting a first-party service that is mass, you know, massive unstructured data for AI because right now Blob is what's positioned for that. You know, there's also managed luster, but obviously, I mean, there's trade-offs. It's like those two different products are like, they have like, one of them is good at one thing and not the other and vice versa, right? So, like you and I, we know the potential, but I'm just thinking that the path of getting there requires putting something on the table. floor solving somebody's pain right now and maybe that's Microsoft AI I hope so and then we need to navigate that together through people like Eagle or the head of hardware and we need to be open for all options because I don't know how we're gonna get there I don't know exactly and I can tell you that you know maybe maybe even if there is like a lot of like less savings on eagles VMs because there's like a waste of memory and CPU but the fact everything is integrated and already available everywhere maybe maybe maybe it will become the path of least resistance I don't know I think like I think we need to have to keep our our minds open I don't know what's going to be the path for for or full-on support from Azure as a service, right?

[00:24:50.29]  Jason Vallery- Well, I think this Apollo project will be what is our first opportunity, and then we actually aren't even dealing with Azure. Like, it is new Azure to speak, and then I think that opens up a world of possibilities. So that puts us in the room for a different set of constraints and topologies that we can leverage to our advantage, I think. - Yeah, well, I was reconnecting. Lots of other things going on. I wanna make sure we keep building the relationship. Looking forward to working more with you. Anything I can do for you?

[00:25:20.02]   Remote- I think, I think building that map different people in Azure, like who are there, like, you know, feel that you're part of the accounting because you have the connections there, right? So, understanding who's calling the shots and what they care about, you know, even one question, one example question, like, can we create something that's blob compatible, even to the extent that we could be like an implementation detail within azure for like you know a hyper blob like a fast blob because we're compatible maybe it's like you know the aws like a single az or or directory buckets or whatever it is that they call it maybe that's another path right for for for azure to not feel that they're you know like acknowledging they lost a war or or putting like a third party in front of the customer and losing the ability to lock them in. You know, they haven't done it with NetApp, but I feel that if somebody wants to store an exabyte of data for training, it's a larger kind of, you know, piece of like weight. So if it's easy to move that elsewhere because you have the same platform elsewhere, it's not the same as like, again, marrying into the blob protocol that creates thickness. So that's another thing that, you know, we're offering that to OpenAI, but we could also offer that to Azure.

[00:26:54.71]  Jason Vallery- You know, a couple of thoughts about that. The blob API, much to my... disappointment remains a Microsoft exclusive API service. Not because Microsoft didn't want anybody else to implement it, because nobody else thought it was valuable enough to implement. You know, over my 13 years, I bet I wrote at least half a dozen strategy papers on why Microsoft needed to abandon the Blob API and go all in on the S3 API, and I can tell you like that, that has been something Microsoft has looked at many times and has been shot down by leadership, many times, but I would say that like that cultures probably changed a bit, where, you know, honestly had AI not shown up and become the, you know, most important priority for the team, you know, Microsoft probably would have eventually, not eventually, They will, I'm sure, at some point, but they would have implemented an S3 API and they probably still will. Because it actually was a huge, like the blob API itself is a huge drag on the business. The number of customers that wanted to come and bring their workload to Azure, but then there wasn't a compatible storage API for them to use, and whatever application they were bringing from the open source world, or that they had developed, whatever, just wasn't ever built to support the Blob API. So an interesting play, honestly, is having a multi-protocol head that supports both Blob and S3 and going after the bulk storage market. But there's a bunch of ways you could tackle that as a SaaS in Azure that supports both S3 and Blob that I think would be interesting and could win an entirely different class of customer the vast has never tried to tap, and you know if I actually look at where the exabytes sit, it's these scenarios. You know the the user generated content. This is mobile device uploads, this is video surveillance, this is you know TikToks and honestly in the opening islands, this is everything you upload or the model generates, the inputs and outputs of the model, it's bulk user data, and that, that is an opportunity we could go after, and I think there's a way that when you layer in global namespace to support capacity management and potentially DR strategies, where we were like a ratio coding across regions and creating it as a multi-region bucket, like Google. cloud has that we could actually win a really good chunk of exabytes and even do it in a way where we're abstracting it over Microsoft's blob hardware, and I think you could even do it at a price point that is attractive but, you know, that would require engineering on our side and I don't know if that's just a distraction.

[00:29:38.19]   RemoteYeah, I think performance is something that, I think this is a second tier problem. Like some of the things you mentioned are like, given we have the superiority of performance right now, and we can keep GPUs utilized, I think this is massive, and we need to continue harping on that. At some point, somebody says, hey, but this is cost-prohibitive for like 40 exabytes, then we can say, okay, let's talk about that. But let's start with like the three exabytes that actually require the performance because that's what's going to get us to foot in the door.

[00:30:12.75]  Jason ValleryYeah, I agree. So in the Bob API, specifically, where it plays value beyond, maybe OpenAI or a handful of Azure customers, not many Azure customers. is integration with first party services. So Foundry, but like, you know, Azure Search, OneLake, whatever. There's dozens and dozens of Azure higher level services, managed services that just intrinsically only support the Blob API for data is you know the core data storage platform for Microsoft, and you know and

[00:30:50.92]   Remoteso this is another reason why we should float this idea with them right where we're not rushing to say hey we're going to implement Blob but we could say hey you know inflection loves us and they're using file but if we were to to be blob compatible how would we accelerate other workloads inside of Azure how would we accelerate workloads that are potentially can come in from

[00:31:16.81]  Jason ValleryOther clouds because they need fast object yeah so the the constraint we'll run into the uphill battle is we can go ahead a blob API we could even build a SaaS offering but like the control plane for all of those managed services is just deeply embedded into the Azure Storage Resource Provider, right? So, you know, it's not like the customer goes in and says, "Here's my FQDN for my Blob Storage account." They go in and they choose from a pick list, like which Blob Storage account to put it in, and, you know, you have to break through that core paradigm of how all of those services integrate with Blob. blob in order to become a option for customers to choose, and, you know, I don't, I don't have a strategy on how we craft that.

[00:32:01.98]   Remote- Isn't this what the first party service called?

[00:32:10.62]  Jason Vallery- Isn't what?

[00:32:12.27]   Remote- Isn't that what being a first party service mean?

[00:32:15.70]  Jason ValleryExactly, right? I mean, you don't give a choice. If you want to store data, it's going in the blob, and that's how all the first-party Microsoft services are architected today. So you could have a blob API, but you're not going to have the experience in the portal to go and choose, "Oh, my storage account is actually managed by VaST over here, and here's the endpoint." First of all, it's on a first-service basis. and second, there's like an institutional problem with like even convincing them to go do that.

[00:32:41.90]   RemoteIf we are, if you look at the NetApp model, does NetApp have that kind of experience that you

[00:32:49.37]  Jason ValleryWould say that's fully integrated or not? I mean, no, they're not. I mean, NetApp isn't providing an object layer at scale for any of the first parties.

[00:32:57.76]   RemoteNo, but I'm asking, I'm asking about the control plane integration. and does it feel natural like any other native service?

[00:33:04.13]  Jason Vallery- Yeah, we can get through that. We can solve that. NatApp has their own resource provider, but that's like Lifter. That's the RPADS model where you can go and build your own resource providers at third party. But all that enables is like more native connections to the network, for example. The experience of managing your resources in the portal. But it doesn't like allow that plumbing and glue in the way that true first party services are pretty wired

[00:33:28.99]   RemoteYeah, okay Okay understood Yeah, I don't I don't know how to solve everything. I know that we need to stick on to to vipin and his team um, and then we need to start having conversations leveraging that with various people to see how we can take over other workloads, and I just felt that the block compatibility thing is a way for Azure to want to invest because we're going to be adopting their standard, investing into their thing, not something that's going to be easy to replicate for others. So I think that's going to help conversations.

[00:34:12.26]  Jason Vallery- I think that the Blob API may not be as interesting to Microsoft as you might think it is. I think it's really interesting to Manish, but everyone outside of Manish's org is like, why the fuck do we have this old legacy API? Why can't we support S3? So, you know, I appreciate that sentiment. been the guy that literally owned the BLOB API for many years. I think we have a number of different irons in the fire across Microsoft that will unlock our opportunity. So I'm optimistic we'll find the right thing, be it MAI or UKMED or one of the others that are brewing. So I think we can get there. Yeah, it's gonna be fun. Okay. - Awesome.

[00:34:53.96]   Remote- Okay, sounds good. So let's talk and let's catch up on Friday after our meeting.

[00:34:58.17]  Jason Vallery- Yeah, sounds good. Thanks a lot.

[00:35:00.44]   Remote- Bye. - Bye bye.
```

<!-- ai:transcript:end -->
