Thank you. Thank you. Nancy, hello sir. Doing well. How about you? How are you doing? I hadn't heard. It's over. You're allowed to do what I need. So what does that mean exactly? So the restraining order isn't in place, Hey Jason, how are you buddy? I'm doing good. I'm doing really good. Nice to be out of the temporary restraining order. So now I can take the meetings. them to extend it. but there's still a case going forward or? What are the consequences for that? Like, how does that play out? Yeah. Well, good. Yeah, but they have dropped basically, like they're only focusing on the solicitation part. And I really don't care about that at all. Uh, according to my contract, nothing. So, but I have a one-on-one with Ong tomorrow and my niece two hours later that they shot me an email saying like, hey, can we have a one-on-one? Are you able to take one-on-ones meeting now? And I was like, yep, I can do that now. And then just I saw an invite from the at 2 o'clock my time tomorrow and then at 3.30 with Manish. Well, I'm sure they'll be pissed off. You know what, Jason? I absolutely love this. I love the work that you have done because this has actually brought me into the table where I can go into the meeting and say, yeah, I mean, there is a different view to it. and say like, hey, why haven't we explored that we can actually do like throw a hell of a lot of engineering hours into integrating with Azure Blob, which we need to do anyways, going forward and say like, hey, let's build a better together story. I mean, we wouldn't be in that position without you and Leo pushing the right buttons. So from a strategic point of view, I feel like this is absolutely brilliant. Yeah. Yeah. Yeah. I mean, there's probably a lot we should go through and make sure you're briefed before you meet with those guys. Yeah. Let me kind of debrief on my thoughts from like the wrap up from last week and what potentially they're going to be pushing on. So obviously, this is rooted in the MAI situation. The MAI situation is multivariant and complicated because they MAI prefer VAST. Yeah. I'm sure you have some of those contacts, but just for sake of completeness. Obviously, they've been using VAST prior to their acquisition of Microsoft. And so we have a good relationship with those guys. You know, they reached out to me after I left Microsoft. So there's a key stakeholder at MAI. His name's Kushaldada. He reached out to me, and I worked with him in my previous role at Microsoft quite a lot. He's the guy that owns the relationship between Microsoft AI and Azure. Yeah. I guess. Two very different organizations internally. And one of the things that happened after that acquisition is they were drug kicking and screaming on Azure. If you haven't heard this before, the things that is a core foundational tenant of Microsoft from way back in the Bill Gates era is we eat our own dog food. Like the idea being that Microsoft is built on top of Microsoft. And when Inflection joined, that was one of the Yeah. Yeah. The first priority was to integrate them into Azure. They got their first supercomputer in Azure last July. And it was a mess. Like, they had a ton of problems up and down the stack, not just storage. Storage was a problem. Kubernetes is a problem. Control plane is a problem. Nodes in service, reliability, networking, problems across the stack. And it triggered an escalation by the guy that runs Microsoft AI, Mustafa Soliman. Yeah. He's a direct report of SELIA to say, "We can't be on an Azure solution." And it caused a whole bunch of upper across Azure. And it kicked off this project Apollo to be go create a net new Azure. And, you know, that was triggered within Manisha's org, a review of VAST. And so a guy who used to report to me before I switched in my previous role, he got actually tasked Yep. Mm-hmm. Yep. with doing a deep dive analysis of the capabilities of vast. Logging, monitoring, Mhmm manageability, performance scale across the stack. Like why does MAI love vast so much was essentially the question. He presented that all up to Manish picked off this new project that he's got going called Bifrost, which is I, I had coffee last week with the guy who's managing the Bifrost team and he called it lipstick on a pig, but it is just taking a bunch of like he's got like 30 something devs working on this thing to go and solve various problems across the stack that improves performance they're talking about like a new client driver a bunch of stuff and ultimately manish went back to MAI and has pitched Bifrost as their holy grail solution to solve all of their problems so that's kind of the politics meanwhile Kushal knowing he was getting this big supercomputer in Nebias and knowing that it's not in Azure Okay. Knowing that it's not in Azure, saw the opportunity to make that VAST. And so Kushal, when I left and he knew I came over here, reached out to me and said, what would this look like being on VAST? So that's picked off an internal set of conversations where I went and talked to the HPC AI team, which is not storage. This is Nitti's former team. And they gave me all of the internal specs of Azure Storage and frankly, like of the Bifrost solution. Okay. And frankly, they shouldn't have. That team knew me and knew that historically this would have been information I would have been giving them. And then they gave me Manisha's new specs. And so in this world, and I was very careful because one of the things Jeff has been super nervous about is that I'm going to end up in the same situation you're in where Microsoft sues Vance because of internal information. I was very careful in the context of the email I put together to say, this came to me in my new role post my hiring here. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. specific person in a nitty's organization that guy almost got fired his name is bolol because leor brought it up in the meeting with garish and i put this powerpoint together which i don't know if you've seen but it has like a comparison people a vast versus the vifrost solution it goes back to microsoft and that's what happens manish hopping mad at me because i've called his child ugly and i knew all internal people to give me all the internal details and now i Yeah. who is already pissed off at Azure, has this huge gun pointed at Azure and saying, your solution sucks. The one that you're promising to fix all the problems, look at it. Here's the numbers. In comparison, it's just a completely waste of power and space. Ultimately, that's why they're pissed because of that happening. Now, in terms of last week, I just had random meetings with lots of people in Manish's org because they are friends of mine and they wanted to get coffee. Yeah. Yeah. was in town. Nothing I did there was aggressive. It was just catching up and saying hi. But, you know, honestly, Vamsi was a little pissed when we met with him because we didn't, you know, give him clarity of why we were even in Redmond last week and what our agenda was and who we were meeting with. And we met with Vamsi on like Monday, like right after we arrived. And I said, you know, we'll take you with us to all of our meetings if you'd like. He was, you know, whatever. I've known Vamsi a very long time. So that's kind of the way. Yeah. Yeah. Yeah. Yeah, but I mean... They're pissed off because I got internal information and handed it over to AMAI and called their baby up. Well, I mean, ultimately, what I think is really important for me to correct is that this actually came to you as a request in your current role. You know, so, and you did not ask for any of this, and you were basically just doing a very simple comparison. And this has actually resulted in my opinion. in a very good opportunity for us to actually fix that, make them understand how we can be, how one plus one equals fucking a billion. Us together, where we can be doing the training and having every single dataset that goes through fine-tuning or training. Inferencing is maybe a little gray on the in the gray zone that can actually be within Blob. But ultimately, what we want to be able to do is have that global namespace to Azure, to Nebius, to Nscale, whoever NPC that they contract or are bringing to the table, that we tie in everything Azure Blob does. So ultimately, we would not have the object storage on us. Yeah. we would be the high-performant filer. I know it reduces probably the overall size of the deal, but ultimately then we have what we are known best for, having a high-performance and scalable file system. And the database and everything that we can actually introduce, the contextualizing the data on the fly, vectorizing it, or whatever we like to Yeah. to sort of pull together. I just... Okay. Okay. Why is your camera always going left and right? Yeah. Okay. Yeah. Here's how I would play this. And this is the way you're going to think about it as well, because it's how OpenAI operates and how MAI operates and where I actually see the opportunity in front of us. It has like an autofocus thing, like it frames me and I move. Anyway. OpenAI has central data lakes. and GPU adjacent storage. Like those are the two workloads we're talking about at scale for AI. At least on the training and data pipelines perspective. And the data lakes are spark, data breaks, custom apps that span tens of exabytes. And I actually think this is going to be kind of the common pattern is that what will inevitably happen is where the data ingestion, the data pre-processing, the normalization, feature extraction, all those traditional analytics capabilities Okay. Mm-hmm. sit inside of open source spark or azure databricks running in big hero azure regions powered by cpu cores which is important and then there's this global disaggregated fleet of gpus that are either in azure or they're in neoclouds or they're on-prem and you get a certain amount of gpu adjacent storage for staging checkpoints staging training data our opportunity is the gpu adjacent storage Yeah. Mm-hmm. Mm-hmm. and Blob's opportunity is the central data lakes that are tens of exabytes in scale. That's, I think, a good way for us to think about carving this up because in the GPU-adjacent storage, you need power and space efficiency, you need high performance, you need GPU-direct storage, all things that vast is really well. And in the data lakes, it's about really cost per exabyte and that's something that Blob can do really well, although I can argue in a different world that we should focus on that. So what I would be careful about then in that lens is not bringing Yep. Yeah. Yeah. Mm-hmm. Yeah. Yeah. Mm-hmm. database into the conversation, not bringing higher level pieces into the conversation, because really those are the play they want to do in the big Azure regions. And what we narrowly asking for their support on is enabling their problem because they have a real problem when they go into these like nebius facilities and NCPs and so forth, is that it's really hard for them to deploy blob and it's very power and efficient. It's very network inefficient because there's a bunch of networking problems when it's not an Azure region that they deal with. And so I think that Yeah. Yeah. That's the win-win partnership is if we agree to deep integration where we can take data in and out of central Azure regions where there's blob capacity and enable the cloud deployments with a small footprint of vast deployed GPU adjacent. That should be the timelines that we want to carve out with them. And they haven't necessarily seen that light. Actually, I had this conversation with BombChain. He wasn't like wildly opposed to the idea. So I don't know if they think about it that way, but I think that's the way we want to carve out. Yeah. I think that is the way you could make it a win-win. I don't know. One of the things I've seen this week is the deep integration with Blob in terms of a write-up. So, you know, I had been talking about this with the folks in Tel Aviv and Shahar and folks about, like, how we would integrate with Blob. And there's multiple ways in terms of the API surface and how we do offload Blob and what the right different touch points are. It felt like up until now, the question was, well, the chicken and egg problem of we want a PO from Microsoft that's I agree. Yeah. that says we must do these things before we actually go to implementation. You know, I think we should just pull the trigger and get going on the project and see what happens. Because I think it'll also help tell a story around our commitment to Azure. It'll tell a story around when we get to some decisions with either open AI or MAI, we've already got what they need ready to go. So I've started that document and I'm going to review with Jeff on Friday in terms of what I see in terms of how a deep integration would look. I could not agree more. And it's kind of funny that just looking at what I wrote in a reply to him. I literally wrote to Ong saying, well, like the better together story from my perspective is that we become the GPU direct storage and have a deeply coupled integration with Azure blob. And Azure blob actually becomes, and I actually went a little bit further and I say, we could even have a global scheduler that is actually measuring or validating where you have the GPU, at any given time, whether it's in Azure or in N-Scale or in Nebius. If Polaris is working on that, like has all of that, then it can literally start doing that global scheduling for you and measuring, you know, the total available GPUs in any given region and in any given NPC that they choose to collaborate with. Thank you. Yeah. The data gravity issue is either caching specific datasets to those availability zones, which I would say like they're extending the availability zones of Azure into the NPCs from a GPU workload or sort of model builder perspective. So that's exactly, I mean, I think we are completely aligned. And like I told Renan in my last night's meeting, I couldn't dream of a better scenario than being in this position where you guys have laid the groundwork. And landing big deals, you have to ruffle some feathers. And now we can actually go back to them and say, hey, let's work together and creating a kick-ass solution that everybody wins. Yeah. I mean, hopefully you'll get Manish to see the light. I doubt you will. I mean, I know Manish very well. You know, I mean, I don't know how much you've interacted with Manish so far in your career, either at NetApp or here, but he's actually probably more reasonable than Ong, Yeah. But he doesn't show his place very well. Ong is just, Ong is very difficult to work with. And he's going to have, actually my biggest challenge with Ong is he doesn't actually even understand storage. I don't know how to say this, but be blunt about it. He's actually not that technical. He doesn't understand how the platforms work. He doesn't understand the workloads or the scenarios. He really doesn't get it. And so because he doesn't get it, he just kind of follows like, No, that's... Yeah. No. Ground Principles around how you know Microsoft first the dog food mantra that I highlighted her he isn't going to get the subtle nuance around why this makes sense it'll be completely lost to on and so in that world you're you're gonna struggle to really even get traction with him Manish on the other hand gets it he's very technical he was very hands-on he gets the workloads scenarios and why this stuff makes sense but you won't really get a clean read from him and he's he's similarly I Yeah. Mm-hmm. Yeah. I don't think he's got a lot of eggs in the bifrost basket at the moment and he's not going to want to see that fail so ultimately what I actually think happens here is that it turns into a bake-off and MAI are already forcing this the timeline for me is uncertain but what I expect to happen is we'll have a heads-up competition of performance scale features whatever against whatever Manish can field in timeline MAI can negotiate Yeah. Yeah. and that will ultimately be a decider at least insofar as what MAI gets for their capacity. How that leads into a broader story beyond just MAI, I don't know, because it's not going to become sort of a standard playbook at that point. It will be, you know, what I was told very clearly by many people is that is Mustafa's decision. It's not Azure's decision. Yeah, but I mean, I know that Satya is also going to play mediator between Scott and Mustafa. Mmhm He always does that. I mean, I've seen it multiple times before where like it is to your point, this dog food or not invented here mentality in Azure is pretty dominant. Like they believe that they can build everything. But I mean, I think they have literally showcased that one thing that they are not good at is building file systems or storage. You know, so ultimately, you know, I mean, take as an example, they could not get SAP HANA. They couldn't even go through the validation or the certification process because of the latency and everything and the performance and the cross-region replication capabilities that they were simply missing in order for SAP to go all in with Azure. So that sort of forced their hand in, doing NetApp. But, you know, I mean, ultimately, or on top, but I mean, ultimately, we need to be able to paint a picture where the bulk of the data actually resides on Azure Blob. But we take care of all or everything that needs performance and everything that needs scalability, you know, scale out high performance file systems. And one thing that Jeff told me when he was in Iceland, and he said that, You guys have now a slide with the power consumption difference between blob and utilizing vast. Yeah. Okay. Yeah. Yeah. No, but I mean, like I said, I... Yeah, this is the slide that I got the details from the Belal guy that probably, I heard he almost got fired over sharing that information with me. But this is the slide that went back to Microsoft AI that really ruffled Manish's feathers that this slide exists. I like the scenario that we have right now. Because I've seen it before. And I saw it with SAP. Not at the same scale, of course, but ultimately SAP is quite big. But we had to go through a lot of it. Lots of back and forward, especially in regards to, well, we can do cross-region replication in six months. to SAP. And then that stalled everything. And then 12 months later, they didn't have procedure application. So SAP comes back and says, we need this. Otherwise, we're going to just go with AWS. And then that triggered Scott Guthrie to actually just make a decision. Yeah. You know, go without monies. Yeah, I mean, there's a bunch of problems with Azure storage architecturally, you know, to get in the weeds on it. I mean, it still all runs on top of Windows Server. You know, and the way, I mean, I have detailed knowledge of how block storage works, and it's all, they tried to shove a whole bunch of capability into one system that wasn't really purpose-built for anything. And it, you know, just has a bunch of technical debt and legacy crap sitting in there, and it's why they run into so many Yeah. and scalability problems across the stack. The way we even got to support OpenAI was frankly just shoestring and duct tape putting a bunch of clusters together and overbuilding storage capacity. You know, the inefficiency in terms of number of racks to get to the kinds of throughput numbers that OpenAI and now Microsoft AI needed are laughable. And so when you think about it, like you're wasting all of that space and power in the data set. Even if you can justify the capex of the hardware, you know, because it's such a small percentage of the total bill of materials of a supercomputer when it's all GPUs and it's a multi-billion dollar asset that you're building, you know, the storage, oh, it's an extra couple hundred million in storage racks, that's fine. But the wasted space and power and opportunity cost associated with that is the big gotcha of inefficiency. And so that is what that slide really tells and that's why that's so painful for them. Because when you look at these neoclouds, I mean, we just heard about this at InScale, they don't have space for all of the racks that Microsoft needs to deploy to provide storage solution. So, you know, being able to take, and I'll send you the slides so you've got it before you meet with them as soon as you're off the call, but, you know, it's like one-seventh the number of racks and one-fourth the number of megawatts or something right about those orders of magnitude to deliver the same throughput and capacity as the blob solution. Yeah. Yeah. Yeah. That's crazy. that doesn't even exist yet, mind you. This is hardware that they're planning on shipping in March. They've never deployed this hardware at scale yet. Our existing solution that we could deploy today has that kind of performance benefit or space power opportunity benefit over what Blob is delivering today. Yeah. I mean, that's amazing work, Jason, to have that already. I mean, from my perspective, yeah, I mean, I mean, when somebody calls your baby ugly, it's always difficult. But sometimes your baby is ugly. And, you know, so I think this is a scenario that we could have barely dressed up, in my opinion. You know, but ultimately, like, and I have gotten this from, do you know Hjalti, who used to be head of Hjalti, the head of Cortev, Yeah. Yeah. Microsoft for years and years and years retired last year. H-J-A-L-T-I. Okay. So we've been close friends for many, many years and he's always worked for Microsoft. He just basically retired and is just doing like board level stuff and different stuff. It doesn't sound familiar. How do you spell the name? What was it? Uh-huh. Don't know. He said that there is a lot of talk because he still lives in Seattle or he lives in Kirkland. There's so many people talking about VAS these days in Microsoft every time I go wherever. He was talking about very large projects that he was not allowed to talk about. I said, I'm pretty sure I know which ones you're talking about. but that's okay. Let's keep the confidentiality. He said if this goes as the underlying storage infrastructure for Apollo and for how they're building out to the NPCs this is easily 10 to 15 billion dollars of software licenses to VAST every year. I'm talking about in the future. I don't know how accurate that is, but that's what he told. And he said, there's no way Asher is going to do that without getting some level of exclusivity, most favored nations, or simply acquiring VAST. Yeah. Yeah. I mean, that's my take. And obviously, this will be a however Renan feels when he gets into these negotiations, but it is a either they buy us, or it is, you know, core weave style deal. I can't imagine 10 to 15 billion a year, that seems a lot. But, you know, it would be, I'm sure Renan would sell for less than that. But, but, yeah, I mean, it would be interested to see how well he's going to do it. Yeah. Yeah. Yeah. with Microsoft's BD team, but that's kind of what we've seen too. This is the slide that almost got Bilal fired and has Manish and on, you know, calling me aggressive. But this is the comparison point. So Azure 10.3, Gen 10.3 is their new all-flash hardware that they're supposed to be shipping in March versus what we designed in an ODM SKU against the 120,000 GPUs that they're getting in Nebius. And so these are the effective, you know, Yep. capacity numbers in terms of 4.3x reduction power, 7x reduction footprint, and giving 1.5x more throughput. And then that, and they even underspect it. So the other thing they did is they were using NVIDIA 1.6 gigabits per second per GPU number, which is actually for the GB200 and the H2 versus 1.9 versus we actually spec'd at 2.4. That's crazy. Yeah. So we're 28% above NVIDIA's targets. There's 16 below NVIDIA's targets. And we're doing this footprint. And I mean, I think NFS and the benefits of file and set aside all the other manageability things that MAI love and all of the control plane reliability things that MAI love. Like that's the fundamentals of why Vast and why Manish is so upset because MAI now has this slide. Yeah. this is insane. Yeah. and they can show it in a Scott Guthrie meeting. This is awesome work, Jason. And to me, this is just presenting facts. This is just a factual presentation. Why is that deemed aggressive? Yeah. Because I got it through aggressive means. I got the data through aggressive means. I shouldn't have that information really like they gave it to me because they knew me and not because they were like you know they overly trusted me I think Jeff is pinging me because I sent Jeff yesterday. Hey, Jeff, this is meeting on Wednesday. It's on and Manish. Do you have the power saving presentations that you talked about in Iceland? And he said, Jason hasn't sent me it. I'm on it. And I said, hey, in a meeting with Jason right now, getting it from me, man, what awesome work he just did. Thanks. This is all. This has gotten us into the position that we are in now. Like the work that you have done. And now we can basically just go in and mend the relationship. Like just a few corrective statements. Like, hey, this is what MIA. Don't forget that MIA is the one that is in contact with us. And they are the ones that are asking Jason to do this. Yeah, precisely. Yeah. Yeah, and you should even mention they had an existing relationship with Alone as well. Like, they reached out to Alone, and they've known him longer than they've known me in the past, right? So, you know, this is a team that was well familiar with that. They already have a production VAST cluster, and it's been live for a couple years. I mean shit Yancey when I was in my previous role I went and got from our finance team at Microsoft a copy of the invoice that they pay for VAST because we were trying to figure out how much they were actually paying for VAST so we could do some COGS calculations and figure out where we would land against VAST from a price point perspective like I had the invoices that MAI was paying back to CoreWeave for their VAST cluster a year ago in my my own inbox so like they already know how much MAI loves VAST it's not a surprise And the fact that they're kind of coming after it now is only because I'm over here and they have a sore spot. Yeah. No, I think you better than anyone, right? I got a previous employer pissed off at you. Yeah. Yeah, yeah, yeah. Yeah. Yeah. Believe you me, I know the feeling. Yeah. Yep. So, no, I love it. I mean, this is awesome, awesome work, Jason. And I'll shoot you a text after the meeting with Ong and Manis, and maybe you can jump on a call and just go over how it went. Yeah. and then formulate a strategy to go forward. Did they give an agenda of like what they want to talk about or is it just let's catch up or like do you have a sense for how they're coming at it? From my perspective, they want to talk about the NAPUS and scale situation, MIA or Microsoft AI. Hmm. I don't know. Like, I'm going into this meeting to listen and say like, hey, let's just partner on providing the best possible solution to our joint customers. Yeah. Well, where it stands at the moment is from NAI's perspective is they want to do a POC And a decision is delayed slightly. The original timeline we were operating under was a go live in January for production scale up in February. That I was very clearly been told those timelines have slipped. And we're now more looking like go live in May and a decision maybe March. And so they wanted to do, MAI wants to do a POC in January. And really, like, a POC is kind of pointless because they already have experience running Vast in production because they do that already for their Condor cluster. So it's much more, I think, about the performance bake-off with Azure and giving Azure the time necessary to field whatever solution they're going to put up against us. Okay. Yeah. Yeah. that introduces a few complexities that we have to navigate, which is first and foremost, supply chain. There's real constraints here on both sides to get hardware and not making decisions around this until, let's call it March. Who knows what the flash supply situation will be in March, but I'm going to guess it's not going to be better and it might be worse. Mm-hmm. Yeah. Yeah. project kind of at risk. We were committed to delivering this if we could get a go decision this month. So, you know, I don't know. Go ahead. So from our lens, it's actually only about 900 petabytes of hardware because MAI get 1.9 BRR on their Was that for the entirety of the 1.5 exabytes or was that for just a portion of that? Without the reduction. Okay. Okay. So we were factoring in 1.8 DRR. So we were only delivering like 900 petabytes. That's another reason why we win here because Blob has no concept of data reduction or data deduplication. They're deploying a full 1.6 exabytes. Yes, what we had kind of talked about with our ops team and Renin and everybody was we would prioritize supply for Microsoft, but we wanted them to sign the full deal now so that we could get the hardware to the Yeah. Mm-hmm. the project build out schedule is still everything is online by December of next year. So, you know, it is a tranche based delivery of four data halls, each data hall contains one 14th of the vast racks and one 14th of the GPUs. And so, you know, if we're not making decisions, then we're going to have the supply chain constraints, we'll have to kind of navigate that through internal prioritization of what we want to do there. But that could be a big Yeah. Yeah. Mm-hmm. to the timeline being slipped. On the Azure side, you know, here's rumors I hear that I probably shouldn't hear, is that they're equally constrained on Flash, and they were actually having conversations with MAI of potentially going back to hard drives, because they have supply of hard drives. Yeah. Really? I thought the hyperscalers had priorities because literally they're the biggest customers in the world. I thought they wouldn't have. So I'll add you to another thread. I'll send you some of the notes that I shared with the team. But so another wild thing happened for me last week, which is the guy who owned the relationship with all of the flash providers and the HDD providers for Microsoft. Yeah. His name is Kaoshi Goshi, I think his name is. He and I worked together actually quite a bit in my previous role, trying to navigate what a forward-looking AI demand curve would look like in terms of how many exabytes of capacity annualized the trend would change, what kind of purchasing Microsoft needed to do. Because his team, and he was a VP level at Microsoft, his team was responsible for all the negotiations with the Yeah. And they like Microsoft notoriously has had worse margins than AWS or Google. And it's because of operational inefficiencies. And so one of the screws that and Manisha owns this decision, right? One of the well, alongside finance, one of the screws that have been turned in order to try and drive the margin up to make Wall Street happy is to do a lot more just in time supply chain. And, and meaning that like, Okay. Oh. to minimize the risk of overbuys just didn't happen. And so Microsoft very much would do minimum long-term contracts and any upside was always purchased just in time. And so going back to my conversation with Kaushik, he left Microsoft like three months ago and now he's a VP at NVIDIA and he's doing the same thing but for HBM and DRAM. Oh. Yeah. And so he pinged me last week and said, we need to chat. And so he and I talked and he was saying that what's happened in the industry is that if you think about like what's necessary to fab HBM and DRAM, it's very similar to what's needed to fab QLC and flash. And so, you know, Micron, SK and Samsung, and actually Samsung just announced they're cutting all of their consumer SSDs. They're repurposing Yeah. from Flash to HBM and DRAM because it's so much higher margin. And so they're self-cannibalizing their production capacity on Flash at the moment. And NVIDIA is now running into, they can't even get enough Flash, and they're getting escalations from Dell and Supermicro, he told me, and they're not even bringing GPU clusters online because there's not even enough Flash for the hosts. Peace. Yeah. Jesus. So on the other side, Microsoft is sitting on this real problem on Flash because they didn't make any long-term buys. And so they're only getting Flash that they committed to, you know, a year, 24 months ago. They're not able to increase their demand signals. And so they're very constrained on Flash. On hard drives, you know, even before I left, Microsoft was constrained on hard drives. the hard drive vendors have the opportunity to scale up manufacturing and obviously there's no action like Seagate Western Digital Toshiba they don't make flash so they're not a petition with HBM and DRAM they're not actually able to repurpose so and see it had like I don't know like 24 months ago actually shut down an entire manufacturing line because they thought hard drive demand was going to crater and so Seagate is bringing a whole manufacturing line online right now so what it what it tells me is that hard drives are going to be Yeah. Yeah. Yeah. far less constrained than flash and Microsoft is in that situation right now where you know I'm sure they're having pain on hard drives but it's not going to be the same level that they're having on flash. No, that's... crazy scenario to be in. I mean, like one thing, honestly, NetApp did really well was they had a guy that was doing all managing their entire supply chain is they're always stocked up and have been for years. And he was criticized so much for overbuying and overprovisioning all everything that they did. But, you know, this is a weird, scenario that we are in now. Because I mean. But I mean ultimately. Let's say. Worst case scenario. How does the comparison look like. If we were actually running on hard drives. You know spinning disks. I mean god damn. I mean I think that we could actually do really well there I mean we'd have to architect some things and have a good cap layer and we'd still need a certain percentage of the cluster to be flash and we'd still need SLC and I think you build a pretty powerful solution with the hybrid storage media like that like hearts and minds and winning over our engineering folks and kind of walking back on our flash Yeah. Yeah. Yeah. only promises and building a supply chain around it. Those are all big efforts. Whereas obviously that's already what Microsoft does. Like if you actually look at one of the HDD clusters that that'll be fielded or that are already fielded, those are actually like a blend of flash and hard drives. So all the right aged on SLC and QLC and then there's they're lazily moved out to disk and then there's a read cache that happens. So like all of that already happens on the hard drive clusters that Microsoft is deploying. That's how they can some Yeah. Meaningful performance out of hard drives and a little bit more reliability and consistency on the latency. But like we could we'd have to have some notion of tiered storage that we just don't have. Yeah. Yeah. Yeah. Yeah. No, that's like, that's always fun to talk to Jeff and Renan and Shakar about tiering, auto-tiering. I mean, this is just yet another example of why we need to have tiering. But ultimately, things change. And if the supply chain is where it looks like it's going, we might be forced to do it anyways. So, yeah. Yeah, absolutely. Absolutely. Yeah. If all you can get your hands on are hard drives, you've got to store them somewhere. Yeah. Even the hard drive demand is crazy, though. So it's still like, I've heard it's still like 12 to 18 months to get incremental buys at the moment. But what is also true is that a bunch of bad decisions, like I said, shutting down manufacturing lines, are being unwound, which should increase the supply. In my previous role, and I got the same information from Kaushik when I was there, worldwide hard drive manufacturing is about 100 exabytes this year. So across the three drive companies, they'll produce about 100 exabytes of drive. Yeah. Yeah. Yeah. Yeah. Not more than that. No, most like 70% of it goes to surveillance and like most of that to China. Yeah. Yeah. That's insane. No, but I mean, like, what? Yeah, I got this request to get Scaleway in France, and that's NPC in France and Netherlands and Poland, into the TPU team in Google. And they came really quickly back, "Hey, we need to have a meeting. Let's jump on a meeting tomorrow." Because they are our competition. large GPU service provider. And I don't think they're moving away from NVIDIA. They're NVIDIA bat and they don't want to sell them TPUs. Which is... It's going to be different if it's Nscale or somebody that is actually far bigger. But Jeff said, he's asking me to do a check with the TPU team if Hmmm Nscale might get a different reaction than Scaleway. Why would they care though? I mean, selling is selling. And Scaleway and Nscale, they don't have the full stack like the public clouds do. They don't have all the associated services in the same manner. So why do they even view them as competition? Does Scaleway don't have any management or control playing capabilities either? They have like a very rudimentary cloud management platform, like a CMP. But there are no services there. They have a Kubernetes environment. That's basically it. Well, I haven't... Yeah. There was a... I was reading some article, I think it was over the weekend, around just the disparity of all of the different and how they're approaching trying to carve out a niche in this world, bring them forward from the end-to-end spectrum. I think it was the guy that runs semi-analysis that wrote, it was like a medium post or something, but it was like his take on going from like the far end of the spectrum where Crusoe's at, where they just view it as a completely managed inferencing cloud where it's all higher services and your business model is charging for tokens, not infrastructure. Yeah. Yeah. to the other spectrum where it's like their data center shells and they're providing space and power. And I saw he claimed there were like 180 neoclouds now or something like that. Companies that have like are trying to get into the crack this market. I mean, it's just like there's obviously going to be consolidation. It doesn't make any sense. And, you know, this this to me is, I think, our biggest opportunity as a business. Yeah. Yeah. That's. Mhm. Yeah. Yeah. I'm excited about going and talking to Microsoft and trying to win MAI, but really where I think the opportunity in the puck is moving is there needs to be a common control plane infrastructure for all of those neoclouds to be able to go to market quickly with all of the services from managed inferencing like Crusoe's doing to managed IaaS for just, you know, I want GPUs as a service and storage all into one sort of very simple cloud. Yeah. Yeah. that takes the like an MVP of cloud, right? I really think we could go and win that. I think we're best positioned. Like we already have most of that plumbing and extending it. This AIOS vision could be re-scoped and reframed to become that and go win all of that business and become a consistent platform that connects all of those neoclads together and gives them a business opportunity. Yeah. Yeah. I agree. Yeah. Yeah. Yeah. Now, of course, the challenge then becomes for them is how do they create their own niche above that? You know, I don't know. I mean, that's a product question for each one of them to go after. But I still think that that should be our play forward and be where we go make our name. Because winning the hyperscalers is always going to be the eat your own dog food, not invented here problem. I don't know. Yeah, yeah, I agree. I totally agree. And, you know, having that, I mean, that's why, like, for example, one of the reasons why VAST doesn't have any ISV strategy or any ISVs that are actually building their applications on top of VAST is because it's not really pluggable or extensible. So that's why we need to create that service layer that allows for them to plug in very seamlessly into VAST and start building their AI applications on top of VAST and therefore taking advantages of the AI operating system, not viewing it as just a dumb file system. And that's one of the things that should scare Azure, in my opinion, is that Microsoft AI is barely scratching the surface in CoreV of VAST because they are basically, it's completely obfuscated away from them. They don't see all the bells and whistles of VAST. They're using it as a file system. And I think this is going to be to your point. If you can actually do something with CoreV that allows their customers to use bigger portions of the vast AI operating system, use the event broker, use the agent or agent engine, then we have the play that you're talking about. Everything is exposed to the end customer instead of just like you get a file system from vast the way it is in Corvive today. Yeah. One of the problems though is the workloads that need an event broker and a database and higher level services like that are not GPU centric workloads. No, I agree. Yeah. And that then becomes either an on-prem or a cloud play, not an NCP play, only because if you look across all of the NCPs, none of them are talking about deploying CPU capacity at scale. There's no NCP right now where I can go and order a million CPU course in order to run my Spark pipelines. And so I'm not running Spark or Databricks or Data Lake and those kinds of scenarios. Yeah. Oh. No. and workloads in the NCP, then the value of those capabilities within VAST disappear. Because in a GPU adjacent world where you're just focused on the GPU workloads, there's really two scenarios. There's training data and checkpoints and model distribution. Those are the only three workloads. And those are file workloads. Those are very straightforward, up and down sequential I.O. where performance matters but not much else. Data spaces comes into play because you have to move that Yeah. Yeah. Yeah. data in and out of information. So you can definitely make a play for data spaces and Sync Engine. But beyond that, the rest of the vast value isn't present in the NCPs because all they have are GBUs. But if they want to be competitive with the hyperscalers that also have GPUs, they have to have some surrounding services that actually helps the application builder to actually create their application, not just to train the model. Yeah. Yep. Agree. But then that means that CoreWeave needs to start carving out some data center space and deploying a bunch of Turin servers instead of Blackwell servers. OpenAI is the largest Databricks customer, period. Their Databricks pipelines were running on 40 million CPU cores. Yeah. Yeah. in Azure US East. And the way it's architected, it requires everything to be in a single region and connect a single Azure Blob storage account. And they hit capacity of 40 million CPU cores. And they hit the limits of what Azure could give them. And so I'm sure you saw the news, but recently, very recently, they were removed their cloud exclusivity. That's insane. That's insane. That's insane. to Azure agreement was removed from the contract Microsoft and OpenAI. But prior to that is when these conversations were happening and Scott Guthrie gave them permission to migrate the whole damn thing to Amazon. Because Amazon stepped up and said they could give them whatever incremental number of CPU cores plus what they already had in a single AWS region. Azure just had no ability to grow US East to give them enough CPU capacity. OpenAI is moving that entire workload out of Azure. Yeah. Yeah. Yeah. And if you were a NeoCloud, if you were, you know, Crusoe, CoreWeave, whatever, like that would have been a really interesting workload to go in if you could have taken and carved out enough megawatts and deployed enough Turin so to go after and just win that Databricks pipeline. But obviously none of the NeoClouds are going after that business. And, you know, it's probably lower risk margin, I would think, but probably not high revenue. Like it's not the same. It's a lot less revenue, but probably lower risk margin. I don't know. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. That's great points. Yeah. I mean, it's a tricky scenario for the MCPs because I mean, right now, most of them are funded by NVIDIA. You know? So, like, the reason why CoreWeave can be this competitive is they're getting substantial discount from NVIDIA compared to, like, for example, GCP with their TPUs. And if you compare the price between PPU workload and GPU, or basically, let's say, an hour GPU versus TPU cost per hour is insane right now. Like, it's one-eighth. You know, like, if I was NVIDIA, I would be panicking right now. It's one-eighth. Yeah. And the anthropic deal is like they're going out there and saying like, we're saving hundreds of millions of dollars by going TPUs. So, yeah. Yeah. Yeah. Well, the NCPs I'm nervous for. I think Corwe is the only one that's publicly traded, but I wouldn't be investing in any of them personally because the reality is the reason they exist, like their entire existence, is because the hyperscalers aren't willing to take on the risk themselves. You know, Sacha's shrewd, Amy's shrewd, Amy Hood, the CFO of Microsoft. Like, I was in meetings with them when the CoreWeave deal was signed, and they were very clear, and there's been plenty of public reporting, even Sacha talked about it in the podcast a month or two ago. Like, the strategy here is they don't want to be holding the bag on a whole shit ton of last generation NVIDIA capacity. Yeah. Yeah. Yeah. Mm-hmm. So how do you, you know, Azure, actually hyperscaler principle 101 is that you depreciate the asset over five and you try to run it for seven or eight and extract value out of it. And, you know, if you're talking about GPUs, the ones that are coming out today, are they really going to be useful in seven to eight years? Are you still going to get revenue out of them? Or are you going to have to life cycle through all of that and replace the data, refresh the data center with a newer generation? Mm-hmm. and then you didn't get the margin you forecasted out of that capex. All of that risk has been shifted to the neoclouds and NVIDIA itself because of the circuit coming situation. What happens here is somebody's going to come and scoop these companies up for a fire sale in a few years when they all file bankruptcy. Or they're going to come collapsing at some point from a business model perspective. Maybe we'll all get to a super Yeah. Yeah. Yeah. Yeah. system and capitalism doesn't make sense anymore by time that happens. No. I don't know. Yeah, we'll all be on citizen pay. Exactly. But the business model doesn't hold over seven years, given the pace of trajectory of what NVIDIA is delivering and what their roadmap looks like and what the costs of these systems use compared to what they're getting in terms of revenue out of the systems today. It just doesn't pencil out over that time horizon. And that's shrewd financial planning on Microsoft's part to go and sign these multi-billion dollar deals that remove all of that long-term risk. Yeah. No, I mean, I think you're spot on. And like, there are so many data centers here, especially in Northern Europe. that are just banking on the NCPs. You know, like you have you have you have nephews here in Iceland. You have Nscale here in Iceland. You have what's his name? 42. Oh, it's a world again. Yeah, yeah. G42. Yeah, whatever. But I mean, they're literally taking up every single data center. Core 42 or G42? space and power in Iceland. And yeah. Yeah. So Iceland is 100% renewable energy, hydro and geothermal. So 30% is geothermal and 70% is hydro. And power here is exceptionally cheap compared to everybody else. I'm sure building aggressively as well is any of thermal like it feels like that's the benefit to Iceland Yeah. And heating is basically free because of geothermal. So, so like, and the good, the positive thing is the aluminum smelters or the aluminum companies are because of restrictions imposed by the European Union is basically making it almost impossible for them to be in Europe. So they're all going to South America and Asia. And if that goes, then Mm-hmm. That's cool. You know, thousands and thousands of megawatts get freed up from the power grid here in Iceland. Yeah. You know the Azure West US 2 region in Quincy, Washington, they there was a big aluminum smelting factory out in eastern Washington that was built in like the 40s that they built their own hydro dam over the Columbia River and it was powering that smelting factory and then it went out of business. I'm sure South America or somewhere who knows and it's been it was vacant for decades. And then Microsoft came in and bought that dam and owned their own dam on the Columbia River. That's what PowerQuest US too. Yeah. Yeah. Yeah. That's crazy. It's the same thing in Tacoma. Everything in Seattle is, let's call it dirty energy, except Tacoma is hydro. There's a lot of data centers in Tacoma, just outside of Seattle. It's pretty crazy. Hey, I have to go into this support training, Paul. Hey, awesome talking to you. I'll give you a call tomorrow. And if you can send me the deck, it would be perfect. Thank you, buddy. Awesome, awesome work. Bye. Yeah, I'll see you right now. Yep, thanks. Bye.