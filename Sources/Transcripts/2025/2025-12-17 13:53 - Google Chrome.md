Thank you. Thank you. www.patreon Avinash, hello. Good morning. Good. How about yourself? Good. Is that a place in Colorado? That's right. Yeah. How about you? Looks like maybe the Bay Area? Yes. In the Bay Area. That's all good. Okay. Well, I'm out there Good morning, how are you doing? Good. And you're based out of Colorado? Yes, in the Bay Area, Los Altos. All the time. Yeah. I grew up in Colorado, so it's home for me. Yeah. Have you been in Colorado all your life? I have. Yeah, I grew up here. And yeah, so I spent the last 13 years at Microsoft prior to joining Vast. And I did do a tour of duty out in Seattle for a little while, but I moved back home a few years ago. So. I see. I see. Yeah. Yeah. I compare me in hotels. I'm sure. Have you been in Colorado all your life? Oh wow, okay. I see, I see. I don't know, this is Zoom AI companion, what the hell is this? Oh, that might be excellent. I have it configured to zoom, but I don't know why I would have turned this. We can boot it. It's like my, I need to disable that. No, no, no. As long as one of those knows who it is, I am at this bitch. I had configured it to, I have it set up to kind of record meetings, but let me boot it. It's fine. I don't need it. Sorry, one second. Time on one second, please. I'm out of your way. Hurry up, because you're getting late. Sorry. That's okay. Oh, that's yours. No, no, no, no. As long as one of us knows who it is, I'm happy with it. Sorry, one second. Just hang on one second, please. I'm out of your way. Hurry up, because you're getting late. Sorry. Sunset after school it's kind of drizzling today so I don't want him riding his bike so he was asking me who's going to drop me off yeah kind of earlier that's all right it's way lipstick so I'll give you the genesis so whale is the last name of one of the greatest mathematicians of last century name was Sun's up is cool it's kind of grizzling today so I don't want him riding his bike. He was asking me who's going to drop me off. I know how that goes. Early in the morning for you. Kind of early though. That's alright. Yeah. So is it Welliptic? How do you say the name of the company? It's Welliptic. I'll give you the genesis. So Well is the last name of one of the greatest mathematicians of last century. Okay. Andr√© Weil, he did some of the foundational work in algebraic geometry that laid the foundations for what became the fundamental building blocks for modern elliptical cryptography. And I was drawn to him because he's also a very kind of a, he's a character. So if I may indulge you with some storytelling, Andre Weil was, you know, he was, he did some of the foundational work in algebraic geometry that laid the foundations for what became the fundamental building blocks for modern elliptical photography. Ah. And I was drawn to him because he's also a very kind of a... he's a character. So if I may indulge you with some storytelling. During World War II, he was incarcerated, his French, he was French rather, he was incarcerated by the French military police. And he got, and they suspected him of avoiding conscription. So he had a prison during World War II, he was incarcerated, he's French, he was French rather, he was incarcerated by the French military police and he got, and they suspected him of avoiding conscription. So he had a prison sentence in a French military prison. And apparently during the time there was a, it was mandated that anyone who spends time in a French military prison, gets stationary material. You get a good supply of paper and pencil. And he says some of his best mathematical works came out during that three-year incarceration period. He was so enamored by it that when he came out of prison, he wanted to mandate that every budding French mathematician get mandatory prison sentence. And elliptic comes from elliptic curves, because we use a lot of elliptic curve cryptography. in our systems. Got it. Makes sense. Yeah, it's kind of like a forced retreat. Like, go hang out in the mountains and have your best thoughts and write down all your ideas. I get it. I certainly appreciate that isolation sometimes when I'm trying to get something done so I can see. After World War II, I think sometime in the early 50s probably, he emigrated to the US and he became a professor at Princeton, which is where he spent a vast majority of his professional life. He died early part of the century by the way. Yeah. Okay, it's a long time ago. So he had a lot to do with elliptical curve encryption, makes sense. I took a look over your website. I kind of understand the gist. You're using your blockchain ledger for auditing agentic AI execution is the gist I get, but maybe you can tell me the story and how you see that relating to what we're doing at VAST. Yes, let me tell you this. Give me one second here. What we have foundationally built is a globally distributed, cryptographically secure compute platform, which is coupled with a virtual sandbox environment in which applications can execute. Now, when I say applications, think AI agents, MCP servers, including Web2 applications, for that matter. Now, what we are... That kind of constitutes what we believe is the foundational trust fabric. It brings in trust, security, verifiability, and observability for any application that comes and sits on this platform. We have two other offerings on top of this platform. One is Icarus, which is a virtual conversational agent, which can bring natural language workflows to any proprietary system that exposes an API. So basically we have, for instance, MCPs that . that can connect you to a vast majority of enterprise-grade systems like Snowflake, BigQuery, Aurora, S3, MongoDB, Kafka. We have a whole slew of them. So you can typically come into your conversational chatbot, stitch multiple flows together, and launch a super agent that can talk to all these systems and perform tasks on your behalf. That's the conversational part. which is a trust-aware agent orchestrator. The thing that these two provide that no one else in the industry provides today is the ability for one to have fine-grained authorization control over what an agent can and cannot do with certain parts of your system. You have end-to-end visibility and observability, and you have auditability. This is immutable logs, so you know who did what In addition to that, because we are cryptographically secure, we have completely eliminated problems that are plaguing the AI industry now. Think prompt poisoning, man in the middle attacks, etc, etc. Where if you submit a prompt, it can be intercepted in between and it can be modified to force the LLM to do things that you had not intended for it to do. Okay. What we bring to the table? So underneath that, a few questions. So let's start with, do you work in like secure enclave environments? Are you managing the underlying infrastructure to achieve this? Or are you just running containers over the top of whatever hosting environment? No. Good question. You can think of us as a SaaS offering that runs across all three physical cloud providers today. as one global fabric. Let me share my screen and I'll show you certain things so you'll be able to wrap your head around it better because seeing will help understand this better. So this is a network that I typically use for demos like today. This is a snapshot into our whale chain foundational platform. We basically deploy what are called pods. So we have three, you're seeing, Okay. Yep. here, four pods. The one in the middle is Senate. Ignore that for a second. I'll come back to it later. But these are the pods that we typically deploy. So in this network, I have one in the West Coast, one in Asia Pacific, one in the East Coast. A pod is nothing but a collection of nodes. Okay. So this pod is made up of the US West Coast pod has five nodes in the US West Coast. Now, you can imagine these five nodes being spread across AWS, GCP, and Azure. Now, why did we take that? If you remember about a month or two ago, there was an outage in the AWS East Coast region, which caused large portions of the Internet to be inaccessible. If those applications were deployed on a platform of this nature, then a loss of a cloud region or loss of an entire cloud provider would not have affected your systems. Okay? So on these pods, you deploy applications or applets as we call them. Again, when I say applets, think MCP servers, think agents, think Web2 applications. Now, why is this important? If you look at OpenAI, if you look at Anthropic, they have a conversational interface. They allow you the ability to configure MCPs. But the infrastructure for the the MCP, the onus is on you, the consumer. The onus of the autoscaling of it is on you. The onus of protecting it from cybersecurity attacks is on you. What we are saying is we take that burden off you. You just focus on what you need to build, which is your MCP. One click deploy on this platform. We take care of infra. We take care of the autoscaling of it. We take care of you being immune to cybersecurity issues because of all the cryptographic capability that we have. So I'm going to pause for a second. Does this give you a good mental model? No, go ahead. Yeah, I mean, I think in terms of the hosting infrastructure and how you've got that going, I guess what I'm hearing then is primarily what you are hosting are MCP servers. Because not... Go ahead. I'm assuming these are running on, you know, like a Kubernetes cluster or just containers running inside of the three clouds. Not GPU hosting of the actual agent. You're kind of a MCP proxy layer. back to some SAS systems for exposing that to whatever agent system you're running. Is that? Yeah. That's a simplistic way of looking at it, but yes, that's a reasonable mental model to have. The MCPs are deployed on us. or agents that are running, even agents that are running do not need access to GPUs. You need GPUs only on machines where the LLMs are running. That's a separate tier. We can do both. We can even host LLMs for you if you want to go down that path. But typically what you could do is these LLMs are now being offered as a service on all the major cloud providers like Google has Vertex AI, Amazon has Bedrock, I think Azure has something equivalent. Right. Yeah, and they have a bunch of LLMs that they offer, including the state-of-the-art models. You can bring your own keys and hook up your own LLMs. It's not a problem. One other thing that I wanted to show you when I said Web2 applications can be deployed on this, what you're seeing here is our chatbot or our conversational Okay. the chatbot that is deployed entirely on this platform. We could do that because we have a service that we run that uses VLLMs to manage a bunch of open source models. We could also say bring your own keys and connect to GPT-5 or to Anthropic. So where's inferencing happening? Are you managing the GPUs? bring your own keys. When you use this chatbot, there is a notion of identity. When you come in here, your identity manifests itself in the form of a wallet that's deployed as a browser plugin. This is your identity. You could actually put your keys in your wallet so I don't even have to see your keys. If you have an OpenAI key, you put it in your wallet and you just pass it through whenever you need to do any API calls that need to be driven through inferencing. We take care of all that. Everything is masked for you. So if you were to use this chatbot, for instance, let me give you a feel for it. This can be used in two modes. One is conversational, one is agentic. So let's give you an example of a few agentic flows. Let me move this away from here. If you come here and if you look at, actually, can I close this? Yes, I can. This is how you configure your MCPs into your chatbot. So I have a few pre-configured MCPs already. But let's say I want to come in here. I have an MCP that we had written for ServiceNow. I just copy its identifier. It's already deployed on my platform. I can come in here. I can say ServiceNow. Add server. And I'm ready to bring natural language workflows to ServiceNow at this point. But let me show you a couple of things as to what I mean by natural language flows. I have a few pre-canned prompts. that I would typically use for this. Let's say I want to pull some information out of a database, for instance. So this is me talking to a Postgres service that's running on AWS. Okay. So if you notice, pure natural language, no SQL, I have no idea what the tables are. I don't know the relationships between the tables, nothing. I just say to return album items, export it to this file. Once you're done, I'm going to wire up another agent to basically go and stick this file into S3, all in just natural language. And then I'm going to say, when you're done, send me an email. And if you notice, I'm using an open source model over here. It's the Quinn family of models that come from Alibaba. I can use any model. That's what I meant to say, but that's why we give you the option. If you have already made investments with ChatGPT or with OpenAI or Anthropic, you can bring your own keys and you can do the same thing. Now, I'm going to hit send to trigger this super agent that can talk to Aurora, that can talk to S3 and then send you an email. But what happens when I hit send is this entire set of prompts gets sent to my foundational platform, but they're digitally signed using your identity, which basically means that when I get it, I verify signatures. What does that imply to me? That means that if anyone in the middle of the If there is any prompt poisoning, I'll be able to detect it because digital signatures won't match. And no, you cannot hash them. You've got to digitally sign them. So we have a bunch of stuff that is written, that is built in Rust, that is part of this chatbot installed as a WASM module. So let me clarify how this is working. in the browser, it looks like you installed maybe a Chrome extension. The Chrome extension is hashing the contents before you click submit and then you're signing it locally. that is typically done. So you cannot hash it because if you hash it, you cannot get the prompts back. Right? So it has to be digitally signed. white hash and signing the hash to make sure that it recomputes on the other side but yeah exactly So if I do this, now this goes and does its stuff. It will talk to Aurora, fetch the data, create a file, put that file into S3. And then it will shoot me an email saying that I am done. Now, the beauty of this is in a typical system like OpenAI or with Anthropic, you can do this. But now I have no visibility into who did what, when. It's basically the Wild West. You have absolutely no control. But because we run on this foundational platform, I can go onto this tab and you can see that this has been logged. And if you notice, it says task ID is this. If I take that, Thank you. and if I filter on the task ID, it'll tell me these are the three flows that you just ran. If I click on this, it gives me a full breakdown of, okay, this was the identity, ran this one minute ago, and this is what they ran, and the natural language that you entered, prompt that you entered got converted into this SQL and executed against your system. And then the next step in the flow was this, talking to S3, and So I can get full auditability, full visibility, and these are immutable logs. So you cannot tamper with them. And this was the final one in the puzzle where it sent the email out. So every flow, every super agent, every step it takes, every tool call it makes, every step it takes, every communication with the LLM can be digitally logged so that you get full visibility end to end. So I get the browser to MCP angle. I mean, my pushback would immediately be, I mean, we've been doing TLS for decades now, and it solves this problem for the most part in a very secure way. and we don't have man in the middle attacks in our credit card transactions and we trust it for the most critical infrastructure there is. So why do we need a browser agent to solve that problem? What additional reason are you justifying installing a thick browser agent for that? No, it's not. I mean, that should be the least of your concerns in my opinion because that's a small WASM module that sits there and does some work that I want done for my ecosystem. So that shouldn't be the focus here. Even if you have TLS and stuff like that, now you'll have to distribute these certificates all over the place. The TLS is kind of typically doing an old school way of signing stuff or encrypting stuff, right? Yeah. Now, modern cryptography allows you to do that without distributing any certificates, nothing like that. And it's very easy to do these digital signatures in the browser and sending them to the backend. So that it's just a very, it's a different way of doing something which none of these other guys do because they'll have to distribute certificates at this point. And the reason it becomes important is because you have a notion of an identity now, which you will typically not have if you use traditional systems like if you You do it the traditional way like OpenAI or Anthropic because your identity is when you log in, you put your email address and your passport or whatever your identity is. There's no private public key pairs associated with your identity, etc. etc. That's where it becomes very, very different. So then from the point once you've received the prompt, you're now making all these communication outward to the various services, the MCP service going outward to S3 or ServiceNow or whatever. What's your special sauce there? You're actually taking the request and signing that as well and storing it as part of this ledger. Yes, exactly. So there's a lot of heavy lifting that happens there in terms of how these things get tied together and how these things get put on the whale chain platform for observability and visibility. And then because this is running geodistributed across clouds, you get fault tolerance across cloud outages or regional region outages. And you also get one other thing that you do get is data sovereignty intrinsically. So you see all this chat history today. When you use other systems today, you know that your chat is being stored, but you don't know where it is being stored. But in our system, since I'm in the West Coast, I'm guaranteed through some DNS and any kind of data. that all my requests will be routed to the West Coast pod and I can guarantee, provably, that all my chat history, everything is stored in the West Coast pods. So I could spin up these pods however you want and I can guarantee that we will persist your data within a jurisdiction that you would like it to be in. Probably. So, but if the west coast pod goes down, then you're able to run in the east coast pod, I assume. Does that mean you're-- Well, that West Coast part going down will practically become an act of God, because you will be spread across so many different regions in the West Coast, that an entire West Coast part going down will mean that at the very minimum, AWS, Azure and GCP going down all going down together. So I guess what I'm probing towards is it sounds like maybe you have some sort of distributed state store. Are you doing some sort of? So how does that behave? Is it some sort of like distributed system? Yeah, of course. Of course. So nothing is open. Nothing is open source. We home built everything. So the each part, think of it as its own ledger. And we built our own BFT consensus into those into those layers. So if you're familiar with works like Narwal and Bullshark, those are like consensus algorithms that we implemented to drive all this capability. So just, I don't know, I never told you about my background, but I've been doing these kinds of things for the last two decades. I was one of the three guys who co-invented the Amazon Dynamo system, which became the genesis for the entire NoSQL movement. And later on, Apache Cassandra, if you're familiar with it, that was my brainchild out of Facebook. Thank you. And then in 2012, I started my first startup, Hedwig, where we built a... We actually solved a multi-hybrid cloud problem for distributed storage. So I've been doing systems for over two decades. So this you could think of... I shudder to use the word culmination, but it is a coming together of last 20 years of systems thinking. Let me put it that way. Yeah. Interesting. You know, so I guess we've got a few minutes left. Where do you see this aligning with what VAST is doing? I don't know how familiar you are with where we're going with our agent engine and, you know, what you're doing. capabilities we already bring from a distributed storage platform. I know about VAST from a distributed storage platform perspective, but I didn't know anything about the AI part. In fact, full disclosure, to be completely honest. When we reached out to you, people told me that, hey, maybe he is the wrong ICP. I said, no, he has a fantastic background. He may be the wrong ICP, but he has a fantastic background. At the very least, I would like him to know what I'm doing. So full disclosure there. Yeah, I mean, so that's, there's a lot going on at the moment. You know, we're looking at, I don't know, there's a, we're calling it the AIOS. You know, obviously we've been a storage platform for a long time. We have made various inroads with the frontier model builders and, you know, we're the de facto software. for core weave at this point. But the idea here is to move up to higher layers in the value stack and leverage the underlying platform capabilities to expose some of the scenarios that kind of align with honestly what you're doing. There's this notion of our agent engine, which is really an event-driven agent framework for execution on data ingestion. We have Kafka pipelines for data ingestion. And all of this leverages the underlying optimizations and hardware and the hardware capabilities that the vast platform bring. When you tie all this together, that vision is very much a distributed execution engine. When you think about how this relates to the hyperscalers and the NeoClouds, what we're very much observing is that the large enterprise customers have their data lakes, their spark pipelines, Databricks, and so forth all running in the hyperscalers, Amazon, Google, Microsoft. And the GPU capacity is very disaggregated across the globe. So a canonical example would be someone like an opening eye where they've got many exabytes of their core data lake sitting in a hero Azure region. But then they have GPUs spread across 50-something Azure regions, Oracle, CoreWeave, Nebius, all of the different neoclouds. And what they really need is some global state store that allows them to run pipelines and ingest data into the central Azure region and then have that exposed across all of the places where their GPUs are located and then ultimately kick that off into agent execution within the GPU infrastructure and fabric. That is the scenario we're pushing towards. Where we sit today, is there still some product development before we get to the full vision realized? When you think about the NeoClouds, Thank you. The additional play we've kind of got focused is I think I saw over the weekend semi-analysis said something like there's 180 neo-clouds in development which is I mean it's just insane and when you talk to them they're the continuum of maturity you've got at the frontier core weave who have a full product team and strong engineers and you know they're ready to go and deliver value and they're building their own cloud platform to the other end of the spectrum where they're effectively a real estate company that knows how to pour concrete Thank you. has a big bank account but have no experience building any kind of systems or software or any kind of infrastructure experience at all. And I would say the majority of those, you know, whatever 180 of them that they're out there kind of tier towards that side where they have no cloud experience whatsoever. And they're, you know, frankly, they're Saudi oil money who are just go ready to go build data centers and get into the game. So what we see is an opportunity to kind of bring what I'll describe simplistically as neocloud and where we bring a storage platform, we bring an inferencing engine, we bring a cloud control plane, logging, monitoring, telemetry, handful of managed services all together so that they can go and deploy in their newly built data center on commodity infrastructure and start generating revenue without needing to build out a lot of this stuff themselves. So I'll say that's where we're pushing towards. Thank you. Got it. Bringing the high hyperscalers with the neoclouts together and offering the full suite of capabilities that a neocloud needs to get to market quickly. Got it. Is it fair to assume that WASD is probably the target for most of the training data that these inference engines or manufacturers use to train their LLMs and things like that? Well, certainly, you know, a cohort of our customers are training focused. You know, where our largest customers are at the moment, like, you know, openly, we're the storage platform underneath XAI and underneath Tesla and underneath X itself. And so, you know, they're our largest customer at the moment, and they leverage us for checkpointing, for training data, and actually model distribution and data pipelines for Spark, open source Spark. So, like, we're the storage platform underneath all of that. for Elon's companies. Got it. We're also the same for CoreWeave. So CoreWeave, I'm sure you're intimately familiar with what CoreWeave is up to. When you go and you get storage from CoreWeave's managed service, even if it's S3 storage or if it's NFS storage, underneath that is the Vast data platform. So they are just reselling our software and running it on their hardware. And that's sort of our two frontier largest customers at the moment. Thank you. We have a long tail of customers that are more on-premises traditional scenarios. We run in a lot of the hedge funds. So Hudson River Trading Company, those kinds of environments where they're doing large quant work or they're doing Monte Carlo simulations, they have big HPC clusters. We have a bunch of the supercomputers and the national labs as customers. We have federal agencies as customers that are doing the same. But you could think about it as high performance, GPU direct, NFS storage is probably the front. That makes sense, makes sense. I think I had calibrated my thinking correctly. Yeah. But, you know, like I said, value proposition here is to work up the stack and get, you know, the NeoClouds a full set of capabilities to get to market quickly. And you might have seen like the CoreWeave deal that we did was kind of revolutionary. I think the announcements of that came out a couple of months ago. But it's a $1.2 billion deal with Vast to deliver that infrastructure for them. Got it, got it. I mean, we are a level up. What we basically want to do is bring trust. security, verifiability, observability. And given the platform capabilities that we have, given that we are a distributed ledger that is globally started, there's a lot of other capabilities that we can do. I mean, I have a roadmap for the next three years in terms of how this could evolve. Like we can, our platform has the ability for one to kind of issue stable coins on top of. So the world is moving towards agent e-commerce. And I think those two worlds will kind of come together and with our platform you have the ability to do that and bring our agent SDK along with the ability to kind of you know deliver a stablecoin infrastructure on top of this bring them together and usher an agent e-commerce with all the trust security and verifiability that one would need so there's a lot of stuff that one could do with our platform but whatever I showed you is what our initial go-to-market is Where I see the interest is, you know, I've, if you're managing agents in a distributed environment and you've got, you know, this kind of global state scenario I described where customers' data sits in the hyperscalers and there's GPU infrastructure out on the edge across the NeoClouds or wherever they're getting it, and you've connected them all together, you've got a lot of data. Thank you. with VAST and we are managing agent execution on GPUs wherever the GPUs are available, having that global blockchain ledger notion of what executed where and when is useful and not something we've got in our current roadmap. I think that's probably the thing that's most interesting and enlightening to me is that opportunity to provide that provable chain of trust around what executed when and where within our ecosystem. So that is somewhat interesting. - Makes sense. I think I have some... If you don't mind me asking if there's anything that you could share with me. If it is publicly available, I'm happy to just look at it. As to how What you just mentioned, I could let my imagination grow wild and see what we could potentially do, especially given what you do. Yeah, I... I forgot. Yeah, let me call the email with a couple of public links. Thank you. I mean, you know, the vision around where we're going here is what I'm describing, not necessarily what we fully shipped. We just released our 5.4, which has some of this in it. But, you know, there's a lot of development in flight at the moment around the global execution piece. Right now, we've kind of got baseline event-based agentic execution flows. But getting to the global distributed agentic management, you know, we're looking towards, Thank you. Thank you. Thank you. taking over and actually running GPU nodes ourselves. Today we run on CPU only and so all of that is still in flight. Today what we end up doing is taking and executing via NIMS and running on APIs based on what we want to go do for inferencing. So today that's where we're at and where we're headed and there's a bunch of gaps but yeah I'll show you some links on what we've got today. Thank you. I would really appreciate that. Then I would love to stay in touch. I think I can try to figure out how this could... I could even paint a picture of how this could potentially even be stitched together, if that's okay with you. And we can take it from there. Yeah, for sure. Let's keep the dialogue going. I'll send you a note and let me know what you think. Great. Nice meeting you, Avanash. Bye. Thanks a lot, Jason. I really appreciate your time. Same here. Bye.