INGESTION PIPELINE REVIEW BUNDLE (TRIMMED_FULL)
Generated: 2026-01-05T21:12:50
Source bundle: Workflow/review_bundles/ingest_pipeline_review_bundle_2026-01-05_205124.txt

NOTES:
- Dropped design/docs (UNIFIED-PIPELINE.md, INSTRUMENTATION.md) and requirements.
- Dropped deprecated wrappers and non-CLI scripts not reachable from ingest paths (manifest_sync, enrich_customer).
- Dropped utils not imported by unified pipeline (utils/entities.py, utils/profiles.py, utils/git_ops.py).
- Kept prompt assets + templates + config.yaml for runtime behavior review.

CONTENTS:
001. [CONFIG] Workflow/config.yaml  â€”  Runtime paths + model policy (model selection, temperatures)
002. [CLI] Workflow/scripts/ingest.py  â€”  Unified ingest CLI entry point (routes to pipeline, enrichment, git commit)
003. [PIPELINE] Workflow/pipeline/__init__.py  â€”  Unified pipeline module
004. [PIPELINE/ADAPTER] Workflow/pipeline/adapters/__init__.py  â€”  Content adapter (parse â†’ ContentEnvelope)
005. [PIPELINE/ADAPTER] Workflow/pipeline/adapters/base.py  â€”  Content adapter (parse â†’ ContentEnvelope)
006. [PIPELINE/ADAPTER] Workflow/pipeline/adapters/document.py  â€”  Content adapter (parse â†’ ContentEnvelope)
007. [PIPELINE/ADAPTER] Workflow/pipeline/adapters/email.py  â€”  Content adapter (parse â†’ ContentEnvelope)
008. [PIPELINE/ADAPTER] Workflow/pipeline/adapters/transcript.py  â€”  Content adapter (parse â†’ ContentEnvelope)
009. [PIPELINE] Workflow/pipeline/apply.py  â€”  Unified pipeline module
010. [PIPELINE] Workflow/pipeline/context.py  â€”  Unified pipeline module
011. [PIPELINE] Workflow/pipeline/entities.py  â€”  Unified pipeline module
012. [PIPELINE] Workflow/pipeline/envelope.py  â€”  Unified pipeline module
013. [PIPELINE] Workflow/pipeline/extract.py  â€”  Unified pipeline module
014. [PIPELINE] Workflow/pipeline/models.py  â€”  Unified pipeline module
015. [PIPELINE] Workflow/pipeline/outputs.py  â€”  Unified pipeline module
016. [PIPELINE] Workflow/pipeline/patch.py  â€”  Unified pipeline module
017. [PIPELINE] Workflow/pipeline/pipeline.py  â€”  Unified pipeline module
018. [ENRICH] Workflow/scripts/enrich_person.py  â€”  People enrichment (includes web search caching)
019. [NORMALIZE] Workflow/scripts/normalize_entity_notes.py  â€”  Post-apply normalization (frontmatter normalization)
020. [NORMALIZE] Workflow/scripts/normalize_note_headers.py  â€”  Post-apply normalization (header cleanup)
021. [NORMALIZE] Workflow/scripts/remove_empty_entity_links.py  â€”  Cleanup helper (removes placeholder entity links)
022. [UTILS] Workflow/scripts/utils/__init__.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
023. [UTILS] Workflow/scripts/utils/ai_client.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
024. [UTILS] Workflow/scripts/utils/cached_prompts.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
025. [UTILS] Workflow/scripts/utils/config.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
026. [UTILS] Workflow/scripts/utils/frontmatter.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
027. [UTILS] Workflow/scripts/utils/fs.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
028. [UTILS] Workflow/scripts/utils/logging.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
029. [UTILS] Workflow/scripts/utils/patch_primitives.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
030. [UTILS] Workflow/scripts/utils/paths.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
031. [UTILS] Workflow/scripts/utils/templates.py  â€”  Shared utilities (config, AI client, templates, logging, git ops)
032. [PROMPTS] Workflow/prompts/persona.md  â€”  Persona context for extraction prompt (ContextBundle)
033. [PROMPTS] Workflow/profiles/jason_persona.yaml  â€”  Drafting persona used by OutputGenerator reply drafting
034. [PROMPTS] Workflow/entities/aliases.yaml  â€”  Name alias canonicalization for entity resolution
035. [PROMPTS] Workflow/entities/glossary.yaml  â€”  Glossary of terms/acronyms for context
036. [TEMPLATES] Workflow/templates/customer.md.j2  â€”  Jinja template used for note/README generation
037. [TEMPLATES] Workflow/templates/journal.md.j2  â€”  Jinja template used for note/README generation
038. [TEMPLATES] Workflow/templates/partners.md.j2  â€”  Jinja template used for note/README generation
039. [TEMPLATES] Workflow/templates/people.md.j2  â€”  Jinja template used for note/README generation
040. [TEMPLATES] Workflow/templates/projects.md.j2  â€”  Jinja template used for note/README generation
041. [TEMPLATES] Workflow/templates/rob.md.j2  â€”  Jinja template used for note/README generation

========================================================================================================================

========================================================================================================================
GROUP: CONFIG
PATH: Workflow/config.yaml
ROLE: Runtime paths + model policy (model selection, temperatures)
========================================================================================================================

# ============================================================================
# Notes Vault Configuration
# ============================================================================
# Runtime settings for the local-first Obsidian automation system.
# 
# Architecture: Extract â†’ Plan â†’ Apply (ChangePlan pattern)
# See DESIGN.md for full system documentation.
#
# Environment variables (from .env) override values here using ${VAR} syntax.
# Relative paths are resolved from vault_root.
#
# Version: 0.2.0
# Last Updated: 2026-01-05
# ============================================================================

# === PATHS ===
# All paths relative to vault_root unless absolute
paths:
  vault_root: "${NOTES_ROOT:-/Users/jason/Documents/Notes}"
  
  # Inbox structure (incoming content)
  inbox:
    root: "Inbox"
    email: "Inbox/Email"
    transcripts: "Inbox/Transcripts"
    voice: "Inbox/Voice"
    attachments: "Inbox/Attachments"
    extraction: "Inbox/_extraction"      # JSON output from AI extraction
    archive: "Inbox/_archive"            # Processed source files
  
  # Resources (prompts, templates) - relative to Workflow/
  resources:
    prompts: "Workflow/prompts"
    templates: "Workflow/templates"
    profiles: "Workflow/profiles"
    entities: "Workflow/entities"
  
  # Work destinations (VAST)
  work:
    root: "VAST"
    people: "VAST/People"
    projects: "VAST/Projects"
    accounts: "VAST/Customers and Partners"
    rob: "VAST/ROB"
    tasks: "TASKS.md"                 # Tasks dashboard (root)
    journal: "VAST/Journal"
  
  # Personal destinations
  personal:
    root: "Personal"
    people: "Personal/People"
    projects: "Personal/Projects"
    tasks: "TASKS.md"                 # Tasks dashboard (root)
    journal: "Personal/Journal"
  
  # Sources (for reprocessing)
  sources:
    root: "Sources"
    email: "Sources/Email"
    transcripts: "Sources/Transcripts"
    documents: "Sources/Documents"
    voice: "Sources/Voice"

# === AI MODELS ===
# Policy-based model selection (not hardcoded)
# See REQUIREMENTS.md Section 4 for model policy details
#
# CRITICAL: All API calls must use:
#   - OpenAI Responses API (not Chat Completions)
#   - store: false (privacy requirement)
#
# NOTE: max_tokens is intentionally omitted - let the model determine output length.
# GPT-5.2 handles variable-length structured outputs better without artificial constraints.

models:
  default_provider: "openai"

  privacy:
    store: false
    api: "responses"

  # Model selection: Use GPT-5.2 for all operations
  # Updated 2026-01-05: Standardized on GPT-5.2 across all operations
  # CRITICAL: No fallbacks - if config is wrong, fail loudly
  
  # NOTE: Classification uses heuristics (scripts/classify.py), not LLM.
  # This config is reserved for future LLM-based classification if needed.
  classify:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # Currently unused - classification is heuristic-based

  extract_transcript:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # No max_tokens - transcripts can be long

  extract_email:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # No max_tokens

  extract_voice:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # No max_tokens

  planning:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # No max_tokens - plans can be complex
  
  # Backfill: Historical content extraction for README population
  backfill:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # No max_tokens
  
  # Extraction: Used by draft_responses.py and other extraction tasks
  extraction:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
  
  # Enrichment: Person and customer enrichment from README content (L2)
  enrichment:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
  
  # Web Enrichment: Web search enrichment (L3) using Responses API
  web_enrichment:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
    # Uses Responses API with web_search_preview tool
  
  # Entity Discovery: AI-powered entity discovery
  entity_discovery:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
  
  # Manifest Sync: AI enrichment for manifest entries
  manifest_enrichment:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.0
  
  # Draft Responses: Email response generation
  draft_responses:
    provider: "openai"
    model: "gpt-5.2"
    temperature: 0.3  # Slightly creative for responses
  
  # Conflict resolution only
  # If you want â€œbest possibleâ€ and youâ€™re willing to switch stacks: use OpenAI Codex with gpt-5.2-codex.
  # If staying in Copilot: pick Claude Opus 4.5 explicitly.
  agent:
    provider: "copilot"
    preference: "claude-opus-4.5"
    scope: "conflict-resolution-only"

  embeddings:
    provider: "openai"
    model: "text-embedding-3-large"



# === NOTE TYPES ===
# Maps note types to their destinations and templates
note_types:
  customer:
    template: "customer.md"
    destinations:
      - "VAST/Customers and Partners/{entity}"
    entity_key: "account"
  
  people:
    template: "people.md"
    destinations:
      - "VAST/People/{entity}"
      - "Personal/People/{entity}"
    entity_key: "person"
  
  projects:
    template: "projects.md"
    destinations:
      - "VAST/Projects/{entity}"
      - "Personal/Projects/{entity}"
    entity_key: "project"
  
  rob:
    template: "rob.md"
    destinations:
      - "VAST/ROB/{entity}"
    entity_key: "rob_forum"
  
  journal:
    template: "journal.md"
    destinations:
      - "Personal/Journal"
      - "VAST/Journal"
    entity_key: "journal"
  
  partners:
    template: "partners.md"
    destinations:
      - "VAST/Customers and Partners/{entity}"
    entity_key: "partner"
  
  travel:
    template: "travel.md"
    destinations:
      - "Personal/Travel"
      - "VAST/Travel"
    entity_key: "trip"

# === PROCESSING RULES ===
# ChangePlan pattern: Extract â†’ Plan â†’ Apply
# See DESIGN.md Section 2 for full pattern documentation
processing:
  # Pipeline phases
  phases:
    extract: true                        # Content â†’ extraction.json
    plan: true                           # extraction.json â†’ changeplan.json
    apply: true                          # changeplan.json â†’ file updates
  
  # Execution mode
  headless: true                         # No Obsidian/plugin dependencies
  template_engine: "jinja2"              # NOT Templater plugin
  
  # ChangePlan validation
  validate_schema: true                  # Validate against JSON schema
  schema_path: "Workflow/schemas/changeplan.schema.json"
  
  # Source file handling
  auto_archive: true                     # Move processed files to archive
  archive_date_folders: true             # Organize archive by YYYY-MM-DD
  save_partial_extractions: true         # Save JSON even if errors occur
  
  # File age limits
  max_file_age_days: 30                  # Skip files older than this
  
  # Duplicate detection
  check_duplicates: true                 # Warn if similar content exists
  duplicate_threshold: 0.85              # Similarity threshold (0-1)
  
  # Retry behavior
  max_retries: 3                         # Retry failed API calls
  retry_delay_seconds: 2                 # Delay between retries
  
  # Dry run support
  dry_run: false                         # Set true to preview without writing

# === ENTITY MATCHING ===
# How to match names/references to existing folders
entity_matching:
  # Strategy: exact, fuzzy, embeddings
  strategy: "fuzzy"
  
  # Fuzzy matching threshold (0-1, higher = stricter)
  fuzzy_threshold: 0.8
  
  # Create new entity folders automatically?
  auto_create_entities: false            # If false, flag for manual review
  
  # Alias file for nicknames/abbreviations
  alias_file: "Workflow/entities/aliases.yaml"
  
  # Known entities cache (auto-generated)
  cache_file: "Workflow/entities/cache.json"

# === TASK MANAGEMENT ===
# Obsidian Tasks plugin format
# IMPORTANT: Tasks live in their source notes; `TASKS.md` is a query-only dashboard.
tasks:
  # Storage policy
  storage:
    location: "source-notes-only"        # Tasks stay where they're created
    dashboard: "TASKS.md"                # Vault-relative path to the Tasks-plugin dashboard
    dashboards: "tasks-plugin-queries"   # Dashboard file contains Tasks queries only
    no_duplication: true                 # Never copy tasks into aggregate lists
  
  # Obsidian Tasks plugin format
  format:
    priority_markers:
      highest: "ðŸ”º"
      high: "â«"
      medium: "ðŸ”¼"
      low: "ðŸ”½"
      lowest: "â¬"
    date_markers:
      due: "ðŸ“…"
      scheduled: "â³"
      start: "ðŸ›«"
      recurrence: "ðŸ”"
      done: "âœ…"
  
  # Default owner for first-person tasks
  default_owner: "Myself"
  
  # Global tag for all extracted tasks
  global_tag: "#task"

# === GIT INTEGRATION ===
git:
  enabled: true
  auto_commit: true
  auto_push: false                       # Require manual push
  commit_message_prefix: "[auto]"
  
  # Commit strategy: per-file, per-batch, daily
  commit_strategy: "per-batch"
  
  # Include source file path in commit message
  include_source_in_message: true
  
  # Clean check configuration
  # Only check these paths for uncommitted changes (prevents false positives)
  check_paths:
    - "TASKS.md"
    - "Inbox/"
    - "VAST/"
    - "Personal/"
  
  # Ignore these paths when checking for clean state
  # (Obsidian config changes, logs, venv shouldn't block automation)
  ignore_paths:
    - ".obsidian/"
    - "Workflow/logs/"
    - "Workflow/.venv/"

# === LOGGING ===
logging:
  level: "INFO"                          # DEBUG, INFO, WARNING, ERROR
  console: true                          # Print to console
  file: "Workflow/logs/automation.log"
  
  # Log rotation
  max_size_mb: 10
  backup_count: 5
  
  # Separate logs for different components
  components:
    extraction: "Workflow/logs/extraction.log"
    agent: "Workflow/logs/agent.log"
    errors: "Workflow/logs/errors.log"

# === NOTIFICATIONS ===
notifications:
  enabled: true
  
  # macOS notifications via osascript
  macos:
    on_success: true
    on_error: true
    sound: true
  
  # Future: Slack, email, etc.
  # slack:
  #   webhook_url: "${SLACK_WEBHOOK_URL}"
  #   on_error: true

# === EMAIL INGESTION ===
# Critical: Save raw .eml for HTML fidelity, not just AppleScript content
email:
  # Source preservation
  save_raw_eml: true                     # Export actual .eml source
  save_markdown: true                    # Also save readable .md conversion
  eml_export_method: "applescript_source"  # Use 'source' property, not 'content'
  
  # Do NOT rely on:
  # - AppleScript 'content' property (lossy)
  # - echo commands (encoding issues)
  # - Pandoc HTML conversion without source

# === SCHEDULING ===
# Daily Loop workflow (see DESIGN.md Section 5)
scheduling:
  # End-of-day processing trigger
  workflow: "manual"                     # Manual trigger, not scheduled
  
  # Future: launchd job times (24h format)
  # process_times:
  #   - "18:00"                          # 6 PM daily
  
  # Days to run (0=Sunday, 6=Saturday)
  # process_days: [1, 2, 3, 4, 5]        # Weekdays only
  
  # Timezone
  timezone: "America/Denver"

# === SENSITIVE CONTENT ===
# Flag content matching these patterns for manual review
sensitive_content:
  enabled: true
  
  # Keywords that trigger manual review
  keywords:
    - "salary"
    - "compensation"
    - "confidential"
    - "NDA"
    - "legal"
    - "lawsuit"
    - "medical"
    - "health"
  
  # Regex patterns (optional)
  patterns:
    - "\\$[0-9,]+[kKmM]?"                # Dollar amounts
    - "SSN|social security"              # SSN references

# === PARALLEL PROCESSING ===
# Concurrent extraction with deferred patch application
parallel:
  enabled: true                          # Enable parallel extraction
  max_workers: 5                         # Concurrent LLM calls (respect rate limits)
  
  # Rate limit protection
  rate_limit:
    requests_per_minute: 50              # Stay under RPM limit (typically 60-500)
    tokens_per_minute: 500000            # Stay under TPM limit
    backoff_on_429: true                 # Auto-retry with exponential backoff
    max_retries: 3                       # Max retries per request
  
  # Patch collection strategy
  patch_strategy: "collect-then-merge"   # collect-then-merge | immediate
  
  # Merge behavior
  merge:
    dedupe_facts: true                   # Remove duplicate facts
    dedupe_tasks: true                   # Remove duplicate tasks
    combine_context: true                # Combine context strings

# === FEATURE FLAGS ===
# Enable/disable experimental features
features:
  # Entity matching with embeddings
  use_embeddings: false
  
  # Multi-part transcript merging
  merge_transcripts: false
  
  # Backfill web enrichment (costly; requires explicit enable)
  backfill_web_enrichment: false

  # Weekly digest generation
  weekly_digest: false
  
  # Calendar integration for meeting context
  calendar_integration: false

========================================================================================================================

========================================================================================================================
GROUP: CLI
PATH: Workflow/scripts/ingest.py
ROLE: Unified ingest CLI entry point (routes to pipeline, enrichment, git commit)
========================================================================================================================

#!/usr/bin/env python3
"""
Unified Ingest CLI - Process all content types through unified pipeline.

Usage:
    python ingest.py --all                        # Process all pending Inbox content
    python ingest.py --type email                 # Process only emails
    python ingest.py --type transcript            # Process only transcripts
    python ingest.py --file path/to/file          # Process single file
    python ingest.py --source --type email --force  # Re-process archived sources
    python ingest.py --dry-run                    # Preview without changes
    python ingest.py --verbose                    # Show extraction details
    python ingest.py --enrich                     # Trigger enrichment for new entities
    python ingest.py --draft-replies              # Generate draft email replies
    python ingest.py --draft-replies --draft-all-emails  # Draft every email (including no-reply/automated)
    python ingest.py --trace-dir /tmp/traces      # Save extraction/changeplan artifacts
    python ingest.py --all --show-cache-stats     # Print cache + timing summary
    python ingest.py --workers 5                  # Use 5 parallel workers
    python ingest.py --sequential                 # Force sequential (1 worker)
"""

import sys
from pathlib import Path
from datetime import datetime

import click
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from pipeline import UnifiedPipeline
from pipeline.envelope import ContentType
from scripts.utils.config import load_config


console = Console()


@click.command()
@click.option("--type", "content_type", type=click.Choice(["email", "transcript", "document", "voice", "all"]), default="all", help="Content type to process")
@click.option("--file", "file_path", type=click.Path(), help="Process single file")
@click.option("--dry-run", is_flag=True, help="Preview without making changes")
@click.option("--verbose", "-v", is_flag=True, help="Show extraction details")
@click.option("--enrich", is_flag=True, help="Trigger enrichment for new entities")
@click.option("--draft-replies", is_flag=True, help="Generate draft email replies")
@click.option("--draft-all-emails", is_flag=True, help="Generate drafts for all emails (including no-reply/automated)")
@click.option("--source", is_flag=True, help="Re-process from Sources/ directory")
@click.option("--force", is_flag=True, help="Skip duplicate detection, reprocess even if already extracted")
@click.option("--show-cache-stats", is_flag=True, help="Print cache + timing summary after run")
@click.option("--trace-dir", type=click.Path(), help="Persist extraction/changeplan artifacts to this directory")
@click.option("--vault-root", type=click.Path(), help="Override vault root (defaults to repo root)")
@click.option("--workers", "-w", type=int, default=None, help="Number of parallel workers (default from config, 1=sequential)")
@click.option("--sequential", is_flag=True, help="Force sequential processing (ignore config)")
def main(content_type: str, file_path: str, dry_run: bool, verbose: bool, enrich: bool, draft_replies: bool, draft_all_emails: bool, source: bool, force: bool, show_cache_stats: bool, trace_dir: str, vault_root: str, workers: int, sequential: bool):
    """Unified content ingest pipeline.
    
    Processes emails, transcripts, documents, and voice memos through a unified
    extraction and patching pipeline.
    """
    
    override_root = Path(vault_root).expanduser().resolve() if vault_root else None
    try:
        config = load_config(vault_root_override=override_root)
    except Exception as exc:
        raise click.ClickException(f"Config error: {exc}")
    
    vault_root_path = Path(config.get("paths", {}).get("vault_root", Path(__file__).parent.parent.parent))
    type_map = {
        "email": ContentType.EMAIL,
        "transcript": ContentType.TRANSCRIPT,
        "document": ContentType.DOCUMENT,
        "voice": ContentType.VOICE,
    }
    
    # Determine worker count: --sequential forces 1, --workers N overrides config
    effective_workers = 1 if sequential else workers

    subtitle_parts = ["DRY RUN" if dry_run else "LIVE"]
    if effective_workers != 1:
        subtitle_parts.append(f"{effective_workers or 'config'} workers")
    if draft_replies:
        subtitle_parts.append("draftsâ†’Outbox")
    if draft_all_emails:
        subtitle_parts.append("all-email-drafts")
    
    console.print(Panel.fit(
        "[bold blue]Unified Ingest Pipeline[/bold blue]",
        subtitle=" | ".join(subtitle_parts)
    ))
    
    # Initialize pipeline
    pipeline = UnifiedPipeline(
        vault_root=vault_root_path,
        dry_run=dry_run,
        verbose=verbose,
        generate_outputs=draft_replies,
        draft_all_emails=draft_all_emails,
        force=force,
        trace_dir=Path(trace_dir) if trace_dir else None,
        show_cache_stats=show_cache_stats,
        config=config,
        max_workers=effective_workers,
    )
    
    batch = None
    single_result = None

    # Process based on options
    if file_path:
        # Single file
        target_path = Path(file_path)
        if not target_path.is_absolute():
            target_path = vault_root_path / target_path
        if not target_path.exists():
            raise click.ClickException(f"File not found: {target_path}")
        result = pipeline.process_file(target_path)
        _display_result(result, verbose)
        single_result = result
        
    elif source:
        # Re-process already archived sources
        selected = type_map.get(content_type) if content_type != "all" else None
        batch = pipeline.process_sources(selected)
        _display_batch(batch, verbose)
    elif content_type == "all":
        # All content types from Inbox
        batch = pipeline.process_all()
        _display_batch(batch, verbose)
    else:
        # Specific content type from Inbox
        selected = type_map[content_type]
        batch = pipeline.process_type(selected)
        _display_batch(batch, verbose)
    
    if (show_cache_stats or verbose) and batch:
        _print_batch_metrics(batch)
    if (show_cache_stats or verbose) and single_result:
        _print_result_metrics(single_result)
    
    # Run enrichment if requested
    if enrich and not dry_run:
        _run_enrichment(vault_root_path, verbose)
    
    # Git commit if not dry run
    if not dry_run:
        _git_commit(vault_root_path)


def _display_result(result, verbose: bool):
    """Display single processing result."""
    if result.success:
        console.print(f"[green]âœ“[/green] {result.source_path}")
        
        if verbose and result.extraction:
            console.print(f"  Type: {result.content_type}")
            console.print(f"  Summary: {result.extraction.get('summary', '')[:80]}...")
            console.print(f"  Facts: {len(result.extraction.get('facts', []))}")
            console.print(f"  Tasks: {len(result.extraction.get('tasks', []))}")
        
        if result.apply_result:
            console.print(f"  Created: {len(result.apply_result.files_created)}")
            console.print(f"  Modified: {len(result.apply_result.files_modified)}")
        
        if result.draft_reply:
            console.print(f"  [blue]Draft reply[/blue]: {result.draft_reply}")
        if result.calendar_invite and result.calendar_invite.get("path"):
            console.print(f"  [blue]Calendar invite[/blue]: {result.calendar_invite.get('path')}")
        if verbose and getattr(result, "outputs", None):
            tasks_emitted = (result.outputs or {}).get("tasks_emitted")
            if tasks_emitted:
                console.print(f"  Tasks emitted: {tasks_emitted}")
    else:
        console.print(f"[red]âœ—[/red] {result.source_path}")
        for error in result.errors:
            console.print(f"  [red]{error}[/red]")


def _display_batch(batch, verbose: bool):
    """Display batch processing results."""
    
    # Summary table
    table = Table(title="Processing Summary")
    table.add_column("Metric", style="cyan")
    table.add_column("Count", justify="right")
    
    table.add_row("Total", str(batch.total))
    table.add_row("Success", f"[green]{batch.success}[/green]")
    table.add_row("Failed", f"[red]{batch.failed}[/red]" if batch.failed else "0")
    table.add_row("Skipped", f"[yellow]{batch.skipped}[/yellow]" if batch.skipped else "0")

    drafts = sum(1 for r in batch.results if getattr(r, "draft_reply", None))
    if drafts:
        table.add_row("Draft replies", f"[blue]{drafts}[/blue]")
    
    console.print(table)

    if drafts:
        console.print("[blue]Drafts written to: Outbox/[/blue]")
    
    # Individual results
    if verbose or batch.failed > 0:
        console.print("\n[bold]Details:[/bold]")
        for result in batch.results:
            _display_result(result, verbose)


def _print_batch_metrics(batch):
    """Print cache + timing summary for batch runs."""
    metrics = getattr(batch, "metrics", {}) or {}
    if not metrics:
        return
    
    console.print("\n[bold]Run Summary[/bold]")
    console.print(
        f"Duration: {metrics.get('run_ms', 0)} ms | Files: {batch.total} "
        f"(success {batch.success}, failed {batch.failed}, skipped {batch.skipped})"
    )
    
    timings = metrics.get("phase_ms_avg", {}) or {}
    if timings:
        table = Table(title="Avg Phase Timings (ms)")
        table.add_column("Phase")
        table.add_column("ms", justify="right")
        for phase, ms in sorted(timings.items()):
            label = phase.replace("_ms", "")
            table.add_row(label, str(ms))
        console.print(table)
    
    cache = metrics.get("cache", {}) or {}
    if cache.get("calls"):
        hit_rate = cache.get("hit_rate", 0)
        console.print(
            f"Cache: {cache.get('hits', 0)}/{cache.get('calls', 0)} hits "
            f"({hit_rate:.0f}%), saved {cache.get('cached_tokens', 0)} tokens "
            f"of {cache.get('prompt_tokens', 0)} prompt tokens"
        )


def _print_result_metrics(result):
    """Print cache + timing summary for single-file runs."""
    metrics = getattr(result, "metrics", {}) or {}
    if not metrics:
        return
    
    console.print("\n[bold]Run Summary (single file)[/bold]")
    timings = metrics.get("timings", {}) or {}
    if timings:
        table = Table(title="Phase Timings (ms)")
        table.add_column("Phase")
        table.add_column("ms", justify="right")
        for phase, ms in sorted(timings.items()):
            label = phase.replace("_ms", "")
            table.add_row(label, str(ms))
        console.print(table)
    
    cache = metrics.get("cache", {}) or {}
    if cache:
        hit_text = "hit" if cache.get("cache_hit") else "miss"
        console.print(
            f"Cache {hit_text}: "
            f"{cache.get('cached_tokens', 0)}/{cache.get('prompt_tokens', 0)} prompt tokens "
            f"saved, latency={cache.get('latency_ms', 0)} ms"
        )


def _run_enrichment(vault_root: Path, verbose: bool):
    """Run enrichment for newly created entities."""
    console.print("\n[bold]Running enrichment...[/bold]")
    
    # Import enrichment module
    try:
        from scripts.enrich_person import enrich_sparse_people
        
        # Find and enrich sparse people
        count = enrich_sparse_people(vault_root, level=2, limit=5, verbose=verbose)
        console.print(f"  Enriched {count} people")
    except ImportError:
        console.print("  [yellow]Enrichment module not available[/yellow]")
    except Exception as e:
        console.print(f"  [red]Enrichment failed: {e}[/red]")


def _git_commit(vault_root: Path):
    """Commit changes to git."""
    import subprocess
    
    try:
        # Check for changes
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            cwd=vault_root,
            capture_output=True,
            text=True
        )
        
        if not result.stdout.strip():
            return  # No changes
        
        # Stage and commit
        subprocess.run(["git", "add", "-A"], cwd=vault_root, check=True)
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        message = f"[auto] Unified ingest: {timestamp}"
        
        subprocess.run(
            ["git", "commit", "-m", message],
            cwd=vault_root,
            check=True,
            capture_output=True
        )
        
        console.print(f"\n[green]Committed changes[/green]")
        
    except subprocess.CalledProcessError as e:
        console.print(f"[yellow]Git commit skipped: {e}[/yellow]")


if __name__ == "__main__":
    main()

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/__init__.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Unified Content Pipeline

This package provides a unified ETL pipeline for processing various content types
(email, transcript, document, voice) into the Obsidian vault.

Architecture:
    adapters/   - Content type adapters (normalize to ContentEnvelope)
    context.py  - Context loading (manifests, persona, glossary)
    extract.py  - Unified extraction (LLM + schema)
    patch.py    - Patch generation (deterministic)
    enrich.py   - Enrichment triggers
    outputs.py  - Output generation (drafts, tasks)
    apply.py    - Transactional apply

Usage:
    from pipeline import UnifiedPipeline
    
    pipeline = UnifiedPipeline()
    result = pipeline.process("Inbox/Email/2026-01-04_*.md")
"""

from .envelope import ContentEnvelope, ContentType
from .context import ContextBundle
from .entities import EntityIndex
from .extract import UnifiedExtractor
from .patch import PatchGenerator
from .apply import TransactionalApply
from .outputs import OutputGenerator
from .pipeline import UnifiedPipeline

__all__ = [
    "ContentEnvelope",
    "ContentType",
    "ContextBundle",
    "EntityIndex",
    "UnifiedExtractor",
    "PatchGenerator",
    "TransactionalApply",
    "UnifiedPipeline",
]

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE/ADAPTER
PATH: Workflow/pipeline/adapters/__init__.py
ROLE: Content adapter (parse â†’ ContentEnvelope)
========================================================================================================================

"""
Content Adapters - Parse raw content into ContentEnvelope.

Each content type has an adapter that:
1. Detects if a file matches the content type
2. Parses metadata from the content
3. Returns a ContentEnvelope for unified processing
"""

from .base import BaseAdapter, AdapterRegistry
from .email import EmailAdapter
from .transcript import TranscriptAdapter
from .document import DocumentAdapter

__all__ = [
    "BaseAdapter",
    "AdapterRegistry",
    "EmailAdapter",
    "TranscriptAdapter",
    "DocumentAdapter",
]

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE/ADAPTER
PATH: Workflow/pipeline/adapters/base.py
ROLE: Content adapter (parse â†’ ContentEnvelope)
========================================================================================================================

"""
Base Adapter - Abstract interface for content adapters.
"""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional, Type
import hashlib

from ..envelope import ContentEnvelope, ContentType


class BaseAdapter(ABC):
    """Abstract base class for content adapters.
    
    Each adapter knows how to:
    1. Detect if a file matches its content type
    2. Parse the file into a ContentEnvelope
    """
    
    @property
    @abstractmethod
    def content_type(self) -> ContentType:
        """The content type this adapter handles."""
        pass
    
    @abstractmethod
    def can_handle(self, path: Path) -> bool:
        """Check if this adapter can handle the given file."""
        pass
    
    @abstractmethod
    def parse(self, path: Path) -> ContentEnvelope:
        """Parse the file into a ContentEnvelope."""
        pass
    
    def compute_hash(self, content: str) -> str:
        """Compute content hash for deduplication."""
        # Strip frontmatter
        if content.startswith("---"):
            end = content.find("\n---", 3)
            if end != -1:
                content = content[end + 4:]
        
        # Normalize and hash first 2000 chars
        normalized = content.strip()[:2000]
        return hashlib.md5(normalized.encode()).hexdigest()[:12]


class AdapterRegistry:
    """Registry of content adapters.
    
    Tries adapters in order to find one that can handle each file.
    """
    
    def __init__(self):
        self._adapters: list[BaseAdapter] = []
    
    def register(self, adapter: BaseAdapter) -> None:
        """Register an adapter."""
        self._adapters.append(adapter)
    
    def get_adapter(self, path: Path) -> Optional[BaseAdapter]:
        """Find an adapter that can handle the given file."""
        for adapter in self._adapters:
            if adapter.can_handle(path):
                return adapter
        return None
    
    def parse(self, path: Path) -> Optional[ContentEnvelope]:
        """Parse a file using the appropriate adapter."""
        adapter = self.get_adapter(path)
        if adapter:
            return adapter.parse(path)
        return None
    
    @classmethod
    def default(cls) -> "AdapterRegistry":
        """Create registry with all default adapters."""
        from .email import EmailAdapter
        from .transcript import TranscriptAdapter
        from .document import DocumentAdapter
        
        registry = cls()
        registry.register(EmailAdapter())
        registry.register(TranscriptAdapter())
        registry.register(DocumentAdapter())
        return registry

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE/ADAPTER
PATH: Workflow/pipeline/adapters/document.py
ROLE: Content adapter (parse â†’ ContentEnvelope)
========================================================================================================================

"""
Document Adapter - Parse documents and articles into ContentEnvelope.

Handles general documents dropped into the vault.
"""

import re
from datetime import datetime
from pathlib import Path
from typing import Optional

from .base import BaseAdapter
from ..envelope import ContentEnvelope, ContentType, DocumentMetadata


class DocumentAdapter(BaseAdapter):
    """Parse general documents and articles."""
    
    @property
    def content_type(self) -> ContentType:
        return ContentType.DOCUMENT
    
    def can_handle(self, path: Path) -> bool:
        """Detect if this is a document file.
        
        Documents are:
        - In Inbox/Attachments/ directory
        - Markdown files not matching email/transcript patterns
        - PDF/DOCX files (future: after text extraction)
        """
        if not path.is_file():
            return False
        
        # Check path
        if "Attachments" in path.parts:
            return path.suffix in [".md", ".txt"]
        
        # Default handler for markdown files
        if path.suffix == ".md":
            return True
        
        return False
    
    def parse(self, path: Path) -> ContentEnvelope:
        """Parse document file into ContentEnvelope."""
        content = path.read_text()
        
        # Extract metadata
        title = self._extract_title(content, path.name)
        date = self._extract_date(content, path)
        author = self._extract_author(content)
        doc_type = self._infer_document_type(content, path)
        
        return ContentEnvelope(
            source_path=path,
            content_type=ContentType.DOCUMENT,
            raw_content=content,
            date=date,
            title=title,
            participants=[author] if author else [],
            content_hash=self.compute_hash(content),
            metadata={
                "document": DocumentMetadata(
                    document_type=doc_type,
                    author=author,
                    file_type=path.suffix[1:] if path.suffix else "unknown",
                ).model_dump()
            }
        )
    
    def _extract_title(self, content: str, filename: str) -> str:
        """Extract title from document."""
        # Check for H1 heading
        lines = content.split("\n")
        for line in lines[:10]:
            if line.startswith("# "):
                return line[2:].strip()
        
        # Fall back to filename
        return filename.replace(".md", "").replace("_", " ").strip()
    
    def _extract_date(self, content: str, path: Path) -> str:
        """Extract date from document."""
        # Check frontmatter
        if content.startswith("---"):
            date_match = re.search(r"date:\s*[\"']?(\d{4}-\d{2}-\d{2})", content[:500])
            if date_match:
                return date_match.group(1)
        
        # Check filename
        date_match = re.match(r"^(\d{4}-\d{2}-\d{2})", path.name)
        if date_match:
            return date_match.group(1)
        
        # Use file modification time
        mtime = path.stat().st_mtime
        return datetime.fromtimestamp(mtime).strftime("%Y-%m-%d")
    
    def _extract_author(self, content: str) -> Optional[str]:
        """Extract author from document."""
        # Check frontmatter
        if content.startswith("---"):
            author_match = re.search(r"author:\s*[\"']?([^\n\"']+)", content[:500])
            if author_match:
                return author_match.group(1).strip()
        
        return None
    
    def _infer_document_type(self, content: str, path: Path) -> str:
        """Infer document type from content and path."""
        content_lower = content.lower()
        
        if "proposal" in content_lower or "proposal" in path.name.lower():
            return "proposal"
        if "spec" in content_lower or "specification" in content_lower:
            return "specification"
        if "report" in content_lower:
            return "report"
        if "article" in path.parts or "articles" in path.parts:
            return "article"
        
        return "general"

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE/ADAPTER
PATH: Workflow/pipeline/adapters/email.py
ROLE: Content adapter (parse â†’ ContentEnvelope)
========================================================================================================================

"""
Email Adapter - Parse exported emails into ContentEnvelope.

Handles emails exported from Apple Mail (markdown format).
"""

import re
from datetime import datetime
from pathlib import Path
from typing import Optional

from .base import BaseAdapter
from ..envelope import ContentEnvelope, ContentType, EmailMetadata


class EmailAdapter(BaseAdapter):
    """Parse emails exported from Apple Mail."""
    
    @property
    def content_type(self) -> ContentType:
        return ContentType.EMAIL
    
    def can_handle(self, path: Path) -> bool:
        """Detect if this is an email file.
        
        Email files are:
        - In Inbox/Email/ directory
        - Named with pattern: YYYY-MM-DD_HHMMSS_NNNN_Subject.md
        """
        if not path.is_file() or path.suffix != ".md":
            return False
        
        # Check path
        if "Email" in path.parts:
            return True
        
        # Check filename pattern
        if re.match(r"^\d{4}-\d{2}-\d{2}_\d{6}_\d{4}_", path.name):
            return True
        
        return False
    
    def parse(self, path: Path) -> ContentEnvelope:
        """Parse email file into ContentEnvelope."""
        content = path.read_text()
        
        # Extract metadata
        date = self._extract_date(content, path.name)
        subject = self._extract_subject(content)
        sender_name, sender_email = self._extract_sender(content)
        recipients_detail = self._extract_recipients(content)
        recipient_names = [r.get("name") for r in recipients_detail if r.get("name")]
        recipient_emails = [r.get("email") for r in recipients_detail if r.get("email")]
        is_reply = self._is_reply(subject)
        
        # Build participants list
        participants = []
        if sender_name:
            participants.append(sender_name)
        participants.extend(recipient_names)
        
        return ContentEnvelope(
            source_path=path,
            content_type=ContentType.EMAIL,
            raw_content=content,
            date=date,
            title=subject,
            participants=participants,
            content_hash=self.compute_hash(content),
            metadata={
                "email": EmailMetadata(
                    sender_name=sender_name,
                    sender_email=sender_email,
                    recipients=recipient_names,
                    recipients_emails=recipient_emails,
                    recipients_detail=recipients_detail,
                    subject=subject,
                    is_reply=is_reply,
                ).model_dump()
            }
        )
    
    def _extract_date(self, content: str, filename: str) -> str:
        """Extract date from email content or filename."""
        # Try content (## YYYY-MM-DD format)
        date_match = re.search(r"##\s*(\d{4}-\d{2}-\d{2})", content)
        if date_match:
            return date_match.group(1)
        
        # Try filename (YYYY-MM-DD_HHMMSS_...)
        filename_match = re.match(r"^(\d{4}-\d{2}-\d{2})", filename)
        if filename_match:
            return filename_match.group(1)
        
        return datetime.now().strftime("%Y-%m-%d")
    
    def _extract_subject(self, content: str) -> str:
        """Extract subject from email content."""
        lines = content.split("\n")
        if lines and lines[0].startswith("# "):
            return lines[0][2:].strip()
        return "Unknown Subject"
    
    def _extract_sender(self, content: str) -> tuple[Optional[str], Optional[str]]:
        """Extract sender name and email from email content."""
        # Look for "From:" line
        from_match = re.search(r"(?:^|\n)\*?\*?From\*?\*?:\s*([^\n<]+?)(?:\s*<([^>]+)>)?(?:\n|$)", content)
        if from_match:
            name = from_match.group(1).strip()
            # Strip markdown bold markers
            name = re.sub(r'^\*\*\s*|\s*\*\*$', '', name)
            email = from_match.group(2) if from_match.group(2) else None
            return (name, email)
        return (None, None)
    
    def _extract_recipients(self, content: str) -> list[dict]:
        """Extract recipient names + emails from email content."""
        recipients: list[dict] = []
        
        # Look for "To:" line
        to_match = re.search(r"(?:^|\n)\*?\*?To\*?\*?:\s*([^\n]+)", content)
        if to_match:
            to_line = to_match.group(1)
            # Parse comma-separated names
            for part in to_line.split(","):
                name = None
                email = None
                name_match = re.match(r"([^<]+?)(?:\s*<([^>]+)>)?$", part.strip())
                if name_match:
                    name = name_match.group(1).strip()
                    email = name_match.group(2).strip() if name_match.group(2) else None
                    # Strip markdown bold markers
                    name = re.sub(r'^\*\*\s*|\s*\*\*$', '', name)
                if name or email:
                    recipients.append({"name": name, "email": email})
        
        return recipients
    
    def _is_reply(self, subject: str) -> bool:
        """Check if this is a reply email."""
        return subject.lower().startswith(("re:", "re[", "fwd:", "fw:"))

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE/ADAPTER
PATH: Workflow/pipeline/adapters/transcript.py
ROLE: Content adapter (parse â†’ ContentEnvelope)
========================================================================================================================

"""
Transcript Adapter - Parse meeting transcripts into ContentEnvelope.

Handles MacWhisper transcripts with speaker diarization.
"""

import re
from datetime import datetime
from pathlib import Path

from .base import BaseAdapter
from ..envelope import ContentEnvelope, ContentType, TranscriptMetadata


class TranscriptAdapter(BaseAdapter):
    """Parse MacWhisper transcripts."""
    
    @property
    def content_type(self) -> ContentType:
        return ContentType.TRANSCRIPT
    
    def can_handle(self, path: Path) -> bool:
        """Detect if this is a transcript file.
        
        Transcript files are:
        - In Inbox/Transcripts/ or Sources/Transcripts/ directory
        - Named with pattern: YYYY-MM-DD HH MM - Title.md
        - Or contain speaker labels (Speaker 1:, [Speaker]:, etc.)
        """
        if not path.is_file() or path.suffix != ".md":
            return False
        
        # Check path
        if "Transcripts" in path.parts:
            return True
        
        # Check filename pattern (date + time)
        if re.match(r"^\d{4}-\d{2}-\d{2}\s+\d{2}[\s:]\d{2}", path.name):
            return True
        
        # Check for speaker labels in content
        try:
            content = path.read_text()[:1000]
            if re.search(r"(?:Speaker \d+:|^\[[^\]]+\]:)", content, re.MULTILINE):
                return True
        except Exception:
            pass
        
        return False
    
    def parse(self, path: Path) -> ContentEnvelope:
        """Parse transcript file into ContentEnvelope."""
        content = path.read_text()
        
        # Extract metadata
        date = self._extract_date_from_filename(path.name)
        title = self._extract_title_from_filename(path.name)
        speakers = self._extract_speakers(content)
        
        return ContentEnvelope(
            source_path=path,
            content_type=ContentType.TRANSCRIPT,
            raw_content=content,
            date=date,
            title=title,
            participants=speakers,
            content_hash=self.compute_hash(content),
            metadata={
                "transcript": TranscriptMetadata(
                    speakers=speakers,
                    source_app="MacWhisper",
                    has_diarization=len(speakers) > 0,
                ).model_dump()
            }
        )
    
    def _extract_date_from_filename(self, filename: str) -> str:
        """Extract date from transcript filename."""
        # Patterns: "2025-12-15 16 10 - Title.md" or "2025-12-15 - Title.md"
        date_match = re.match(r"^(\d{4}-\d{2}-\d{2})", filename)
        if date_match:
            return date_match.group(1)
        return datetime.now().strftime("%Y-%m-%d")
    
    def _extract_title_from_filename(self, filename: str) -> str:
        """Extract title from transcript filename."""
        # Remove date and time patterns
        title = re.sub(r"^\d{4}-\d{2}-\d{2}\s*\d*:?\d*\s*-?\s*", "", filename)
        title = title.replace(".md", "").strip()
        
        # Handle emoji titles (e.g., "G24 Flight School ðŸ§‘â€ðŸš€:  VAST Story")
        title = re.sub(r"\s+", " ", title)
        
        return title if title else "Meeting"
    
    def _extract_speakers(self, content: str) -> list[str]:
        """Extract speaker names from transcript.
        
        Handles speaker mapping lines like:
        - "Speaker 1: Jason Vallery" (header definition - standalone line at top)
        - "Speaker 1: Hello there..." (dialogue line - has more content)
        - "[Name]: text" (inline speech)
        - "**Name:** text" (MacWhisper with names)
        """
        speakers = set()
        speaker_map: dict[str, str] = {}  # e.g., "Speaker 1" -> "Jason Vallery"
        
        lines = content.split("\n")
        
        # First pass: Look for speaker definitions in header area
        # These are short lines that look like name mappings, not dialogue
        # Pattern: "Speaker 1: Jason Vallery" where the value is just a name (2-5 words, no punctuation)
        for line in lines[:20]:  # Check first 20 lines for definitions
            mapping_match = re.match(r"^(Speaker \d+):\s*(.+)$", line.strip())
            if mapping_match:
                speaker_label = mapping_match.group(1)
                speaker_value = mapping_match.group(2).strip()
                # It's a name mapping if: short (< 50 chars), 2-5 words (real names are 2+ words), no sentence punctuation
                words = speaker_value.split()
                is_name = (
                    len(speaker_value) < 50 
                    and 2 <= len(words) <= 5  # Require at least 2 words (first + last name)
                    and not any(punct in speaker_value for punct in '.!?')
                    and not speaker_value.lower().startswith(('i ', "i'", 'we ', 'you ', 'the ', 'a '))
                )
                if is_name:
                    speaker_map[speaker_label] = speaker_value
        
        # Second pass: Collect all speakers
        # Pattern 1: "Speaker 1:" in dialogue
        for match in re.finditer(r"(Speaker \d+):", content):
            label = match.group(1)
            # Use mapped name if available, otherwise use the speaker label
            if label in speaker_map:
                speakers.add(speaker_map[label])
            else:
                speakers.add(label)
        
        # Pattern 2: "[Name]:" format
        for match in re.finditer(r"\[([^\]]+)\]:", content):
            name = match.group(1).strip()
            if name and len(name) < 50:  # Sanity check
                speakers.add(name)
        
        # Pattern 3: "**Name:** text" format (MacWhisper with names)
        for match in re.finditer(r"\*\*([^*]+)\*\*:", content):
            name = match.group(1).strip()
            if name and len(name) < 50:
                speakers.add(name)
        
        return list(speakers)

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/apply.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Transactional Apply - Execute change plans atomically.

Features:
- Backup before modification
- Rollback on failure
- Archive sources after success
- Git commit with summary
"""

import shutil
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

from .patch import ChangePlan, PatchOperation, ManifestPatch


class ApplyResult:
    """Result of applying a change plan."""
    
    def __init__(self):
        self.success = True
        self.files_created: list[str] = []
        self.files_modified: list[str] = []
        self.files_archived: list[str] = []
        self.errors: list[str] = []
    
    def __str__(self):
        if self.success:
            return f"Applied: {len(self.files_created)} created, {len(self.files_modified)} modified"
        else:
            return f"Failed: {', '.join(self.errors)}"


class TransactionalApply:
    """Apply change plans transactionally.
    
    Either all changes succeed (git commit) or all fail (rollback).
    """
    
    def __init__(self, vault_root: Path, dry_run: bool = False):
        self.vault_root = vault_root
        self.dry_run = dry_run
        # Use microseconds to avoid collisions when multiple applies run concurrently.
        self.backup_dir = vault_root / ".workflow_backups" / datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        self._backed_up: dict[Path, Path] = {}
        self._created: list[Path] = []
    
    def apply(
        self,
        plan: ChangePlan,
        source_path: Optional[Path] = None,
        extra_meeting_notes: Optional[list[tuple[str, dict]]] = None,
        extra_source_paths: Optional[list[Path]] = None,
    ) -> ApplyResult:
        """Apply a change plan.
        
        Args:
            plan: ChangePlan to apply
            source_path: Original source file (for archiving)
            extra_meeting_notes: Additional meeting notes to create atomically
            extra_source_paths: Additional source files to archive atomically
        
        Returns:
            ApplyResult with details
        """
        result = ApplyResult()
        
        if self.dry_run:
            return self._dry_run_apply(plan, result, extra_meeting_notes=extra_meeting_notes, extra_source_paths=extra_source_paths)
        
        try:
            # 1. Create meeting note
            if plan.meeting_note_path and plan.meeting_note:
                self._create_meeting_note(plan.meeting_note_path, plan.meeting_note, result)
            if extra_meeting_notes:
                for note_path, note_context in extra_meeting_notes:
                    if note_path and note_context:
                        self._create_meeting_note(note_path, note_context, result)
            
            # 2. Apply patches
            for patch in plan.patches:
                self._apply_patch(patch, result)
            
            # 3. Apply manifest patches (aliases and acronyms)
            for manifest_patch in plan.manifest_patches:
                self._apply_manifest_patch(manifest_patch, result)

            # 3.5 Post-apply normalization (idempotent cleanup on touched notes)
            self._post_apply_normalize(result)
            
            # 4. Archive source
            sources_to_archive: list[Path] = []
            if source_path:
                sources_to_archive.append(source_path)
            if extra_source_paths:
                sources_to_archive.extend(extra_source_paths)
            for src in sources_to_archive:
                if src and src.exists():
                    self._archive_source(src, result)
            
            # 5. Cleanup backups on success
            if self.backup_dir.exists():
                shutil.rmtree(self.backup_dir)
            
            return result
            
        except Exception as e:
            result.success = False
            result.errors.append(str(e))
            self._rollback()
            return result
    
    def _dry_run_apply(
        self,
        plan: ChangePlan,
        result: ApplyResult,
        extra_meeting_notes: Optional[list[tuple[str, dict]]] = None,
        extra_source_paths: Optional[list[Path]] = None,
    ) -> ApplyResult:
        """Simulate applying a plan (dry run)."""
        
        if plan.meeting_note_path:
            result.files_created.append(plan.meeting_note_path)
        if extra_meeting_notes:
            for note_path, _ctx in extra_meeting_notes:
                if note_path:
                    result.files_created.append(note_path)
        
        for patch in plan.patches:
            result.files_modified.append(patch.target_path)
        
        for manifest_patch in plan.manifest_patches:
            result.files_modified.append(manifest_patch.manifest_path)

        sources_to_archive: list[Path] = []
        if extra_source_paths:
            sources_to_archive.extend(extra_source_paths)
        for src in sources_to_archive:
            if src:
                result.files_archived.append(str(src))
        
        return result
    
    def _create_meeting_note(self, note_path_rel: str, note_context: dict, result: ApplyResult):
        """Create the meeting note from plan."""
        from jinja2 import Environment, FileSystemLoader
        import re
        import os
        
        note_path = self.vault_root / note_path_rel
        
        # Skip if exists
        if note_path.exists():
            return
        
        # Ensure parent directory exists
        note_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Load template
        templates_dir = self.vault_root / "Workflow" / "templates"
        env = Environment(loader=FileSystemLoader(str(templates_dir)))
        
        # Add custom filters
        def slugify(text):
            """Convert text to slug format for tags."""
            if not text:
                return ""
            text = text.lower()
            text = re.sub(r'[^\w\s-]', '', text)
            text = re.sub(r'[\s_]+', '-', text)
            return text.strip('-')
        
        def strip_extension(path):
            """Remove file extension from path."""
            if not path:
                return ""
            return os.path.splitext(str(path))[0]
        
        def basename(path):
            """Get basename of path."""
            if not path:
                return ""
            return os.path.basename(str(path))
        
        env.filters['slugify'] = slugify
        env.filters['strip_extension'] = strip_extension
        env.filters['basename'] = basename
        
        # Select template based on note type
        note_type = note_context.get("type", "people")
        template_name = f"{note_type}.md.j2"
        
        try:
            template = env.get_template(template_name)
        except Exception:
            template = env.get_template("people.md.j2")  # Fallback
        
        # Render
        content = template.render(**note_context)
        
        # Write
        note_path.write_text(content)
        self._created.append(note_path)
        result.files_created.append(note_path_rel)
    
    def _apply_patch(self, patch: PatchOperation, result: ApplyResult):
        """Apply a single patch operation."""
        target = self.vault_root / patch.target_path
        
        if not target.exists():
            return
        
        # Backup
        self._backup(target)
        
        # Read current content
        content = target.read_text()
        
        # Apply patch primitives
        from scripts.utils.patch_primitives import (
            upsert_frontmatter, 
            append_under_heading,
            ensure_wikilinks
        )
        
        # Frontmatter updates - convert dict to list of {key, value} patches
        if patch.add_frontmatter:
            fm_patches = [{"key": k, "value": v} for k, v in patch.add_frontmatter.items()]
            content = upsert_frontmatter(content, fm_patches)
        
        # Add facts under ## Key Facts
        if patch.add_facts:
            for fact in patch.add_facts:
                if fact not in content:
                    content = append_under_heading(content, "## Key Facts", f"- {fact}")
        
        # Add topics under ## Topics
        if patch.add_topics:
            for topic in patch.add_topics:
                if topic not in content:
                    content = append_under_heading(content, "## Topics", f"- {topic}")
        
        # Add decisions under ## Key Decisions
        if patch.add_decisions:
            for decision in patch.add_decisions:
                if decision not in content:
                    content = append_under_heading(content, "## Key Decisions", f"- {decision}")
        
        # Add context under ## Recent Context
        if patch.add_context:
            if patch.add_context not in content:
                content = append_under_heading(content, "## Recent Context", patch.add_context)
        
        # Add wikilinks
        if patch.add_wikilinks:
            content = ensure_wikilinks(content, patch.add_wikilinks)
        
        # Write
        self._atomic_write(target, content)
        result.files_modified.append(patch.target_path)
    
    def _apply_manifest_patch(self, patch: ManifestPatch, result: ApplyResult):
        """Apply a manifest patch (add aliases or acronyms).
        
        For People manifest: Adds aliases to the Aliases column
        For Projects manifest: Adds acronym/definition to appropriate columns
        """
        target = self.vault_root / patch.manifest_path
        
        if not target.exists():
            return
        
        # Backup
        self._backup(target)
        
        # Read current content
        content = target.read_text()
        lines = content.split("\n")
        
        if patch.manifest_type == "people" and patch.person_name:
            # Find the row for this person and update Aliases column
            lines = self._update_people_manifest_row(lines, patch.person_name, patch.aliases_to_add)
        elif patch.manifest_type == "projects" and patch.project_name:
            # Find or add project row with acronym/definition
            lines = self._update_projects_manifest_row(lines, patch.project_name, patch.acronym, patch.definition)
        
        # Write
        new_content = "\n".join(lines)
        self._atomic_write(target, new_content)
        result.files_modified.append(patch.manifest_path)
    
    def _update_people_manifest_row(self, lines: list[str], person_name: str, aliases_to_add: list[str]) -> list[str]:
        """Update a row in the People manifest to add aliases."""
        # Find header row to get column indices
        header_idx = None
        alias_col_idx = None
        
        for i, line in enumerate(lines):
            if line.strip().startswith("|") and "Name" in line and "Role" in line:
                header_idx = i
                cols = [c.strip() for c in line.split("|")]
                for j, col in enumerate(cols):
                    if "Alias" in col:
                        alias_col_idx = j
                        break
                break
        
        if header_idx is None or alias_col_idx is None:
            return lines  # Can't find manifest structure
        
        # Find the person's row
        person_name_lower = person_name.lower()
        for i in range(header_idx + 2, len(lines)):  # Skip header and separator
            line = lines[i]
            if not line.strip().startswith("|"):
                continue
            
            cols = line.split("|")
            if len(cols) <= alias_col_idx:
                continue
            
            # Check if this row matches the person
            name_col = cols[1].strip() if len(cols) > 1 else ""
            if name_col.lower() == person_name_lower or person_name_lower in name_col.lower():
                # Found the person - update aliases
                current_aliases = cols[alias_col_idx].strip() if alias_col_idx < len(cols) else ""
                
                # Parse existing aliases
                parts = [a.strip() for a in re.split(r"[;,]", current_aliases or "") if a.strip()]
                existing = set(parts)
                
                # Add new aliases
                existing.update(aliases_to_add)
                
                # Update the column
                new_aliases = "; ".join(sorted(existing))
                cols[alias_col_idx] = f" {new_aliases} "
                
                lines[i] = "|".join(cols)
                break
        
        return lines
    
    def _update_projects_manifest_row(self, lines: list[str], project_name: str, acronym: str, definition: str) -> list[str]:
        """Update or add a row in the Projects manifest with acronym/definition."""
        # Find header row to get column indices
        header_idx = None
        acronym_col_idx = None
        definition_col_idx = None
        
        for i, line in enumerate(lines):
            if line.strip().startswith("|") and "Name" in line:
                header_idx = i
                cols = [c.strip() for c in line.split("|")]
                for j, col in enumerate(cols):
                    if "Acronym" in col:
                        acronym_col_idx = j
                    if "Definition" in col:
                        definition_col_idx = j
                break
        
        if header_idx is None:
            return lines  # Can't find manifest structure
        
        # Find the project's row
        project_name_lower = project_name.lower()
        for i in range(header_idx + 2, len(lines)):  # Skip header and separator
            line = lines[i]
            if not line.strip().startswith("|"):
                continue
            
            cols = line.split("|")
            if len(cols) < 2:
                continue
            
            # Check if this row matches the project
            name_col = cols[1].strip() if len(cols) > 1 else ""
            if name_col.lower() == project_name_lower or project_name_lower in name_col.lower():
                # Found the project - update acronym and definition
                if acronym_col_idx and acronym_col_idx < len(cols):
                    current = cols[acronym_col_idx].strip()
                    if not current or current == "-":
                        cols[acronym_col_idx] = f" {acronym} "
                
                if definition_col_idx and definition_col_idx < len(cols):
                    current = cols[definition_col_idx].strip()
                    if not current or current == "-":
                        cols[definition_col_idx] = f" {definition} "
                
                lines[i] = "|".join(cols)
                return lines
        
        # Project not found - could add a new row, but for now just skip
        # (would need to know all column values to add properly)
        return lines

    def _post_apply_normalize(self, result: ApplyResult) -> None:
        """Run post-import normalization helpers on notes touched by this apply."""
        from scripts.normalize_entity_notes import normalize_frontmatter_dict
        from scripts.normalize_note_headers import normalize_body_header
        from scripts.utils.frontmatter import parse_frontmatter, render_frontmatter

        created_files = set(self._created)
        touched = {*(result.files_created or []), *(result.files_modified or [])}

        for rel_str in sorted(touched):
            if not rel_str:
                continue
            rel_path = Path(rel_str)
            if rel_path.suffix.lower() != ".md":
                continue
            if rel_path.name == "README.md":
                continue
            if rel_path.name.startswith("_") or rel_path.name.endswith("_MANIFEST.md"):
                continue

            scope = self._infer_entity_note_scope(rel_path)
            if not scope:
                continue

            entity_name, entity_key, note_type, header_label = scope
            abs_path = self.vault_root / rel_path
            if not abs_path.exists():
                continue

            text = abs_path.read_text(errors="ignore")
            fm, body = parse_frontmatter(text)
            if fm is None:
                fm = {}
                body = text

            existing_type = str(fm.get("type") or "").strip().strip('"').strip("'").lower()
            if note_type == "customer" and existing_type == "partners":
                # Don't normalize partner notes into customer notes.
                continue

            normalized_fm = normalize_frontmatter_dict(
                fm,
                entity_key=entity_key,
                entity_name=entity_name,
                note_type=note_type,
            )
            updated_body = body
            if header_label:
                updated_body = normalize_body_header(
                    updated_body,
                    header_label=header_label,
                    entity_name=entity_name,
                )

            updated = render_frontmatter(normalized_fm) + updated_body
            if updated == text:
                continue

            # Backup existing files modified again during post-normalization.
            if abs_path not in created_files and abs_path not in self._backed_up:
                self._backup(abs_path)

            self._atomic_write(abs_path, updated)
            if rel_str not in result.files_modified and rel_str not in result.files_created:
                result.files_modified.append(rel_str)

    def _infer_entity_note_scope(
        self, rel_path: Path
    ) -> Optional[tuple[str, str, str, Optional[str]]]:
        """Infer (entity_name, entity_key, note_type, header_label) for an entity note."""
        parts = rel_path.parts
        if len(parts) < 4:
            return None

        if parts[0] == "VAST" and parts[1] == "People":
            return parts[2], "person", "people", None
        if parts[0] == "Personal" and parts[1] == "People":
            return parts[2], "person", "people", None

        if parts[0] == "VAST" and parts[1] == "Projects":
            return parts[2], "project", "projects", "Project"
        if parts[0] == "Personal" and parts[1] == "Projects":
            return parts[2], "project", "projects", "Project"

        if parts[0] == "VAST" and parts[1] == "Customers and Partners":
            return parts[2], "account", "customer", "Account"

        if parts[0] == "VAST" and parts[1] == "ROB":
            return parts[2], "rob_forum", "rob", "Forum"

        return None

    def _archive_source(self, source_path: Path, result: ApplyResult):
        """Archive source file to Sources directory."""
        from .envelope import ContentType
        
        try:
            relative = source_path.relative_to(self.vault_root)
        except ValueError:
            relative = None

        # Already archived; no-op but record for reporting
        if relative and "Sources" in relative.parts:
            result.files_archived.append(str(relative))
            return

        # Determine archive location
        year = datetime.now().strftime("%Y")
        
        # Detect content type from path
        if "Email" in source_path.parts:
            archive_dir = self.vault_root / "Sources" / "Email" / year
        elif "Transcripts" in source_path.parts:
            archive_dir = self.vault_root / "Sources" / "Transcripts" / year
        else:
            archive_dir = self.vault_root / "Sources" / "Documents" / year
        
        archive_dir.mkdir(parents=True, exist_ok=True)
        archive_path = archive_dir / source_path.name
        
        # Move to archive
        shutil.move(str(source_path), str(archive_path))
        result.files_archived.append(str(archive_path.relative_to(self.vault_root)))
    
    def _backup(self, path: Path):
        """Backup a file before modification."""
        if path in self._backed_up:
            return
        
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        try:
            rel = path.relative_to(self.vault_root)
        except ValueError:
            rel = Path(path.name)
        backup_path = self.backup_dir / rel
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(path, backup_path)
        self._backed_up[path] = backup_path
    
    def _atomic_write(self, path: Path, content: str):
        """Write file atomically (write temp, then rename)."""
        temp_path = path.with_suffix(path.suffix + ".tmp")
        temp_path.write_text(content)
        temp_path.rename(path)
    
    def _rollback(self):
        """Rollback all changes on failure."""
        # Restore backups
        for original, backup in self._backed_up.items():
            if backup.exists():
                shutil.copy2(backup, original)
        
        # Delete created files
        for created in self._created:
            if created.exists():
                created.unlink()
        
        # Cleanup backup directory
        if self.backup_dir.exists():
            shutil.rmtree(self.backup_dir)

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/context.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Context Bundle - Load context for extraction and output generation.

The ContextBundle provides:
- Persona (my role, priorities, communication style)
- Entity manifests (compact lists of known people/companies/projects)
- Glossary (acronyms, terms)
- Aliases (name normalization)
- Relevant entity READMEs (for entities mentioned in content)

PROMPT CACHING:
OpenAI caches prompt prefixes that are 1024+ tokens and identical across calls.
The ContextBundle structures prompts with static content FIRST (cacheable):
  1. Persona (identity, style, rules)
  2. Entity glossary (people, companies, projects)
  3. Aliases and acronyms

Followed by dynamic content (per-call):
  4. Relevant READMEs for mentioned entities
  5. Source-specific instructions
"""

import sys
import hashlib
from pathlib import Path
from typing import Optional, Tuple, Any
from functools import lru_cache
from pydantic import BaseModel, Field, ConfigDict

sys.path.insert(0, str(Path(__file__).parent.parent))

from .envelope import ContentEnvelope
from .entities import EntityIndex
from scripts.utils.config import load_config

# Optional cached prompt helpers (keeps persona/glossary in cache-friendly format)
try:  # pragma: no cover - helper is optional in runtime
    from scripts.utils.cached_prompts import (  # type: ignore
        get_persona_context as _cached_persona_context,
        get_glossary_context as _cached_glossary_context,
    )
except Exception:  # pragma: no cover
    _cached_persona_context = None
    _cached_glossary_context = None


class ContextBundle(BaseModel):
    """All context needed for extraction and output generation."""
    
    model_config = ConfigDict(extra="ignore")
    
    # Static context (loaded once per session)
    persona: str = ""
    people_manifest: str = ""
    company_manifest: str = ""
    project_manifest: str = ""  # Projects manifest with acronyms/definitions
    project_list: list[str] = Field(default_factory=list)
    glossary: dict[str, str] = Field(default_factory=dict)
    aliases: dict[str, str] = Field(default_factory=dict)
    
    # Dynamic context (per content item)
    relevant_readmes: dict[str, str] = Field(default_factory=dict)
    
    @classmethod
    def load(cls, vault_root: Path, envelope: Optional[ContentEnvelope] = None, entity_index: Optional[EntityIndex] = None, config: Optional[dict[str, Any]] = None) -> "ContextBundle":
        """Load context for extraction.
        
        Args:
            vault_root: Path to vault root
            envelope: Optional ContentEnvelope to load relevant READMEs for
        
        Returns:
            ContextBundle with all context loaded
        """
        bundle = cls()
        cfg = config or load_config(vault_root_override=vault_root)
        paths_cfg = cfg.get("paths", {})
        work_paths = paths_cfg.get("work", {})
        personal_paths = paths_cfg.get("personal", {})
        resources_paths = paths_cfg.get("resources", {})
        
        people_manifest_path = Path(work_paths.get("people", vault_root / "VAST" / "People")) / "_MANIFEST.md"
        company_manifest_path = Path(work_paths.get("accounts", vault_root / "VAST" / "Customers and Partners")) / "_MANIFEST.md"
        project_manifest_path = Path(work_paths.get("projects", vault_root / "VAST" / "Projects")) / "_MANIFEST.md"
        project_paths = [
            Path(work_paths.get("projects", vault_root / "VAST" / "Projects")),
            Path(personal_paths.get("projects", vault_root / "Personal" / "Projects")),
        ]
        
        index = entity_index or EntityIndex(vault_root, config=cfg)
        
        # Load static context
        bundle.persona = _load_persona(vault_root, resources_paths)
        bundle.people_manifest = _load_manifest(people_manifest_path)
        bundle.company_manifest = _load_manifest(company_manifest_path)
        bundle.project_manifest = _load_manifest(project_manifest_path)
        bundle.project_list = _list_projects(project_paths)
        
        # Load glossary from YAML file (legacy) and merge with project manifest acronyms
        bundle.glossary = _load_glossary(vault_root, resources_paths)
        project_acronyms = _extract_acronyms_from_manifest(bundle.project_manifest)
        bundle.glossary.update(project_acronyms)
        
        # Load aliases from YAML file and merge with manifest Aliases column
        bundle.aliases = _load_aliases(vault_root, resources_paths)
        manifest_aliases = _extract_aliases_from_manifest(bundle.people_manifest)
        bundle.aliases.update(manifest_aliases)
        
        # Load dynamic context if envelope provided
        if envelope:
            mentioned = set(_quick_entity_scan(envelope.raw_content, bundle))
            mentioned.update(envelope.participants or [])
            mentioned.update(_extract_candidate_names(envelope.raw_content))
            
            normalized_candidates = {index.normalize_name(name) for name in mentioned if name}
            enriched: set[str] = set()
            for name in normalized_candidates:
                # Try exact matches first
                folder = index.find_person(name) or index.find_company(name) or index.find_project(name)
                if folder:
                    enriched.add(folder.name)
                    continue
                
                # Fuzzy matches by entity type
                fuzzy_person = index.search_person(name, limit=1)
                if fuzzy_person:
                    enriched.add(fuzzy_person[0].name)
                    continue
                fuzzy_company = index.search_company(name, limit=1)
                if fuzzy_company:
                    enriched.add(fuzzy_company[0].name)
                    continue
                fuzzy_project = index.search_project(name, limit=1)
                if fuzzy_project:
                    enriched.add(fuzzy_project[0].name)
            
            enriched.update(normalized_candidates)
            bundle.relevant_readmes = _load_entity_readmes(list(enriched)[:12], vault_root, index)
        
        return bundle
    
    def get_cacheable_prefix(self) -> Tuple[str, str]:
        """Get the cacheable (static) portion of the prompt.
        
        Returns a tuple of (prefix, hash) where:
        - prefix: The static prompt content that should be cached
        - hash: A content hash to verify cache hits
        
        OpenAI caches prompt prefixes >= 1024 tokens that are identical.
        By separating static (persona + glossary) from dynamic (READMEs),
        we get cache hits across different extraction calls.
        """
        sections = []
        
        # 1. Persona (static)
        persona_text = None
        if _cached_persona_context:
            persona_text = _cached_persona_context(include_full=True)
        elif self.persona:
            persona_text = self.persona
        if persona_text:
            sections.append(f"## PERSONA\n{persona_text}")
        
        # 2. Entity glossary (static - changes only when manifests change)
        glossary = None
        if _cached_glossary_context:
            glossary = _cached_glossary_context(compact=True)
        else:
            glossary = self._format_compact_glossary()
        if glossary:
            sections.append(f"## ENTITY GLOSSARY\n{glossary}")
        
        # 3. Aliases (static)
        if self.aliases:
            alias_section = "## NAME ALIASES\n"
            alias_items = [f"- {k} â†’ {v}" for k, v in list(self.aliases.items())]
            alias_section += "\n".join(alias_items)
            sections.append(alias_section)
        
        # 4. Glossary terms (static)
        if self.glossary:
            terms_section = "## TERMS & ACRONYMS\n"
            terms_items = [f"- **{k}**: {v}" for k, v in list(self.glossary.items())]
            terms_section += "\n".join(terms_items)
            sections.append(terms_section)
        
        prefix = "\n\n".join(sections)
        
        # Generate hash for cache verification
        prefix_hash = hashlib.md5(prefix.encode()).hexdigest()[:8]
        
        return prefix, prefix_hash
    
    def get_dynamic_suffix(self) -> str:
        """Get the dynamic (per-call) portion of the prompt.
        
        This includes content-specific READMEs that vary per extraction.
        """
        if not self.relevant_readmes:
            return ""
        
        lines = ["## RELEVANT ENTITY CONTEXT"]
        for name, summary in self.relevant_readmes.items():
            lines.append(f"\n### {name}\n{summary}")
        
        return "\n".join(lines)

    def get_extraction_context(self, compact: bool = True, verbose: bool = False) -> str:
        """Format context for injection into extraction prompt.
        
        Args:
            compact: If True, use compact format for token efficiency
            verbose: If True, log cache info
        
        Returns:
            Formatted context string
        """
        # Get cacheable prefix and dynamic suffix
        prefix, prefix_hash = self.get_cacheable_prefix()
        suffix = self.get_dynamic_suffix()
        
        if verbose:
            prefix_tokens = len(prefix) // 4  # Rough estimate
            suffix_tokens = len(suffix) // 4
            print(f"  Context: prefix={prefix_tokens} tokens (hash:{prefix_hash}), suffix={suffix_tokens} tokens")
            if prefix_tokens >= 1024:
                print("  âœ“ Prefix eligible for caching (>= 1024 tokens)")
            else:
                print("  âš  Prefix too short for caching (< 1024 tokens)")
        
        # Combine: cacheable prefix first, then dynamic suffix
        if suffix:
            return f"{prefix}\n\n{suffix}"
        return prefix
    
    def _format_compact_glossary(self) -> str:
        """Format glossary in compact form for token efficiency.
        
        Includes relationship info when available to help the LLM:
        - Understand urgency (manager email > random contact)
        - Assign task owners appropriately
        - Craft appropriate tone
        """
        lines = []
        
        # People - include relationship if available
        if self.people_manifest:
            lines.append("**Known People:**")
            # Extract names with relationships from manifest table
            people_info = self._extract_people_with_relationships(self.people_manifest)
            if people_info:
                # Format: "Name (relationship)" or just "Name" if no relationship
                formatted = [
                    f"{name} ({rel})" if rel else name 
                    for name, rel in people_info[:80]
                ]
                lines.append(", ".join(formatted))
            lines.append("")
        
        # Companies - include my_role if available
        if self.company_manifest:
            lines.append("**Known Companies:**")
            company_info = self._extract_companies_with_roles(self.company_manifest)
            if company_info:
                formatted = [
                    f"{name} [{role}]" if role else name
                    for name, role in company_info[:40]
                ]
                lines.append(", ".join(formatted))
            lines.append("")
        
        # Projects (just names for now, could add my_role later)
        if self.project_list:
            lines.append("**Known Projects:**")
            lines.append(", ".join(self.project_list[:50]))
            lines.append("")
        
        # Aliases
        if self.aliases:
            lines.append("**Name Aliases:**")
            alias_items = [f"{k} â†’ {v}" for k, v in list(self.aliases.items())[:30]]
            lines.append(", ".join(alias_items))
        
        return "\n".join(lines)
    
    def _extract_people_with_relationships(self, manifest: str) -> list[tuple[str, str]]:
        """Extract people names and their relationship to me from manifest table."""
        import re
        results = []
        
        # Parse manifest table - find column indices first
        lines = manifest.split('\n')
        header_idx = -1
        rel_col_idx = -1
        
        for i, line in enumerate(lines):
            if '| Name |' in line:
                # Parse header to find My Relationship column
                headers = [h.strip() for h in line.split('|')]
                for j, h in enumerate(headers):
                    if 'My Relationship' in h:
                        rel_col_idx = j
                header_idx = i
                break
        
        if header_idx < 0:
            # Fallback to just names
            for match in re.finditer(r"\|\s*([^|\[]+)\s*\|", manifest):
                name = match.group(1).strip()
                if name and name not in ["Name", "---", ""]:
                    results.append((name, ""))
            return results[:80]
        
        # Parse data rows
        for line in lines[header_idx + 2:]:  # Skip header and separator
            if not line.strip() or not line.startswith('|'):
                continue
            cols = [c.strip() for c in line.split('|')]
            if len(cols) < 2:
                continue
            
            name = cols[1].strip()  # First data column is Name
            rel = ""
            if rel_col_idx > 0 and rel_col_idx < len(cols):
                rel = cols[rel_col_idx].strip()
            
            if name and name not in ["Name", "---"]:
                results.append((name, rel))
        
        return results
    
    def _extract_companies_with_roles(self, manifest: str) -> list[tuple[str, str]]:
        """Extract company names and my role from manifest table."""
        import re
        results = []
        
        lines = manifest.split('\n')
        header_idx = -1
        role_col_idx = -1
        stage_col_idx = -1
        type_col_idx = -1
        
        for i, line in enumerate(lines):
            if '| Name |' in line:
                headers = [h.strip() for h in line.split('|')]
                for j, h in enumerate(headers):
                    if 'My Role' in h:
                        role_col_idx = j
                    if 'Stage' in h:
                        stage_col_idx = j
                    if 'Type' in h:
                        type_col_idx = j
                header_idx = i
                break
        
        if header_idx < 0:
            return results
        
        for line in lines[header_idx + 2:]:
            if not line.strip() or not line.startswith('|'):
                continue
            cols = [c.strip() for c in line.split('|')]
            if len(cols) < 2:
                continue
            
            name = cols[1].strip()
            role = ""
            stage = ""
            account_type = ""
            if role_col_idx > 0 and role_col_idx < len(cols):
                role = cols[role_col_idx].strip()
            if stage_col_idx > 0 and stage_col_idx < len(cols):
                stage = cols[stage_col_idx].strip()
            if type_col_idx > 0 and type_col_idx < len(cols):
                account_type = cols[type_col_idx].strip()
            
            display_role = role or stage or account_type
            
            if name and name not in ["Name", "---"]:
                results.append((name, display_role))
        
        return results
    
    def _format_full_glossary(self) -> str:
        """Format full glossary with details."""
        return self.people_manifest + "\n" + self.company_manifest
    
    def _extract_names_from_manifest(self, manifest: str) -> list[str]:
        """Extract entity names from manifest table."""
        import re
        names = []
        
        # Match table rows: | Name | ... |
        for match in re.finditer(r"\|\s*\[\[([^\]|]+)", manifest):
            name = match.group(1).strip()
            if name and name not in ["Name", "---"]:
                names.append(name)
        
        return names


# =============================================================================
# LOADERS
# =============================================================================

def _load_persona(vault_root: Path, resources_paths: dict) -> str:
    """Load persona from prompts/persona.md."""
    prompts_root = Path(resources_paths.get("prompts", vault_root / "Workflow" / "prompts"))
    persona_path = prompts_root / "persona.md"
    if persona_path.exists():
        return persona_path.read_text()
    return ""


def _load_manifest(manifest_path: Path) -> str:
    """Load entity manifest file."""
    if manifest_path.exists():
        return manifest_path.read_text()
    return ""


def _list_projects(project_paths: list[Path]) -> list[str]:
    """List all project folder names."""
    names: list[str] = []
    for projects_dir in project_paths:
        if not projects_dir.exists():
            continue
        names.extend(
            folder.name for folder in projects_dir.iterdir()
            if folder.is_dir() and not folder.name.startswith("_")
        )
    return names


def _load_glossary(vault_root: Path, resources_paths: dict) -> dict[str, str]:
    """Load glossary of terms and acronyms."""
    glossary_root = Path(resources_paths.get("entities", vault_root / "Workflow" / "entities"))
    glossary_path = glossary_root / "glossary.yaml"
    if not glossary_path.exists():
        return {}
    
    try:
        import yaml
        return yaml.safe_load(glossary_path.read_text()) or {}
    except Exception:
        return {}


@lru_cache(maxsize=1)
def _load_aliases_cached(aliases_path: str) -> dict[str, str]:
    """Load aliases with caching."""
    path = Path(aliases_path)
    if not path.exists():
        return {}
    
    try:
        import yaml
        data = yaml.safe_load(path.read_text()) or {}
        
        # Flatten nested structure
        aliases = {}
        for canonical, variants in data.items():
            aliases[canonical.lower()] = canonical
            if isinstance(variants, list):
                for variant in variants:
                    aliases[variant.lower()] = canonical
            elif isinstance(variants, str):
                aliases[variants.lower()] = canonical
        
        return aliases
    except Exception:
        return {}


def _load_aliases(vault_root: Path, resources_paths: dict) -> dict[str, str]:
    """Load name aliases."""
    aliases_root = Path(resources_paths.get("entities", vault_root / "Workflow" / "entities"))
    aliases_path = aliases_root / "aliases.yaml"
    return _load_aliases_cached(str(aliases_path))


def _extract_aliases_from_manifest(manifest: str) -> dict[str, str]:
    """Extract aliases from People manifest Aliases column.
    
    The Aliases column contains semicolon-separated nicknames/variants.
    Returns a dict mapping each alias (lowercase) to the canonical name.
    
    Example row:
        | Jeff Denworth | ... | Jeff; JD; Jeff D | ... |
    Returns:
        {"jeff": "Jeff Denworth", "jd": "Jeff Denworth", "jeff d": "Jeff Denworth"}
    """
    import re
    aliases = {}
    
    lines = manifest.split('\n')
    header_idx = -1
    alias_col_idx = -1
    
    # Find header and Aliases column index
    for i, line in enumerate(lines):
        if '| Name |' in line:
            headers = [h.strip() for h in line.split('|')]
            for j, h in enumerate(headers):
                if h == 'Aliases':
                    alias_col_idx = j
            header_idx = i
            break
    
    if header_idx < 0 or alias_col_idx < 0:
        return aliases
    
    # Parse data rows
    for line in lines[header_idx + 2:]:  # Skip header and separator
        if not line.strip() or not line.startswith('|'):
            continue
        cols = [c.strip() for c in line.split('|')]
        if len(cols) < alias_col_idx + 1:
            continue
        
        name = cols[1].strip()  # First data column is Name
        alias_str = cols[alias_col_idx].strip() if alias_col_idx < len(cols) else ""
        
        if name and alias_str and name not in ["Name", "---"]:
            # Parse semicolon-separated aliases
            for alias in alias_str.split(';'):
                alias = alias.strip()
                if alias and alias.lower() != name.lower():
                    aliases[alias.lower()] = name
    
    return aliases


def _extract_acronyms_from_manifest(manifest: str) -> dict[str, str]:
    """Extract acronyms and definitions from Projects manifest.
    
    The Projects manifest has Acronym and Definition columns.
    Returns a dict mapping each acronym to its full name/definition.
    
    Example row:
        | Microsoft AI Infrastructure | ... | MAI | Microsoft's AI compute infra | ... |
    Returns:
        {"MAI": {"full_name": "Microsoft AI Infrastructure", "definition": "Microsoft's AI compute infra"}}
    """
    acronyms = {}
    
    lines = manifest.split('\n')
    header_idx = -1
    acronym_col_idx = -1
    definition_col_idx = -1
    
    # Find header and column indices
    for i, line in enumerate(lines):
        if '| Name |' in line:
            headers = [h.strip() for h in line.split('|')]
            for j, h in enumerate(headers):
                if h == 'Acronym':
                    acronym_col_idx = j
                if h == 'Definition':
                    definition_col_idx = j
            header_idx = i
            break
    
    if header_idx < 0:
        return acronyms
    
    # Parse data rows
    for line in lines[header_idx + 2:]:
        if not line.strip() or not line.startswith('|'):
            continue
        cols = [c.strip() for c in line.split('|')]
        if len(cols) < 2:
            continue
        
        name = cols[1].strip()
        acronym = cols[acronym_col_idx].strip() if acronym_col_idx > 0 and acronym_col_idx < len(cols) else ""
        definition = cols[definition_col_idx].strip() if definition_col_idx > 0 and definition_col_idx < len(cols) else ""
        
        if acronym and name not in ["Name", "---"]:
            # Handle multiple acronyms separated by semicolons
            for acr in acronym.split(';'):
                acr = acr.strip()
                if acr:
                    acronyms[acr] = {
                        "full_name": name,
                        "definition": definition
                    }
    
    return acronyms


def _extract_candidate_names(content: str) -> list[str]:
    """Extract likely proper names from free text."""
    import re
    if not content:
        return []
    
    candidates = re.findall(r"\b[A-Z][a-zA-Z\.]+ [A-Z][a-zA-Z\.]+\b", content)
    # Preserve order but dedupe
    seen = set()
    ordered = []
    for c in candidates:
        if c not in seen:
            seen.add(c)
            ordered.append(c)
    return ordered[:10]


def _quick_entity_scan(content: str, context: "ContextBundle") -> list[str]:
    """Quick scan for entity mentions in content.
    
    Returns list of entity names that appear in the content.
    """
    import re
    mentioned = []
    content_lower = content.lower()
    
    # Check known people
    people_names = context._extract_names_from_manifest(context.people_manifest)
    for name in people_names:
        canonical = context.aliases.get(name.lower(), name)
        if name.lower() in content_lower or canonical.lower() in content_lower:
            mentioned.append(canonical)
    
    # Check known companies
    company_names = context._extract_names_from_manifest(context.company_manifest)
    for name in company_names:
        canonical = context.aliases.get(name.lower(), name)
        if name.lower() in content_lower or canonical.lower() in content_lower:
            mentioned.append(canonical)
    
    # Check projects
    for project in context.project_list:
        if project.lower() in content_lower:
            mentioned.append(project)
    
    return mentioned[:20]  # Limit for token efficiency


def _load_entity_readmes(entities: list[str], vault_root: Path, entity_index: Optional[EntityIndex] = None) -> dict[str, str]:
    """Load README summaries for mentioned entities."""
    readmes = {}
    
    for entity in entities:
        readme = _find_entity_readme(entity, vault_root, entity_index)
        if readme:
            summary = _summarize_readme(readme)
            if summary:
                readmes[readme.parent.name] = summary
    
    return readmes


def _find_entity_readme(entity: str, vault_root: Path, entity_index: Optional[EntityIndex] = None) -> Optional[Path]:
    """Find README for an entity by name."""
    if entity_index:
        folder = (
            entity_index.find_person(entity)
            or entity_index.find_company(entity)
            or entity_index.find_project(entity)
        )
        if not folder:
            for search in (
                entity_index.search_person,
                entity_index.search_company,
                entity_index.search_project,
            ):
                hits = search(entity, limit=1)
                if hits:
                    folder = hits[0]
                    break
        if folder:
            readme_path = folder / "README.md"
            if readme_path.exists():
                return readme_path
    
    # Try People
    people_path = vault_root / "VAST" / "People" / entity / "README.md"
    if people_path.exists():
        return people_path
    
    # Try Customers
    customers_path = vault_root / "VAST" / "Customers and Partners" / entity / "README.md"
    if customers_path.exists():
        return customers_path
    
    # Try Projects
    projects_path = vault_root / "VAST" / "Projects" / entity / "README.md"
    if projects_path.exists():
        return projects_path
    
    return None


def _summarize_readme(readme_path: Path) -> str:
    """Extract key summary from README."""
    try:
        content = readme_path.read_text()
        
        # Extract frontmatter info
        summary_parts = []
        
        import re
        
        # Get role/title
        role_match = re.search(r"Role.*?:\s*(.+)", content)
        if role_match:
            summary_parts.append(f"Role: {role_match.group(1).strip()}")
        
        # Get company
        company_match = re.search(r"Company.*?:\s*(.+)", content)
        if company_match:
            summary_parts.append(f"Company: {company_match.group(1).strip()}")
        
        # Get key facts (first 3)
        facts_match = re.search(r"## Key Facts\s*\n((?:- .+\n?)+)", content)
        if facts_match:
            facts = facts_match.group(1).strip().split("\n")[:3]
            summary_parts.append("Key Facts: " + "; ".join(f.strip("- ") for f in facts))
        
        return "\n".join(summary_parts) if summary_parts else ""
        
    except Exception:
        return ""

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/entities.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Entity Index - Centralized entity lookup and management.

Provides fast lookups by:
- Email address (primary key for people)
- Full name (exact and partial matching)
- Company name
- Project name

This replaces the duplicated index-building code in ingest_emails.py and ingest_transcripts.py.
"""

import sys
from difflib import get_close_matches
from pathlib import Path
from typing import Optional, Tuple, Any
from functools import lru_cache

sys.path.insert(0, str(Path(__file__).parent.parent))
from scripts.utils.config import load_config


class EntityIndex:
    """Centralized entity index for fast lookups.
    
    Caches indices on first use, rebuilds if vault changes.
    """
    
    def __init__(self, vault_root: Path, config: Optional[dict[str, Any]] = None):
        self.vault_root = vault_root
        self.config = config or load_config(vault_root_override=vault_root)
        paths = self.config.get("paths", {})
        work_paths = paths.get("work", {})
        personal_paths = paths.get("personal", {})
        resources = paths.get("resources", {})
        entities_root = resources.get("entities", self.vault_root / "Workflow" / "entities")
        
        self.people_dirs = [
            Path(work_paths.get("people", self.vault_root / "VAST" / "People")),
            Path(personal_paths.get("people", self.vault_root / "Personal" / "People")),
        ]
        self.customer_dirs = [
            Path(work_paths.get("accounts", self.vault_root / "VAST" / "Customers and Partners")),
        ]
        self.project_dirs = [
            Path(work_paths.get("projects", self.vault_root / "VAST" / "Projects")),
            Path(personal_paths.get("projects", self.vault_root / "Personal" / "Projects")),
        ]
        self.alias_path = Path(entities_root) / "aliases.yaml"
        self._email_index: Optional[dict[str, Path]] = None
        self._name_index: Optional[dict[str, Path]] = None
        self._company_index: Optional[dict[str, Path]] = None
        self._project_index: Optional[dict[str, Path]] = None
        self._aliases: Optional[dict[str, str]] = None
        self._search_cache: dict[tuple[str, str], list[Path]] = {}
    
    def find_person(self, name: str, email: Optional[str] = None) -> Optional[Path]:
        """Find existing person folder.
        
        Lookup order:
        1. Exact email match (if email provided)
        2. Alias resolution (name â†’ canonical name)
        3. Exact name match
        4. Partial name match (first AND last name must match)
        
        Args:
            name: Person's name to search for
            email: Optional email address (preferred lookup key)
        
        Returns:
            Path to person folder, or None if not found
        """
        self._ensure_indices()
        
        # 1. Email match (highest priority)
        if email:
            email_lower = email.strip().lower()
            if email_lower in self._email_index:
                return self._email_index[email_lower]
        
        # 2. Normalize name via aliases
        normalized = self.normalize_name(name)
        
        # 3. Exact name match
        name_lower = normalized.lower()
        if name_lower in self._name_index:
            return self._name_index[name_lower]
        
        # 4. Partial match (first AND last name must match)
        name_parts = name_lower.split()
        if len(name_parts) >= 2:
            for folder_name, folder_path in self._name_index.items():
                folder_parts = folder_name.split()
                if len(folder_parts) >= 2:
                    first_initial = name_parts[0].strip(".")
                    if name_parts[0] == folder_parts[0] and name_parts[-1] == folder_parts[-1]:
                        return folder_path
                    if first_initial and first_initial[0] == folder_parts[0][0] and name_parts[-1] == folder_parts[-1]:
                        return folder_path
        
        # Fuzzy match fallback
        fuzzy = self.search_person(name, limit=1, cutoff=0.82)
        if fuzzy:
            return fuzzy[0]
        
        return None
    
    def find_company(self, company: str) -> Optional[Path]:
        """Find existing company/customer folder.
        
        Args:
            company: Company name to search for
        
        Returns:
            Path to company folder, or None if not found
        """
        self._ensure_indices()
        
        company_lower = company.lower().strip()
        
        # Exact match
        if company_lower in self._company_index:
            return self._company_index[company_lower]
        
        # Partial match
        for folder_name, folder_path in self._company_index.items():
            if company_lower in folder_name or folder_name in company_lower:
                return folder_path
        
        # Fuzzy match fallback
        fuzzy = self.search_company(company, limit=1, cutoff=0.78)
        if fuzzy:
            return fuzzy[0]
        
        return None
    
    def find_project(self, project: str) -> Optional[Path]:
        """Find existing project folder.
        
        Args:
            project: Project name to search for
        
        Returns:
            Path to project folder, or None if not found
        """
        self._ensure_indices()
        
        project_lower = project.lower().strip()
        
        # Exact match
        if project_lower in self._project_index:
            return self._project_index[project_lower]
        
        # Partial match
        for folder_name, folder_path in self._project_index.items():
            if project_lower in folder_name or folder_name in project_lower:
                return folder_path
        
        # Fuzzy match fallback
        fuzzy = self.search_project(project, limit=1)
        if fuzzy:
            return fuzzy[0]
        
        return None
    
    def search_person(self, name: str, limit: int = 3, cutoff: float = 0.72) -> list[Path]:
        """Fuzzy search for a person by name."""
        self._ensure_indices()
        normalized = self.normalize_name(name)
        cache_key = ("person", normalized.lower())
        if cache_key in self._search_cache:
            return self._search_cache[cache_key]
        
        if normalized.lower() in self._name_index:
            match = [self._name_index[normalized.lower()]]
            self._search_cache[cache_key] = match
            return match
        
        matches = self._fuzzy_match(normalized, list(self._name_index.keys()), limit, cutoff)
        paths = [self._name_index[m] for m in matches]
        self._search_cache[cache_key] = paths
        return paths
    
    def search_company(self, company: str, limit: int = 3, cutoff: float = 0.7) -> list[Path]:
        """Fuzzy search for a company by name."""
        self._ensure_indices()
        cache_key = ("company", company.lower().strip())
        if cache_key in self._search_cache:
            return self._search_cache[cache_key]
        
        matches = self._fuzzy_match(company, list(self._company_index.keys()), limit, cutoff)
        paths = [self._company_index[m] for m in matches]
        self._search_cache[cache_key] = paths
        return paths
    
    def search_project(self, project: str, limit: int = 3, cutoff: float = 0.65) -> list[Path]:
        """Fuzzy search for a project by name."""
        self._ensure_indices()
        cache_key = ("project", project.lower().strip())
        if cache_key in self._search_cache:
            return self._search_cache[cache_key]
        
        matches = self._fuzzy_match(project, list(self._project_index.keys()), limit, cutoff)
        paths = [self._project_index[m] for m in matches]
        self._search_cache[cache_key] = paths
        return paths
    
    def find_similar_people(self, name: str, limit: int = 2, cutoff: float = 0.82) -> list[str]:
        """Return similar people names for duplicate detection."""
        self._ensure_indices()
        normalized = self.normalize_name(name).lower()
        matches = [
            m for m in self._fuzzy_match(normalized, list(self._name_index.keys()), limit + 1, cutoff)
            if m != normalized
        ]
        return [self._name_index[m].name for m in matches[:limit]]
    
    def find_similar_companies(self, name: str, limit: int = 2, cutoff: float = 0.8) -> list[str]:
        """Return similar company names for duplicate detection."""
        self._ensure_indices()
        name_lower = name.lower().strip()
        matches = [
            m for m in self._fuzzy_match(name_lower, list(self._company_index.keys()), limit + 1, cutoff)
            if m != name_lower
        ]
        return [self._company_index[m].name for m in matches[:limit]]
    
    def find_similar_projects(self, name: str, limit: int = 2, cutoff: float = 0.8) -> list[str]:
        """Return similar project names for duplicate detection."""
        self._ensure_indices()
        name_lower = name.lower().strip()
        matches = [
            m for m in self._fuzzy_match(name_lower, list(self._project_index.keys()), limit + 1, cutoff)
            if m != name_lower
        ]
        return [self._project_index[m].name for m in matches[:limit]]
    
    def normalize_name(self, name: str) -> str:
        """Normalize a name using aliases.
        
        Args:
            name: Name to normalize
        
        Returns:
            Canonical name if alias exists, otherwise original
        """
        self._ensure_aliases()
        
        name_lower = name.lower().strip()
        return self._aliases.get(name_lower, name)
    
    def list_people(self) -> list[str]:
        """List all known people names."""
        self._ensure_indices()
        return list(self._name_index.keys())
    
    def list_companies(self) -> list[str]:
        """List all known company names."""
        self._ensure_indices()
        return list(self._company_index.keys())
    
    def list_projects(self) -> list[str]:
        """List all known project names."""
        self._ensure_indices()
        return list(self._project_index.keys())
    
    def invalidate(self):
        """Invalidate cached indices (call after creating entities)."""
        self._email_index = None
        self._name_index = None
        self._company_index = None
        self._project_index = None
        self._search_cache = {}
    
    # =========================================================================
    # PRIVATE METHODS
    # =========================================================================
    def _fuzzy_match(self, query: str, choices: list[str], limit: int, cutoff: float) -> list[str]:
        """Return close matches using difflib for lightweight fuzzy search."""
        if not query or not choices:
            return []
        normalized = " ".join(query.lower().replace(".", " ").split())
        choice_map = {" ".join(c.split()): c for c in choices}
        matches = get_close_matches(normalized, list(choice_map.keys()), n=limit, cutoff=cutoff)
        return [choice_map[m] for m in matches]
    
    def _ensure_indices(self):
        """Build indices if not already cached."""
        if self._email_index is None:
            self._build_person_index()
        if self._company_index is None:
            self._build_company_index()
        if self._project_index is None:
            self._build_project_index()
    
    def _ensure_aliases(self):
        """Load aliases if not already cached."""
        if self._aliases is None:
            self._load_aliases()
    
    def _build_person_index(self):
        """Build email and name indices for people."""
        from scripts.utils.frontmatter import parse_frontmatter
        
        self._email_index = {}
        self._name_index = {}
        
        for people_dir in self.people_dirs:
            if not people_dir.exists():
                continue
            
            for folder in people_dir.iterdir():
                if not folder.is_dir() or folder.name.startswith("_"):
                    continue
                
                readme = folder / "README.md"
                if not readme.exists():
                    continue
                
                # Index by folder name
                self._name_index[folder.name.lower()] = folder
                
                # Parse frontmatter for email
                try:
                    content = readme.read_text()
                    fm, _ = parse_frontmatter(content)
                    if fm and fm.get("email"):
                        email = fm["email"].strip().lower()
                        if email and email != "''":
                            self._email_index[email] = folder
                except Exception:
                    pass
    
    def _build_company_index(self):
        """Build index for companies/customers."""
        self._company_index = {}
        
        for customers_dir in self.customer_dirs:
            if not customers_dir.exists():
                continue
            for folder in customers_dir.iterdir():
                if folder.is_dir() and not folder.name.startswith("_"):
                    self._company_index[folder.name.lower()] = folder
    
    def _build_project_index(self):
        """Build index for projects."""
        self._project_index = {}
        
        for projects_dir in self.project_dirs:
            if not projects_dir.exists():
                continue
            for folder in projects_dir.iterdir():
                if folder.is_dir() and not folder.name.startswith("_"):
                    self._project_index[folder.name.lower()] = folder
    
    def _load_aliases(self):
        """Load name aliases from YAML file."""
        import yaml
        
        self._aliases = {}
        aliases_path = self.alias_path
        
        if not aliases_path.exists():
            return
        
        try:
            data = yaml.safe_load(aliases_path.read_text()) or {}

            # Support the vault's categorized aliases format:
            #   people/accounts/projects/rob: {Canonical: [aliases...]}, plus legacy flat mappings.
            categories = ["people", "accounts", "projects", "rob", "rob_forums"]
            if any(isinstance(data.get(cat), dict) for cat in categories):
                for category in categories:
                    cat_aliases = data.get(category)
                    if not isinstance(cat_aliases, dict):
                        continue
                    for key, value in cat_aliases.items():
                        if not key:
                            continue
                        if isinstance(value, list):
                            canonical = str(key)
                            self._aliases[canonical.lower()] = canonical
                            for alias in value:
                                if not alias:
                                    continue
                                self._aliases[str(alias).lower()] = canonical
                        elif isinstance(value, str):
                            # Dict form: {"alias": "Canonical Name"}
                            alias = str(key)
                            canonical = str(value)
                            self._aliases[canonical.lower()] = canonical
                            self._aliases[alias.lower()] = canonical
                return

            # Legacy flat mapping: {"Canonical Name": ["alias1", "alias2"], "Alias": "Canonical"}
            for key, value in data.items():
                if not key:
                    continue
                canonical = str(key)
                self._aliases[canonical.lower()] = canonical
                if isinstance(value, list):
                    for alias in value:
                        if not alias:
                            continue
                        self._aliases[str(alias).lower()] = canonical
                elif isinstance(value, str):
                    self._aliases[str(key).lower()] = str(value)
        except Exception:
            pass


# Singleton pattern for module-level access
_entity_index: Optional[EntityIndex] = None


def get_entity_index(vault_root: Optional[Path] = None, config: Optional[dict[str, Any]] = None) -> EntityIndex:
    """Get or create the entity index singleton.
    
    Args:
        vault_root: Vault root path. Required on first call.
        config: Optional pre-loaded config (avoids reload)
    
    Returns:
        EntityIndex instance
    """
    global _entity_index
    
    if _entity_index is None:
        if vault_root is None:
            # Try to get from utils
            from scripts.utils import vault_root as get_vault_root
            vault_root = get_vault_root()
        _entity_index = EntityIndex(vault_root, config=config)
    
    return _entity_index

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/envelope.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Content Envelope - Normalized container for all content types.

Each adapter converts raw content to a ContentEnvelope for unified processing.
"""

from enum import Enum
from datetime import datetime
from pathlib import Path
from typing import Optional, Any
from pydantic import BaseModel, Field, ConfigDict


class ContentType(str, Enum):
    """Supported content types."""
    EMAIL = "email"
    TRANSCRIPT = "transcript"
    DOCUMENT = "document"
    VOICE = "voice"
    SMS = "sms"


class ContentEnvelope(BaseModel):
    """Normalized container for all content types.
    
    Adapters convert raw content (email, transcript, etc.) to this format,
    allowing unified downstream processing.
    """
    
    model_config = ConfigDict(extra="ignore")
    
    # Identity
    source_path: Path
    content_type: ContentType
    
    # Content
    raw_content: str
    
    # Common metadata (extracted by adapter)
    date: str  # YYYY-MM-DD
    title: str
    participants: list[str] = Field(default_factory=list)
    
    # Type-specific metadata
    metadata: dict[str, Any] = Field(default_factory=dict)
    
    # Processing state
    created_at: datetime = Field(default_factory=datetime.now)
    content_hash: Optional[str] = None
    
    def __str__(self) -> str:
        return f"{self.content_type.value}: {self.source_path.name}"


class EmailMetadata(BaseModel):
    """Email-specific metadata."""
    
    sender_name: Optional[str] = None
    sender_email: Optional[str] = None
    recipients: list[str] = Field(default_factory=list)
    recipients_emails: list[str] = Field(default_factory=list)
    recipients_detail: list[dict] = Field(default_factory=list)
    cc: list[str] = Field(default_factory=list)
    subject: str = ""
    thread_id: Optional[str] = None
    in_reply_to: Optional[str] = None
    is_reply: bool = False
    

class TranscriptMetadata(BaseModel):
    """Transcript-specific metadata."""
    
    speakers: list[str] = Field(default_factory=list)
    duration_estimate: Optional[str] = None
    source_app: str = "MacWhisper"
    has_diarization: bool = True
    

class DocumentMetadata(BaseModel):
    """Document-specific metadata."""
    
    document_type: str = "general"  # article, spec, proposal, report, etc.
    author: Optional[str] = None
    source_url: Optional[str] = None
    file_type: str = "markdown"  # pdf, docx, markdown, etc.

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/extract.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Unified Extractor - Extract structured knowledge from any content type.

Uses LLM with rich context (persona, manifests, glossary) to produce
UnifiedExtraction for downstream patching and output generation.
"""

import json
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

from .envelope import ContentEnvelope, ContentType
from .context import ContextBundle
from .models import (
    UnifiedExtraction, ContactInfo, Fact, TaskItem, 
    MentionedEntity, EntityRef, SuggestedOutputs
)
from scripts.utils import get_logger, get_model_config


class UnifiedExtractor:
    """Extract structured knowledge from content.
    
    Features:
    - Rich context injection (persona, manifests, glossary)
    - Content-type specific guidance
    - Entity-attached facts for smart patching
    - Suggested outputs (replies, calendar, tasks)
    - Prompt caching for token efficiency
    """
    
    def __init__(self, vault_root: Path, verbose: bool = False):
        self.vault_root = vault_root
        self.verbose = verbose
        self._client = None
        self._context: Optional[ContextBundle] = None
        self.logger = get_logger("unified_extractor")
        self.last_usage: dict = {}
    
    @property
    def client(self):
        """Get OpenAI client lazily."""
        if self._client is None:
            from scripts.utils.ai_client import get_openai_client
            self._client = get_openai_client("unified_extractor")
        return self._client
    
    def extract(self, envelope: ContentEnvelope, context: Optional[ContextBundle] = None) -> UnifiedExtraction:
        """Extract structured knowledge from content.
        
        Args:
            envelope: ContentEnvelope with normalized content
            context: Optional ContextBundle (loaded if not provided)
        
        Returns:
            UnifiedExtraction with all extracted knowledge
        """
        # Load context if not provided
        if context is None:
            context = ContextBundle.load(self.vault_root, envelope)
        
        self.last_usage = {}
        # Build prompt - pass verbose flag for cache logging
        system_prompt = self._build_system_prompt(envelope, context)
        user_prompt = self._build_user_prompt(envelope)
        
        if self.verbose:
            _, prefix_hash = context.get_cacheable_prefix()
            self.logger.info(f"System prompt length={len(system_prompt)} chars, cacheable prefix hash={prefix_hash}")
        
        # Select model based on content type if configured (e.g., extract_email, extract_transcript)
        task_key = f"extract_{envelope.content_type.value}"
        model_config = get_model_config(task_key)
        
        # Call LLM with prompt caching enabled
        call_start = time.time()
        try:
            with self.logger.context(phase="extract", file=str(envelope.source_path)):
                response = self.client.chat.completions.create(
                    model=model_config["model"],
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    temperature=model_config.get("temperature", 0.0),
                    # Prompt caching: static context (persona/glossary/aliases) is first in system prompt.
                )
            
            latency_ms = int((time.time() - call_start) * 1000)
            self.last_usage = self._capture_usage(response, model_config["model"], latency_ms)
            
            # Log cache stats if available (OpenAI returns cached_tokens in usage)
            if self.verbose and self.last_usage:
                prompt_tokens = self.last_usage.get("prompt_tokens", 0)
                cached_tokens = self.last_usage.get("cached_tokens", 0)
                if self.last_usage.get("cache_hit"):
                    cache_pct = (cached_tokens / prompt_tokens * 100) if prompt_tokens > 0 else 0
                    self.logger.info(f"Cache HIT: {cached_tokens}/{prompt_tokens} tokens ({cache_pct:.0f}%)")
                elif prompt_tokens:
                    self.logger.info(f"Cache miss: {prompt_tokens} prompt tokens")
            
            result = response.choices[0].message.content.strip()
            
            # Parse JSON
            if result.startswith("```"):
                result = re.sub(r'^```\w*\n?', '', result)
                result = re.sub(r'\n?```$', '', result)
            
            data = json.loads(result)
            
            return self._build_extraction(envelope, data)
            
        except json.JSONDecodeError as e:
            return self._build_minimal_extraction(envelope, f"JSON parse error: {e}")
        except Exception as e:
            self.logger.error("Extraction failed", exc_info=True)
            raise RuntimeError(f"Extraction failed: {e}")
    
    def _capture_usage(self, response, model: str, latency_ms: int) -> dict:
        """Capture token + cache usage stats from an OpenAI response."""
        usage = getattr(response, "usage", None)
        prompt_tokens = self._get_usage_value(usage, "prompt_tokens")
        completion_tokens = self._get_usage_value(usage, "completion_tokens")
        total_tokens = self._get_usage_value(usage, "total_tokens")
        cached_tokens = self._get_usage_value(usage, "cached_tokens")
        
        # Some SDKs nest cached tokens under prompt_tokens_details
        if not cached_tokens and usage is not None:
            details = getattr(usage, "prompt_tokens_details", None)
            if isinstance(details, dict):
                cached_tokens = details.get("cached_tokens", 0) or 0
            elif details is not None:
                cached_tokens = getattr(details, "cached_tokens", 0) or 0
        
        if total_tokens == 0:
            total_tokens = (prompt_tokens or 0) + (completion_tokens or 0)
        
        cache_hit = cached_tokens > 0
        
        return {
            "model": model,
            "prompt_tokens": prompt_tokens or 0,
            "completion_tokens": completion_tokens or 0,
            "total_tokens": total_tokens or 0,
            "cached_tokens": cached_tokens or 0,
            "cache_hit": cache_hit,
            "cache_savings_tokens": cached_tokens or 0,
            "latency_ms": latency_ms,
        }
    
    def _get_usage_value(self, usage, key: str) -> int:
        """Helper to safely read usage values from dicts or SDK objects."""
        if usage is None:
            return 0
        if isinstance(usage, dict):
            return usage.get(key, 0) or 0
        return getattr(usage, key, 0) or 0
    
    def _build_system_prompt(self, envelope: ContentEnvelope, context: ContextBundle) -> str:
        """Build system prompt with context and extraction instructions.
        
        PROMPT CACHING: OpenAI caches identical prompt prefixes >= 1024 tokens.
        We structure the prompt so static content (persona, glossary) comes FIRST,
        followed by content-type-specific instructions (dynamic).
        """
        
        # Get formatted context - static portion first for caching
        # Pass verbose flag to show cache eligibility info
        context_section = context.get_extraction_context(compact=True, verbose=self.verbose)
        
        # Content-type specific guidance (dynamic, comes after cached prefix)
        type_guidance = self._get_type_guidance(envelope.content_type)
        
        instructions = f"""You are extracting structured knowledge from content for a personal knowledge management system.

{context_section}

## CONTENT TYPE
This is a {envelope.content_type.value}. {type_guidance}

## EXTRACTION SCHEMA
Return a JSON object with this exact structure:

{{
    "note_type": "customer|people|projects|rob|journal",
    "primary_entity": {{"entity_type": "person|company|project", "name": "...", "confidence": 0.9}} or null,
    
    "title": "Brief descriptive title",
    "summary": "1-3 sentence summary",
    
    "participants": ["Person 1", "Person 2"],
    "contacts": [
        {{"name": "...", "email": "...", "phone": "...", "title": "...", "company": "...", "linkedin": "..."}}
    ],
    
    "facts": [
        {{"text": "fact about someone/something", "about_entity": {{"entity_type": "person", "name": "..."}}, "fact_type": "background|preference|technical|relationship", "confidence": 0.8}}
    ],
    "decisions": ["Decision 1", "Decision 2"],
    "topics": ["Topic 1", "Topic 2"],
    
    "tasks": [
        {{"text": "action item", "owner": "Myself or person name", "due": "YYYY-MM-DD", "priority": "high|medium|low", "related_person": "...", "related_project": "...", "related_customer": "..."}}
    ],
    "questions": ["Question needing answer"],
    "commitments": ["Commitment made by anyone"],
    
    "mentioned_entities": [
        {{"entity_type": "person|company|project", "name": "...", "role": "discussed|action_owner|mentioned", "facts_about": ["fact 1", "fact 2"], "aliases_discovered": ["nickname", "abbreviation"], "acronym_discovered": "ABC", "confidence": 0.8}}
    ],
    
    "discovered_aliases": [
        {{"canonical_name": "Full Name", "alias": "nickname or abbreviation", "entity_type": "person|company|project"}}
    ],
    "discovered_acronyms": [
        {{"term": "Full Term Name", "acronym": "ABC", "definition": "Brief definition if mentioned"}}
    ],
    
    "email_requires_response": true|false,
    "email_urgency": "low|medium|high|critical",
    "email_type": "request|information|follow_up|introduction|scheduling|other",
    
    "suggested_outputs": {{
        "needs_reply": true|false,
        "reply_urgency": "urgent|normal|low",
        "reply_context": "Key points to address in reply",
        "calendar_invite": {{"title": "...", "proposed_date": "YYYY-MM-DD", "attendees": [...], "duration_minutes": 30}} or null,
        "follow_up_reminder": {{"text": "...", "remind_date": "YYYY-MM-DD"}} or null
    }},
    
    "confidence": 0.85
}}

## CRITICAL RULES

1. **FACTS ABOUT ENTITIES**: When you learn something about a person/company/project, 
   add it to both the `facts` array AND the entity's `facts_about` in `mentioned_entities`.
   This is how we know which entities to update.

2. **TASK OWNERSHIP**: 
   - If the speaker (me) commits to something â†’ owner is "Myself"
   - If someone else commits â†’ use their full name (from ENTITY GLOSSARY if possible)
   - If unclear â†’ owner is "TBD"

3. **ENTITY MATCHING**: Use names from the ENTITY GLOSSARY when possible to ensure consistency.
   If you see a name that's similar to a known entity, use the known entity's name.

4. **PRIMARY ENTITY**:
   - 1:1 meetings â†’ the other person is primary_entity (type: person)
   - Customer meetings â†’ the customer/company is primary_entity (type: company)
   - Project discussions â†’ the project is primary_entity (type: project)
   - General updates â†’ primary_entity can be null

5. **NOTE_TYPE CLASSIFICATION** (based on MEETING FORMAT, not content):
   - "people" = 1:1 meetings or small group with VAST colleagues. The primary_entity is the other person.
     Even if discussing projects/customers, a 1:1 with Jeff goes under Jeff's folder.
   - "customer" = external customer/partner meeting (multiple people from customer org present)
   - "projects" = internal project-focused group discussion (3+ VAST people)
   - "rob" = recurring team sync (Rhythm of Business) - weekly, biweekly standups
   - "journal" = personal reflection

   **CRITICAL**: Look at WHO is in the meeting, not WHAT is discussed:
   - 1:1 with Jeff Denworth discussing Microsoft â†’ note_type: "people", primary_entity: Jeff Denworth
   - Group call with Microsoft team â†’ note_type: "customer", primary_entity: Microsoft
   - Internal project standup â†’ note_type: "projects", primary_entity: project name

6. **VERBOSITY - ALL FACTS/DECISIONS/TOPICS MUST BE SELF-CONTAINED**:
   Every extracted fact, decision, topic, and task MUST be fully understandable 6 months 
   from now WITHOUT reading the source document. Never use vague references.
   
   ALWAYS include:
   - WHO: Full names of people (not just "he" or "they" or first names only)
   - WHAT: Specific project, product, or initiative name (not "the project" or "this")
   - WHICH: Specific customer/company when relevant (not "the customer")
   - CONTEXT: Enough detail that the statement stands alone
   
   BAD EXAMPLES (vague, useless later):
   - "Use the same concepts from previous CSP projects" â†’ Which projects? What concepts?
   - "Discussed storage architecture" â†’ Whose storage? Which architecture?
   - "Follow up on pricing" â†’ Which customer? What pricing tier?
   - "He mentioned the timeline" â†’ Who? What timeline for what?
   
   GOOD EXAMPLES (self-contained, useful later):
   - "Apply Microsoft Azure marketplace SKU patterns from LSv4 launch to GCP offer design"
   - "Discussed VAST storage architecture for Google GDC RFP encryption requirements"
   - "Follow up with Kanchan Mehrotra on Microsoft Apollo project pricing proposal"
   - "Jeff Denworth mentioned Q1 2026 timeline for MAI unified cache GA"
   
   If you don't know a specific name, say so: "Unknown Microsoft contact mentioned..."

7. **ALIAS AND ACRONYM DISCOVERY**:
   When you encounter nicknames, abbreviations, or acronyms that are NOT in the existing 
   NAME ALIASES or TERMS & ACRONYMS sections, capture them:
   
   - **Aliases**: If someone is called by a nickname (e.g., "Lior mentioned that LG would...")
     where "LG" refers to Lior Genzel, add to `discovered_aliases`:
     {{"canonical_name": "Lior Genzel", "alias": "LG", "entity_type": "person"}}
   
   - **Acronyms**: If a project or term is abbreviated (e.g., "the DASE architecture" where
     DASE = Data Application Storage Engine), add to `discovered_acronyms`:
     {{"term": "Data Application Storage Engine", "acronym": "DASE", "definition": "VAST's storage architecture"}}
   
   This helps us build our glossary over time. Only capture NEW aliases/acronyms not 
   already in the glossary provided above.

Return ONLY valid JSON, no markdown fences or explanation."""

        return instructions
    
    def _get_type_guidance(self, content_type: ContentType) -> str:
        """Get content-type specific extraction guidance."""
        
        guidance = {
            ContentType.EMAIL: """
Pay special attention to:
- Sender and recipient information (extract all contact details)
- Whether a response is needed (direct questions, requests)
- Urgency signals (deadline mentions, "urgent", "ASAP")
- Commitments made by sender or requested from me
- Any scheduling/calendar mentions for calendar_invite suggestion""",
            
            ContentType.TRANSCRIPT: """
This is a meeting transcript with speaker labels.
Pay special attention to:
- **PARTICIPANT COUNT**: How many distinct speakers/people are in this meeting?
  - 2 people (1:1) â†’ note_type: "people", primary_entity is the OTHER person
  - Multiple VAST employees â†’ note_type: "projects" 
  - External customer/partner present â†’ note_type: "customer"
- Identify all participants from speaker labels and mentions
- Capture action items with clear owners
- Note decisions made during the meeting  
- Extract facts learned about people, companies, or projects
- Summarize the main discussion points as topics""",
            
            ContentType.DOCUMENT: """
This is a document or article.
Pay special attention to:
- Key information and facts
- Relevant entities mentioned
- Any action items or recommendations
- The main topics covered""",
            
            ContentType.VOICE: """
This is a voice memo transcription.
Pay special attention to:
- Tasks and reminders mentioned
- Ideas or thoughts to capture
- References to people or projects
- Follow-up items""",
        }
        
        return guidance.get(content_type, "Extract all relevant information.")
    
    def _build_user_prompt(self, envelope: ContentEnvelope) -> str:
        """Build user prompt with content."""
        
        # Truncate very long content
        content = envelope.raw_content
        if len(content) > 12000:
            content = content[:12000] + "\n\n[... content truncated ...]"
        
        return f"""Extract knowledge from this {envelope.content_type.value}:

Date: {envelope.date}
Title: {envelope.title}
Participants: {', '.join(envelope.participants) if envelope.participants else 'Unknown'}

---

{content}"""
    
    def _build_extraction(self, envelope: ContentEnvelope, data: dict) -> UnifiedExtraction:
        """Build UnifiedExtraction from parsed JSON data."""
        
        # Parse primary entity
        primary_entity = None
        if data.get("primary_entity"):
            pe = data["primary_entity"]
            primary_entity = EntityRef(
                entity_type=pe.get("entity_type", "person"),
                name=pe.get("name", ""),
                confidence=pe.get("confidence", 0.8)
            )
        
        # Parse contacts (check multiple field names for compatibility)
        contacts = []
        # Standard contacts array
        for c in data.get("contacts", []):
            contacts.append(ContactInfo(**c))
        # Email-specific: contacts_mentioned
        for c in data.get("contacts_mentioned", []):
            contacts.append(ContactInfo(**c))
        # Email-specific: sender
        if data.get("sender"):
            sender = data["sender"]
            if isinstance(sender, dict) and sender.get("name"):
                contacts.append(ContactInfo(**sender))
        
        # Parse facts
        facts = []
        for f in data.get("facts", []):
            about = None
            if f.get("about_entity"):
                about = EntityRef(**f["about_entity"])
            facts.append(Fact(
                text=f.get("text", ""),
                about_entity=about,
                fact_type=f.get("fact_type", "general"),
                confidence=f.get("confidence", 0.8)
            ))
        
        # Parse tasks
        tasks = []
        for t in data.get("tasks", []):
            tasks.append(TaskItem(**t))
        
        # Parse mentioned entities
        mentioned = []
        for m in data.get("mentioned_entities", []):
            mentioned.append(MentionedEntity(
                entity_type=m.get("entity_type", "person"),
                name=m.get("name", ""),
                role=m.get("role"),
                facts_about=m.get("facts_about", []),
                confidence=m.get("confidence", 0.8)
            ))
        
        # Parse suggested outputs
        suggested = SuggestedOutputs()
        if data.get("suggested_outputs"):
            so = data["suggested_outputs"]
            suggested = SuggestedOutputs(
                needs_reply=so.get("needs_reply", False),
                reply_urgency=so.get("reply_urgency", "normal"),
                reply_context=so.get("reply_context"),
            )
        
        allowed_note_types = {"customer", "people", "projects", "rob", "journal", "partners", "travel"}
        note_type = data.get("note_type", "people")
        if note_type not in allowed_note_types:
            note_type = "people"

        # Participants: prefer model output when non-empty, otherwise fall back to envelope metadata.
        participants_raw = data.get("participants")
        participants: list[str] = []
        if isinstance(participants_raw, list):
            participants = [str(p).strip() for p in participants_raw if str(p).strip()]
        elif isinstance(participants_raw, str):
            # Defensive: sometimes models return a delimited string.
            participants = [p.strip() for p in re.split(r"[;,]", participants_raw) if p.strip()]

        if not participants:
            participants = [p.strip() for p in (envelope.participants or []) if str(p).strip()]

        # If still empty, try the primary entity for people-notes.
        if not participants and primary_entity and primary_entity.entity_type == "person" and primary_entity.name:
            participants = [primary_entity.name]

        # Final fallback: ensure at least one participant for transcript/meeting-style notes.
        if not participants:
            participants = ["Jason Vallery"]

        # Build legacy mentions for compatibility (use normalized participants).
        mentions = {
            "people": participants,
            "projects": [e["name"] for e in data.get("mentioned_entities", []) if e.get("entity_type") == "project"],
            "accounts": [e["name"] for e in data.get("mentioned_entities", []) if e.get("entity_type") == "company"],
        }
        
        return UnifiedExtraction(
            source_file=str(envelope.source_path),
            content_type=envelope.content_type.value,
            processed_at=datetime.now(),
            note_type=note_type,
            primary_entity=primary_entity,
            date=envelope.date,
            title=data.get("title", envelope.title),
            summary=data.get("summary", ""),
            participants=participants,
            contacts=contacts,
            facts=facts,
            decisions=data.get("decisions", []),
            topics=data.get("topics", []),
            tasks=tasks,
            questions=data.get("questions", []),
            commitments=data.get("commitments", []),
            mentioned_entities=mentioned,
            mentions=mentions,
            email_requires_response=data.get("email_requires_response", False),
            email_urgency=data.get("email_urgency", "medium"),
            email_type=data.get("email_type", "other"),
            suggested_outputs=suggested,
            confidence=data.get("confidence", 0.8)
        )
    
    def _build_minimal_extraction(self, envelope: ContentEnvelope, error: str) -> UnifiedExtraction:
        """Build minimal extraction when parsing fails."""
        return UnifiedExtraction(
            source_file=str(envelope.source_path),
            content_type=envelope.content_type.value,
            processed_at=datetime.now(),
            note_type="people",
            date=envelope.date,
            title=envelope.title,
            summary=f"Extraction failed: {error}",
            participants=envelope.participants,
            confidence=0.0
        )

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/models.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Unified Extraction Model - Single schema for all content types.

This replaces the separate ExtractionV1 (transcripts) and EmailExtraction (emails)
with a unified model that works for all content types.
"""

from datetime import datetime
from typing import Optional, Literal, Any
from pydantic import BaseModel, Field, ConfigDict


class EntityRef(BaseModel):
    """Reference to an entity (person, company, or project)."""
    
    model_config = ConfigDict(extra="ignore")
    
    entity_type: Literal["person", "company", "project"]
    name: str
    confidence: float = 0.8


class ContactInfo(BaseModel):
    """Contact information for a person."""
    
    model_config = ConfigDict(extra="ignore")
    
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    title: Optional[str] = None
    company: Optional[str] = None
    linkedin: Optional[str] = None


class Fact(BaseModel):
    """A key fact discovered in the content.
    
    Facts are attached to the entity they're ABOUT (not just mentioned).
    """
    
    model_config = ConfigDict(extra="ignore")
    
    text: str
    about_entity: Optional[EntityRef] = None  # Who/what is this fact about?
    fact_type: str = "general"  # preference, background, technical, relationship, etc.
    confidence: float = 0.8


class TaskItem(BaseModel):
    """A task or action item extracted from content."""
    
    model_config = ConfigDict(extra="ignore")
    
    text: str
    owner: Optional[str] = None  # "Myself" for first-person, else person's name
    due: Optional[str] = None  # YYYY-MM-DD
    priority: str = "medium"  # highest, high, medium, low, lowest
    
    # Related entities
    related_person: Optional[str] = None
    related_project: Optional[str] = None
    related_customer: Optional[str] = None


class MentionedEntity(BaseModel):
    """An entity mentioned in the content with associated facts.
    
    This enables smart patching: we patch entities when we learn facts ABOUT them,
    not just because they're mentioned.
    """
    
    model_config = ConfigDict(extra="ignore")
    
    entity_type: Literal["person", "company", "project"]
    name: str
    role: Optional[str] = None  # How they relate to the content (e.g., "discussed", "action owner")
    facts_about: list[str] = Field(default_factory=list)  # Facts discovered about this entity
    confidence: float = 0.8


class DiscoveredAlias(BaseModel):
    """An alias discovered for a person (to update People manifest).
    
    Example: "LG" is an alias for "Lior Genzel" 
    """
    
    model_config = ConfigDict(extra="ignore")
    
    alias: str  # The short name/nickname/initials (e.g., "LG")
    canonical_name: str  # The full name (e.g., "Lior Genzel")
    confidence: float = 0.8


class DiscoveredAcronym(BaseModel):
    """An acronym discovered for a project/term (to update Projects manifest).
    
    Example: "DASE" = "Data Application Storage Engine"
    """
    
    model_config = ConfigDict(extra="ignore")
    
    acronym: str  # The acronym (e.g., "DASE")
    expansion: str  # The expanded form (e.g., "Data Application Storage Engine")
    project_name: Optional[str] = None  # If this is a project acronym, which project?
    confidence: float = 0.8


class CalendarSuggestion(BaseModel):
    """Suggested calendar invite from content."""
    
    model_config = ConfigDict(extra="ignore")
    
    title: str
    proposed_date: Optional[str] = None  # YYYY-MM-DD
    proposed_time: Optional[str] = None  # HH:MM
    duration_minutes: int = 30
    attendees: list[str] = Field(default_factory=list)
    description: Optional[str] = None


class ReminderSuggestion(BaseModel):
    """Suggested follow-up reminder."""
    
    model_config = ConfigDict(extra="ignore")
    
    text: str
    remind_date: str  # YYYY-MM-DD
    related_entity: Optional[str] = None


class SuggestedOutputs(BaseModel):
    """AI-suggested outputs based on content analysis."""
    
    model_config = ConfigDict(extra="ignore")
    
    needs_reply: bool = False
    reply_urgency: str = "normal"  # urgent, normal, low
    reply_context: Optional[str] = None  # Key points to address
    
    calendar_invite: Optional[CalendarSuggestion] = None
    follow_up_reminder: Optional[ReminderSuggestion] = None


class UnifiedExtraction(BaseModel):
    """Unified extraction schema for all content types.
    
    This single schema works for emails, transcripts, documents, etc.
    """
    
    model_config = ConfigDict(extra="ignore")
    
    # Version and source
    version: str = "2.0"
    source_file: str
    content_type: str  # email, transcript, document, voice
    processed_at: datetime
    
    # Classification
    note_type: Literal["customer", "people", "projects", "rob", "journal", "partners", "travel"]
    primary_entity: Optional[EntityRef] = None  # Main subject of the content
    
    # Basic metadata
    date: str  # YYYY-MM-DD
    title: str
    summary: str
    
    # Participants/contacts
    participants: list[str] = Field(default_factory=list)
    contacts: list[ContactInfo] = Field(default_factory=list)  # Detailed contact info
    
    # Extracted knowledge
    facts: list[Fact] = Field(default_factory=list)
    decisions: list[str] = Field(default_factory=list)
    topics: list[str] = Field(default_factory=list)
    
    # Actionable items
    tasks: list[TaskItem] = Field(default_factory=list)
    questions: list[str] = Field(default_factory=list)  # Questions that need answers
    commitments: list[str] = Field(default_factory=list)  # Commitments made by anyone
    
    # Entity mentions with facts (enables smart patching)
    mentioned_entities: list[MentionedEntity] = Field(default_factory=list)
    
    # Discovered aliases and acronyms (for manifest updates)
    discovered_aliases: list[DiscoveredAlias] = Field(default_factory=list)
    discovered_acronyms: list[DiscoveredAcronym] = Field(default_factory=list)
    
    # Legacy compatibility (will be removed)
    mentions: dict[str, list[str]] = Field(
        default_factory=lambda: {"people": [], "projects": [], "accounts": []}
    )
    
    # Email-specific
    email_requires_response: bool = False
    email_urgency: str = "medium"
    email_type: str = "other"
    
    # Suggested outputs
    suggested_outputs: SuggestedOutputs = Field(default_factory=SuggestedOutputs)
    
    # Confidence
    confidence: float = 0.8
    
    def get_entities_with_facts(self) -> list[MentionedEntity]:
        """Get entities that have facts associated with them.
        
        These are the entities we should patch (we learned something about them).
        """
        return [e for e in self.mentioned_entities if e.facts_about]
    
    def get_all_mentioned_people(self) -> list[str]:
        """Get all mentioned people names (for context patches)."""
        people = set(self.participants)
        
        # From contacts
        for contact in self.contacts:
            if contact.name:
                people.add(contact.name)
        
        # From mentioned entities
        for entity in self.mentioned_entities:
            if entity.entity_type == "person":
                people.add(entity.name)
        
        # Legacy format
        people.update(self.mentions.get("people", []))
        
        return list(people)
    
    def get_all_mentioned_companies(self) -> list[str]:
        """Get all mentioned company names."""
        companies = set()
        
        # From contacts
        for contact in self.contacts:
            if contact.company:
                companies.add(contact.company)
        
        # From mentioned entities
        for entity in self.mentioned_entities:
            if entity.entity_type == "company":
                companies.add(entity.name)
        
        # Legacy format
        companies.update(self.mentions.get("accounts", []))
        
        return list(companies)
    
    def get_all_mentioned_projects(self) -> list[str]:
        """Get all mentioned project names."""
        projects = set()
        
        # From mentioned entities
        for entity in self.mentioned_entities:
            if entity.entity_type == "project":
                projects.add(entity.name)
        
        # Legacy format
        projects.update(self.mentions.get("projects", []))
        
        return list(projects)

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/outputs.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Output Generation - Create draft replies, calendar invites, and task files.

This module handles the output generation phase of the unified pipeline,
producing actionable outputs from extracted knowledge.

Outputs:
- Draft email replies â†’ Outbox/
- Calendar invites â†’ Outbox/_calendar/
- Task aggregations â†’ TASKS_INBOX.md or entity README

Usage:
    generator = OutputGenerator(vault_root)
    draft_path = generator.generate_reply(extraction, context)
    ics_path = generator.generate_calendar_invite(extraction)
"""

import json
import sys
import re
import yaml
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

from .models import UnifiedExtraction, SuggestedOutputs, CalendarSuggestion
from scripts.utils import get_model_config, workflow_root


def _load_persona() -> dict:
    """Load the communication persona from profiles/jason_persona.yaml."""
    persona_path = workflow_root() / "profiles" / "jason_persona.yaml"
    if persona_path.exists():
        with open(persona_path) as f:
            return yaml.safe_load(f)
    return {}


class OutputGenerator:
    """Generate output files from extraction results.
    
    Generates:
    - Draft email replies with frontmatter metadata
    - Calendar .ics files for suggested meetings
    - Follow-up reminder tasks
    """
    
    def __init__(self, vault_root: Path, dry_run: bool = False, verbose: bool = False):
        self.vault_root = vault_root
        self.dry_run = dry_run
        self.verbose = verbose
        
        # Ensure output directories exist
        self.outbox_dir = vault_root / "Outbox"
        self.replies_dir = self.outbox_dir
        self.calendar_dir = self.outbox_dir / "_calendar"
        self.prompts_dir = self.outbox_dir / "_prompts"
        
        if not dry_run:
            self.outbox_dir.mkdir(parents=True, exist_ok=True)
            self.calendar_dir.mkdir(parents=True, exist_ok=True)
            self.prompts_dir.mkdir(parents=True, exist_ok=True)

    def _as_vault_relative(self, path: Path) -> str:
        """Return a vault-relative path string when possible."""
        try:
            return str(path.relative_to(self.vault_root))
        except Exception:
            return str(path)
    
    def generate_all(
        self, 
        extraction: UnifiedExtraction, 
        context_bundle: Optional[object] = None,
        source_content: str = "",
        force_reply: bool = False
    ) -> dict:
        """Generate all outputs based on extraction.
        
        Args:
            extraction: The UnifiedExtraction with suggested_outputs
            context_bundle: Optional ContextBundle for reply generation
            source_content: Original content for reply generation
            force_reply: If True, generate reply regardless of needs_reply
        
        Returns:
            Dict with paths to generated files:
            {
                "reply": Path or None,
                "calendar": Path or None,
                "reminder": Path or None,
                "tasks": list of task lines or []
            }
        """
        outputs = {
            "reply": None,
            "calendar": None,
            "reminder": None,
            "tasks": [],
        }
        
        suggested = extraction.suggested_outputs
        
        # Generate reply if needed (or forced for emails)
        if suggested.needs_reply or force_reply:
            outputs["reply"] = self.generate_reply(extraction, context_bundle, source_content)
        
        # Generate calendar invite if suggested
        if suggested.calendar_invite:
            outputs["calendar"] = self.generate_calendar_invite(extraction)
        
        # Generate reminder if suggested
        if suggested.follow_up_reminder:
            outputs["reminder"] = self.generate_reminder(extraction)
        
        # Emit all tasks from extraction to TASKS_INBOX.md
        if extraction.tasks:
            outputs["tasks"] = self.emit_tasks(extraction)
        
        return outputs
    
    def generate_reply(
        self,
        extraction: UnifiedExtraction,
        context_bundle: Optional[object] = None,
        source_content: str = ""
    ) -> Optional[Path]:
        """Generate a draft email reply.
        
        Creates a markdown file with frontmatter metadata and draft body.
        The user can review, edit, and send from their email client.
        
        Args:
            extraction: UnifiedExtraction with email metadata
            context_bundle: Optional ContextBundle for LLM generation
            source_content: Original email content for context
        
        Returns:
            Path to generated draft file, or None if not generated
        """
        suggested = extraction.suggested_outputs
        
        # Note: needs_reply check is handled by caller (generate_all)
        # This method generates a reply unconditionally when called
        
        # Determine recipient from extraction
        # For emails, the sender is in contacts[0] (not participants which includes "Myself")
        sender = "Unknown"
        sender_email = ""
        if extraction.contacts:
            first_contact = extraction.contacts[0]
            sender = first_contact.name if first_contact.name else "Unknown"
            sender_email = first_contact.email or ""
        elif extraction.participants:
            # Fallback to first non-self participant
            for p in extraction.participants:
                if p.lower() not in ("myself", "jason vallery", "jason"):
                    sender = p
                    break
            if sender == "Unknown" and extraction.participants:
                sender = extraction.participants[0]
        
        # Generate filename
        today = datetime.now().strftime("%Y-%m-%d")
        subject_slug = re.sub(r"[^\w\s-]", "", extraction.title or "email")
        subject_slug = re.sub(r"\s+", "-", subject_slug).strip("-")[:40] or "email"

        filename = f"{today}_Reply-To_{subject_slug}.md"
        output_path = self.replies_dir / filename

        # Avoid overwriting
        counter = 1
        while output_path.exists():
            filename = f"{today}_Reply-To_{subject_slug}_{counter}.md"
            output_path = self.replies_dir / filename
            counter += 1

        prompt_path = self.prompts_dir / f"{output_path.stem}.prompt.json"
        prompt_ref = self._as_vault_relative(prompt_path)
        model_config = get_model_config("draft_responses")
        
        # Build draft content
        # Use LLM with persona for high-quality replies
        draft_body = self._build_reply_body(
            extraction,
            suggested.reply_context or "",
            source_content,
            prompt_path=prompt_path,
            draft_path=output_path,
            context_bundle=context_bundle,
        )
        
        # Include email if available
        to_field = f"{sender} <{sender_email}>" if sender_email else sender
        
        content = f"""---
type: draft-reply
status: pending
created: "{datetime.now().isoformat()}"
urgency: "{suggested.reply_urgency}"
to: "{to_field}"
subject: "Re: {extraction.title}"
source_file: "{extraction.source_file}"
ai_model: "{model_config.get('model', '')}"
ai_temperature: {float(model_config.get('temperature', 0.7))}
prompt_file: "{prompt_ref}"
---

# Draft Reply to {sender}

**Regarding**: {extraction.title}
**Urgency**: {suggested.reply_urgency}

---

## Key Points to Address

{suggested.reply_context or "No specific points identified"}

---

## Draft Response

{draft_body}

---

## Original Summary

{extraction.summary}

---

*This draft was auto-generated. Edit and send via your email client.*
"""
        
        if self.dry_run:
            if self.verbose:
                print(f"  [DRY RUN] Would generate reply: {output_path}")
            return output_path
        
        output_path.write_text(content)
        
        if self.verbose:
            print(f"  Generated draft reply: {output_path.name}")
        
        return output_path
    
    def _build_reply_body(
        self,
        extraction: UnifiedExtraction,
        reply_context: str,
        source_content: str = "",
        *,
        prompt_path: Optional[Path] = None,
        draft_path: Optional[Path] = None,
        context_bundle: Optional[object] = None,
    ) -> str:
        """Build the draft reply body using LLM with persona.
        
        Uses the jason_persona.yaml to generate impact-driven, persona-aligned responses.
        Falls back to a simple template if LLM call fails.
        """
        if self.dry_run:
            return self._build_template_reply(extraction, reply_context)

        # Try LLM-based generation
        try:
            return self._generate_llm_reply(
                extraction,
                reply_context,
                source_content,
                prompt_path=prompt_path,
                draft_path=draft_path,
                context_bundle=context_bundle,
            )
        except Exception as e:
            if self.verbose:
                print(f"  [WARN] LLM reply generation failed: {e}, using template fallback")
            return self._build_template_reply(extraction, reply_context)
    
    def _generate_llm_reply(
        self,
        extraction: UnifiedExtraction,
        reply_context: str,
        source_content: str = "",
        *,
        prompt_path: Optional[Path] = None,
        draft_path: Optional[Path] = None,
        context_bundle: Optional[object] = None,
    ) -> str:
        """Generate reply body using LLM with persona."""
        from scripts.utils.ai_client import get_openai_client
        
        model_config = get_model_config("draft_responses")
        persona = _load_persona()
        
        # Build persona context
        identity = persona.get("identity", {})
        style = persona.get("style", {})
        
        # Determine sender
        sender = "Unknown"
        if extraction.contacts:
            sender = extraction.contacts[0].name or "Unknown"
        elif extraction.participants:
            for p in extraction.participants:
                if p.lower() not in ("myself", "jason vallery", "jason"):
                    sender = p
                    break
        first_name = sender.split()[0] if sender != "Unknown" else "there"
        
        # Build questions and commitments
        questions = extraction.questions if hasattr(extraction, 'questions') else []
        commitments = extraction.commitments if hasattr(extraction, 'commitments') else []

        vault_context = ""
        if context_bundle and hasattr(context_bundle, "get_dynamic_suffix"):
            try:
                vault_context = context_bundle.get_dynamic_suffix() or ""
            except Exception:
                vault_context = ""

        raw_excerpt = (source_content or "").strip()
        max_chars = 8000
        if len(raw_excerpt) > max_chars:
            raw_excerpt = raw_excerpt[:max_chars].rstrip() + "\n\n[...TRUNCATED...]"

        system_prompt = f"""You are {identity.get('name', 'Jason Vallery')}, {identity.get('role', 'VP of Product Management for Cloud')} at {identity.get('company', 'VAST Data')}.

## COMMUNICATION STYLE
- Direct but Empathetic: Respect their time while acknowledging their effort
- Warm & Human: Professional warmth, not terse or robotic
- Bias for Action: Use active voice with specific next steps
- Confident & Expert: No hedging on technical facts
- BLUF: Bottom Line Up Front - the answer goes in the first 2 sentences

## FORMATTING RULES
- Start with a greeting (e.g., "Hi {first_name},")
- Short paragraphs (1-2 sentences), but use as many as needed to be helpful
- Use bullets/numbered lists for multi-part answers or steps
- Use specific dates/times, not "soon" or "when you can"
- Professional warmth without emojis
- Avoid em dashes (â€”); use commas or hyphens instead
- Do not mention internal notes/READMEs/manifests; use them only to inform what you say

## SAFETY & FACTUALITY
- Never invent personal/contact details (phone numbers, meeting links, addresses). Only include them if present in the SOURCE EMAIL or provided context.
- Do not fabricate names, titles, companies, or commitments. If something is missing, ask for it or propose a next step without guessing.

## YOUR TASK
Write a complete, ready-to-send email reply. Do NOT include placeholders like [TODO] or [Add answer].
If you don't have enough information to answer something, either:
1. Make a reasonable assumption and answer
2. Acknowledge you'll need to check and get back to them with a specific timeframe

Return ONLY the email body text (no subject line, no markdown headers, no frontmatter)."""

        user_prompt = f"""Write a reply to {sender} about: {extraction.title}

## SOURCE EMAIL (VERBATIM)
{raw_excerpt or "[No source content provided]"}

## EMAIL SUMMARY
{extraction.summary}

## KEY POINTS TO ADDRESS
{reply_context or "No specific points identified"}

## QUESTIONS ASKED
{json.dumps(questions) if questions else "None"}

## COMMITMENTS MADE
{json.dumps(commitments) if commitments else "None"}

## CONTEXT
- Recipient first name: {first_name}
- Urgency: {extraction.suggested_outputs.reply_urgency if extraction.suggested_outputs else "normal"}

## RELEVANT CONTEXT FROM MY NOTES (PRIVATE - DO NOT QUOTE VERBATIM)
{vault_context or "None"}

Write the complete email body now (greeting through signature):"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        if prompt_path and not self.dry_run:
            try:
                prompt_payload = {
                    "created": datetime.now().isoformat(),
                    "operation": "draft_reply",
                    "model": model_config.get("model", ""),
                    "temperature": model_config.get("temperature", 0.7),
                    "source_file": extraction.source_file,
                    "draft_path": str(draft_path) if draft_path else None,
                    "messages": messages,
                }
                prompt_path.write_text(json.dumps(prompt_payload, indent=2))
            except Exception as exc:
                if self.verbose:
                    print(f"  [WARN] Failed to write prompt artifact: {exc}")

        client = get_openai_client("pipeline.outputs.generate_reply")
        if draft_path:
            client.set_context(
                {
                    "source_file": extraction.source_file,
                    "draft_path": str(draft_path),
                    "prompt_path": str(prompt_path) if prompt_path else None,
                }
            )

        response = client.chat.completions.create(
            model=model_config["model"],
            messages=messages,
            temperature=model_config.get("temperature", 0.7),
        )
        
        return response.choices[0].message.content.strip()
    
    def _build_template_reply(self, extraction: UnifiedExtraction, reply_context: str) -> str:
        """Build a simple template-based reply (fallback)."""
        questions = extraction.questions if hasattr(extraction, 'questions') else []
        commitments = extraction.commitments if hasattr(extraction, 'commitments') else []
        
        body_parts = []
        
        # Determine the sender's first name
        sender = "there"
        if extraction.contacts:
            sender = extraction.contacts[0].name or "there"
        elif extraction.participants:
            for p in extraction.participants:
                if p.lower() not in ("myself", "jason vallery", "jason"):
                    sender = p
                    break
        first_name = sender.split()[0] if sender != "there" else "there"
        body_parts.append(f"Hi {first_name},")
        body_parts.append("")
        
        if reply_context:
            body_parts.append("Thank you for your email. Here's my response:")
            body_parts.append("")
            for point in reply_context.split(". "):
                if point.strip():
                    body_parts.append(f"- {point.strip()}")
            body_parts.append("")
        
        if questions:
            body_parts.append("To answer your questions:")
            for q in questions[:3]:
                body_parts.append(f"- {q}: [TODO: Add answer]")
            body_parts.append("")
        
        if commitments:
            body_parts.append("I'll follow up on:")
            for c in commitments[:3]:
                body_parts.append(f"- {c}")
            body_parts.append("")
        
        body_parts.append("Let me know if you have any questions.")
        body_parts.append("")
        body_parts.append("Best,")
        body_parts.append("Jason")
        
        return "\n".join(body_parts)
    
    def generate_calendar_invite(self, extraction: UnifiedExtraction) -> Optional[Path]:
        """Generate a calendar .ics file from suggested calendar invite.
        
        Creates a standard iCalendar file that can be imported into
        any calendar application.
        
        Args:
            extraction: UnifiedExtraction with calendar_invite suggestion
        
        Returns:
            Path to generated .ics file, or None if not generated
        """
        cal_suggest = extraction.suggested_outputs.calendar_invite
        
        if not cal_suggest:
            return None
        
        # Generate filename
        date_str = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        safe_title = "".join(c if c.isalnum() or c in "- " else "_" for c in cal_suggest.title)[:30]
        filename = f"{date_str}_mtg_{safe_title}.ics"
        
        output_path = self.calendar_dir / filename
        
        # Parse proposed date
        try:
            if cal_suggest.proposed_date:
                start_date = datetime.strptime(cal_suggest.proposed_date, "%Y-%m-%d")
                # Default to 10 AM if no time specified
                start_dt = start_date.replace(hour=10, minute=0)
            else:
                # Default to tomorrow at 10 AM
                start_dt = datetime.now().replace(hour=10, minute=0, second=0, microsecond=0) + timedelta(days=1)
        except ValueError:
            start_dt = datetime.now().replace(hour=10, minute=0, second=0, microsecond=0) + timedelta(days=1)
        
        duration = cal_suggest.duration_minutes or 30
        end_dt = start_dt + timedelta(minutes=duration)
        
        # Format for iCal
        dtstart = start_dt.strftime("%Y%m%dT%H%M%S")
        dtend = end_dt.strftime("%Y%m%dT%H%M%S")
        dtstamp = datetime.now().strftime("%Y%m%dT%H%M%SZ")
        uid = f"{date_str}-{extraction.source_file.replace('/', '-')}"
        
        # Build attendee list
        attendees = cal_suggest.attendees or []
        attendee_lines = "\n".join(
            f"ATTENDEE;ROLE=REQ-PARTICIPANT:mailto:{a}@example.com"
            for a in attendees if a
        )
        
        # Build iCal content
        ics_content = f"""BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//VAST Notes Pipeline//Calendar Generator//EN
CALSCALE:GREGORIAN
METHOD:REQUEST
BEGIN:VEVENT
UID:{uid}
DTSTAMP:{dtstamp}
DTSTART:{dtstart}
DTEND:{dtend}
SUMMARY:{cal_suggest.title}
DESCRIPTION:Auto-generated from email/meeting notes.\\nSource: {extraction.source_file}
{attendee_lines}
STATUS:TENTATIVE
END:VEVENT
END:VCALENDAR
"""
        
        if self.dry_run:
            if self.verbose:
                print(f"  [DRY RUN] Would generate calendar: {output_path}")
            return output_path
        
        output_path.write_text(ics_content)
        
        if self.verbose:
            print(f"  Generated calendar invite: {output_path.name}")
        
        return output_path
    
    def generate_reminder(self, extraction: UnifiedExtraction) -> Optional[Path]:
        """Generate a reminder task entry.
        
        Instead of a separate file, appends to TASKS_INBOX.md with
        the proposed [?] status for triage.
        
        Args:
            extraction: UnifiedExtraction with follow_up_reminder suggestion
        
        Returns:
            Path to TASKS_INBOX.md if modified, or None
        """
        reminder = extraction.suggested_outputs.follow_up_reminder
        
        if not reminder:
            return None
        
        tasks_inbox = self.vault_root / "TASKS_INBOX.md"
        
        # Format task with proposed status
        reminder_text = reminder.text
        remind_date = reminder.remind_date or (datetime.now() + timedelta(days=3)).strftime("%Y-%m-%d")
        
        task_line = f"- [?] {reminder_text} ðŸ“… {remind_date} #task #proposed #auto\n"
        
        if self.dry_run:
            if self.verbose:
                print(f"  [DRY RUN] Would add reminder: {task_line.strip()}")
            return tasks_inbox
        
        # Append to TASKS_INBOX.md
        self._append_task_to_inbox(task_line)
        
        if self.verbose:
            print(f"  Added reminder to TASKS_INBOX.md")
        
        return tasks_inbox
    
    def emit_tasks(self, extraction: UnifiedExtraction) -> list[str]:
        """Emit all tasks from extraction to TASKS_INBOX.md.
        
        Formats tasks with Obsidian Tasks plugin syntax:
        - [?] Task text @Owner ðŸ“… YYYY-MM-DD ðŸ”º #task #proposed #auto
        
        Priority markers: ðŸ”º highest â†’ â« high â†’ ðŸ”¼ medium â†’ ðŸ”½ low â†’ â¬ lowest
        
        Args:
            extraction: UnifiedExtraction with tasks
        
        Returns:
            List of task lines emitted
        """
        if not extraction.tasks:
            return []
        
        # Priority emoji mapping
        priority_emoji = {
            "highest": "ðŸ”º",
            "high": "â«",
            "medium": "ðŸ”¼",
            "low": "ðŸ”½",
            "lowest": "â¬",
        }
        
        task_lines = []
        
        for task in extraction.tasks:
            # Build task line with Obsidian Tasks format
            parts = [f"- [?] {task.text}"]
            
            # Add owner if specified (not myself)
            if task.owner and task.owner.lower() not in ["myself", "me", "i"]:
                parts.append(f"@{task.owner}")
            
            # Add due date if specified
            if task.due:
                parts.append(f"ðŸ“… {task.due}")
            
            # Add priority marker
            priority = task.priority.lower() if task.priority else "medium"
            if priority in priority_emoji:
                parts.append(priority_emoji[priority])
            
            # Add tags
            parts.append("#task #proposed #auto")
            
            task_line = " ".join(parts) + "\n"
            task_lines.append(task_line)
            
            if not self.dry_run:
                self._append_task_to_inbox(task_line)
            elif self.verbose:
                print(f"  [DRY RUN] Would emit task: {task_line.strip()}")
        
        if self.verbose and not self.dry_run:
            print(f"  Emitted {len(task_lines)} tasks to TASKS_INBOX.md")
        
        return task_lines
    
    def _append_task_to_inbox(self, task_line: str):
        """Append a task line to TASKS_INBOX.md.
        
        Creates the file with proper structure if it doesn't exist.
        """
        tasks_inbox = self.vault_root / "TASKS_INBOX.md"
        
        if tasks_inbox.exists():
            existing = tasks_inbox.read_text()
            # Add under "## Inbox" section if it exists, or at end
            if "## Inbox" in existing:
                existing = existing.replace("## Inbox\n", f"## Inbox\n{task_line}")
            else:
                existing += f"\n{task_line}"
            tasks_inbox.write_text(existing)
        else:
            tasks_inbox.write_text(f"# Tasks Inbox\n\n## Inbox\n{task_line}")


def generate_outputs_from_extraction(
    extraction: UnifiedExtraction,
    vault_root: Path,
    dry_run: bool = False,
    verbose: bool = False,
) -> dict:
    """Convenience function to generate all outputs from an extraction.
    
    Args:
        extraction: UnifiedExtraction result
        vault_root: Path to vault root
        dry_run: If True, don't write files
        verbose: If True, log output generation
    
    Returns:
        Dict with paths to generated files
    """
    generator = OutputGenerator(vault_root, dry_run=dry_run, verbose=verbose)
    return generator.generate_all(extraction)

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/patch.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Patch Generator - Generate vault patches from extractions.

Key insight: We patch entities when we LEARN something about them (facts),
not just because they're mentioned.
"""

import sys
from datetime import datetime
from pathlib import Path
from typing import Optional
from pydantic import BaseModel, Field

sys.path.insert(0, str(Path(__file__).parent.parent))

from .models import UnifiedExtraction, MentionedEntity
from .entities import EntityIndex


class PatchOperation(BaseModel):
    """A single patch operation to apply to the vault."""
    
    operation: str  # create, patch, link
    target_path: str  # Relative path in vault
    target_entity: str  # Entity name for logging
    
    # For create operations
    template: Optional[str] = None
    template_context: Optional[dict] = None
    
    # For patch operations
    add_frontmatter: Optional[dict] = None
    add_facts: Optional[list[str]] = None
    add_topics: Optional[list[str]] = None
    add_decisions: Optional[list[str]] = None
    add_context: Optional[str] = None
    add_tasks: Optional[list[dict]] = None
    
    # For link operations
    add_wikilinks: Optional[list[str]] = None


class ManifestPatch(BaseModel):
    """A patch to update a manifest file (People or Projects)."""
    
    manifest_type: str  # "people" or "projects"
    manifest_path: str  # Relative path to manifest
    
    # For People manifest - add aliases to a person
    person_name: Optional[str] = None
    aliases_to_add: list[str] = Field(default_factory=list)
    
    # For Projects manifest - add acronyms/definitions
    project_name: Optional[str] = None
    acronym: Optional[str] = None
    definition: Optional[str] = None


class ChangePlan(BaseModel):
    """Complete change plan for a content extraction."""
    
    version: str = "2.0"
    source_file: str
    extraction_file: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.now)
    
    # Meeting note creation
    meeting_note_path: Optional[str] = None
    meeting_note: Optional[dict] = None
    
    # Entity patches
    patches: list[PatchOperation] = Field(default_factory=list)
    
    # Manifest patches (for discovered aliases/acronyms)
    manifest_patches: list[ManifestPatch] = Field(default_factory=list)
    
    # Entities to create (for new entities)
    entities_to_create: list[dict] = Field(default_factory=list)
    
    # Warnings
    warnings: list[str] = Field(default_factory=list)
    
    def validate_plan(self) -> list[str]:
        """Validate the change plan and return a list of issues.
        
        Returns:
            List of validation issues (empty if plan is valid)
        """
        issues = []
        
        # 1. Check source file is set
        if not self.source_file:
            issues.append("source_file is required")
        
        # 2. Check meeting note path is valid if set
        if self.meeting_note_path:
            if ".." in self.meeting_note_path:
                issues.append(f"meeting_note_path contains invalid path traversal: {self.meeting_note_path}")
            if not self.meeting_note:
                issues.append("meeting_note_path is set but meeting_note context is missing")
        
        # 3. Validate each patch operation
        patch_targets = set()
        for i, patch in enumerate(self.patches):
            # Check for duplicate targets
            if patch.target_path in patch_targets:
                issues.append(f"Duplicate patch target: {patch.target_path}")
            patch_targets.add(patch.target_path)
            
            # Validate operation type
            if patch.operation not in ("create", "patch", "link"):
                issues.append(f"Patch {i}: Invalid operation '{patch.operation}'")
            
            # Check target path validity
            if ".." in patch.target_path:
                issues.append(f"Patch {i}: target_path contains invalid path traversal")
            
            # Check that patch operations have something to patch
            if patch.operation == "patch":
                has_changes = any([
                    patch.add_frontmatter,
                    patch.add_facts,
                    patch.add_topics,
                    patch.add_decisions,
                    patch.add_context,
                    patch.add_tasks,
                ])
                if not has_changes:
                    issues.append(f"Patch {i}: patch operation has no changes for {patch.target_entity}")
        
        return issues
    
    def is_valid(self) -> bool:
        """Check if the change plan is valid."""
        return len(self.validate_plan()) == 0


class PatchGenerator:
    """Generate vault patches from extractions.
    
    Patching rules:
    1. Primary entity always gets patched (context, tasks)
    2. Entities with facts_about get patched (we learned something)
    3. Participants get context updates (lighter touch)
    4. Companies get patched if facts are discovered (not just mentions)
    """
    
    # Companies to skip (internal/obvious)
    SKIP_COMPANIES = {
        "vast data", "vast", "microsoft", "google", "amazon", "aws",
        "openai", "anthropic", "nvidia"  # Can be configured
    }
    
    def __init__(self, vault_root: Path, entity_index: Optional[EntityIndex] = None):
        self.vault_root = vault_root
        self.entity_index = entity_index or EntityIndex(vault_root)
        # Track folders created during this plan generation (not yet in manifests)
        self._created_folders: dict[str, Path] = {}  # normalized name -> folder path
    
    def generate(self, extraction: UnifiedExtraction) -> ChangePlan:
        """Generate change plan from extraction.
        
        Uses EntityIndex for alias resolution and duplicate detection.
        
        Args:
            extraction: UnifiedExtraction with all extracted knowledge
        
        Returns:
            ChangePlan with all operations
        """
        # Reset created folders for this plan generation
        self._created_folders = {}
        
        plan = ChangePlan(source_file=extraction.source_file)
        
        # Track patched targets to avoid duplicates
        patched_paths: set[str] = set()
        
        # Track entity names we've processed (for dedup warnings)
        processed_entities: dict[str, str] = {}  # normalized name -> original name
        
        # 1. Meeting note creation
        note_path, note_context = self._generate_meeting_note(extraction)
        if note_path:
            plan.meeting_note_path = note_path
            plan.meeting_note = note_context
        
        # 2. Primary entity patches
        if extraction.primary_entity:
            normalized = self.entity_index.normalize_name(extraction.primary_entity.name)
            if normalized != extraction.primary_entity.name:
                plan.warnings.append(f"Resolved alias: '{extraction.primary_entity.name}' â†’ '{normalized}'")
            processed_entities[normalized.lower()] = extraction.primary_entity.name
            self._warn_duplicate(extraction.primary_entity.name, extraction.primary_entity.entity_type, plan)
            
            patches = self._generate_primary_patches(extraction)
            for patch in patches:
                if patch.target_path not in patched_paths:
                    plan.patches.append(patch)
                    patched_paths.add(patch.target_path)
        
        # 3. Entities with facts (we learned something)
        for entity in extraction.get_entities_with_facts():
            normalized = self.entity_index.normalize_name(entity.name)
            norm_lower = normalized.lower()
            
            # Check for potential duplicates
            if norm_lower in processed_entities and processed_entities[norm_lower] != entity.name:
                plan.warnings.append(
                    f"Potential duplicate: '{entity.name}' may be same as '{processed_entities[norm_lower]}' (consider merge)"
                )
                continue  # Skip to avoid double-patching
            
            if normalized != entity.name:
                plan.warnings.append(f"Resolved alias: '{entity.name}' â†’ '{normalized}'")
            processed_entities[norm_lower] = entity.name
            self._warn_duplicate(entity.name, entity.entity_type, plan)
            
            patches = self._generate_fact_patches(entity, extraction)
            for patch in patches:
                if patch.target_path not in patched_paths:
                    plan.patches.append(patch)
                    patched_paths.add(patch.target_path)
        
        # 4. Participant context updates
        for participant in extraction.participants:
            normalized = self.entity_index.normalize_name(participant)
            norm_lower = normalized.lower()
            
            # Check for potential duplicates
            if norm_lower in processed_entities:
                continue  # Already processed
            
            processed_entities[norm_lower] = participant
            
            patches = self._generate_participant_patches(participant, extraction)
            for patch in patches:
                if patch.target_path not in patched_paths:
                    plan.patches.append(patch)
                    patched_paths.add(patch.target_path)
        
        # 5. Company patches (only if facts discovered)
        for entity in extraction.mentioned_entities:
            if entity.entity_type == "company" and entity.facts_about:
                company_lower = entity.name.lower()
                if company_lower not in self.SKIP_COMPANIES:
                    patches = self._generate_company_patches(entity, extraction)
                    for patch in patches:
                        if patch.target_path not in patched_paths:
                            plan.patches.append(patch)
                            patched_paths.add(patch.target_path)
        
        # 6. Manifest patches (discovered aliases and acronyms)
        manifest_patches = self._generate_manifest_patches(extraction)
        plan.manifest_patches.extend(manifest_patches)
        
        return plan
    
    def _generate_meeting_note(self, extraction: UnifiedExtraction) -> tuple[Optional[str], Optional[dict]]:
        """Generate meeting note path and context."""
        
        # Determine destination folder
        folder: Optional[Path] = None
        entity_name: Optional[str] = None
        entity_type: Optional[str] = None

        note_type = (extraction.note_type or "people").lower()
        desired_entity_type = {
            "people": "person",
            "customer": "company",
            "partners": "company",
            "projects": "project",
        }.get(note_type)

        # Pick an entity matching the note_type when possible (prevents customer/project
        # notes from being filed under a person just because a person was the sender).
        candidate = None
        if extraction.primary_entity and (
            desired_entity_type is None or extraction.primary_entity.entity_type == desired_entity_type
        ):
            candidate = extraction.primary_entity
        elif desired_entity_type in ("company", "project"):
            best = None
            for e in extraction.mentioned_entities or []:
                if e.entity_type != desired_entity_type:
                    continue
                if best is None or (e.confidence or 0) > (best.confidence or 0):
                    best = e
            candidate = best
        elif desired_entity_type == "person" and extraction.participants:
            # Default to first participant (skip self)
            for participant in extraction.participants:
                if participant.lower() in ["myself", "jason", "jason vallery"]:
                    continue
                email = self._get_email_for_participant(participant, extraction)
                folder = self.entity_index.find_person(participant, email=email)
                entity_name = participant
                entity_type = "person"
                break

        if not folder and candidate:
            folder = self._get_entity_folder(candidate, extraction)
            entity_name = candidate.name
            entity_type = candidate.entity_type
        
        if not folder and extraction.primary_entity:
            # Fallback to primary entity even when it doesn't match note_type
            folder = self._get_entity_folder(extraction.primary_entity, extraction)
            entity_name = entity_name or extraction.primary_entity.name
            entity_type = entity_type or extraction.primary_entity.entity_type
        
        if not folder and entity_name:
            # Create entity folder on-demand
            folder = self._create_entity_folder(entity_name, entity_type, extraction)
        
        if not folder:
            return None, None
        
        # Sanitize title for filename
        from scripts.utils.templates import sanitize_path_name
        safe_title = sanitize_path_name(extraction.title)
        
        note_path = f"{folder.relative_to(self.vault_root)}/{extraction.date} - {safe_title}.md"

        # Select template type. Use the extracted note_type only when it matches the resolved
        # entity folder; otherwise prefer a type that won't emit placeholder headers/tags.
        template_type = note_type
        if desired_entity_type is not None and entity_type:
            if entity_type == "person":
                template_type = "people"
            elif entity_type == "project":
                template_type = "projects"
            elif entity_type == "company":
                template_type = note_type if note_type in ("customer", "partners") else "customer"
        
        # Build context for template
        context = {
            "title": extraction.title,
            "date": extraction.date,
            "type": template_type,
            # Template expects `source` for correct defaulting.
            "source": extraction.content_type,
            # Entity key fields expected by templates (people/customer/projects).
            "person": entity_name if entity_type == "person" else "",
            "account": entity_name if entity_type == "company" else "",
            "project": entity_name if entity_type == "project" else "",
            "participants": extraction.participants,
            "summary": extraction.summary,
            "topics": extraction.topics,
            "decisions": extraction.decisions,
            "tasks": [t.model_dump() for t in extraction.tasks],
            "facts": [f.text for f in extraction.facts],
            "source_ref": extraction.source_file,
        }
        
        return note_path, context
    
    def _generate_primary_patches(self, extraction: UnifiedExtraction) -> list[PatchOperation]:
        """Generate patches for the primary entity."""
        patches = []
        
        if not extraction.primary_entity:
            return patches
        
        folder = self._get_entity_folder(extraction.primary_entity, extraction)
        if not folder:
            return patches
        
        readme_path = folder / "README.md"
        if not readme_path.exists():
            return patches
        
        # Context entry
        context_entry = f"- {extraction.date}: {extraction.summary[:100]}..."
        
        # Facts from extraction
        facts = [f.text for f in extraction.facts 
                 if f.about_entity and f.about_entity.name == extraction.primary_entity.name]
        
        # Build frontmatter with contact info if available
        frontmatter = {"last_contact": extraction.date}
        if extraction.primary_entity.entity_type == "person":
            contact_info = self._get_contact_info_for_person(extraction.primary_entity.name, extraction)
            frontmatter.update(contact_info)
        
        patches.append(PatchOperation(
            operation="patch",
            target_path=str(readme_path.relative_to(self.vault_root)),
            target_entity=extraction.primary_entity.name,
            add_frontmatter=frontmatter,
            add_facts=facts if facts else None,
            add_topics=extraction.topics[:5] if extraction.topics else None,
            add_decisions=extraction.decisions if extraction.decisions else None,
            add_context=context_entry,
        ))
        
        return patches
    
    def _generate_fact_patches(self, entity: MentionedEntity, extraction: UnifiedExtraction) -> list[PatchOperation]:
        """Generate patches for entities we learned facts about."""
        patches = []
        
        folder = self._get_entity_folder(entity, extraction)
        
        if not folder:
            return patches
        
        readme_path = folder / "README.md"
        if not readme_path.exists():
            return patches
        
        patches.append(PatchOperation(
            operation="patch",
            target_path=str(readme_path.relative_to(self.vault_root)),
            target_entity=entity.name,
            add_facts=entity.facts_about,
            add_context=f"- {extraction.date}: Mentioned in: {extraction.title}",
        ))
        
        return patches
    
    def _generate_participant_patches(self, participant: str, extraction: UnifiedExtraction) -> list[PatchOperation]:
        """Generate light-touch patches for participants."""
        patches = []
        
        # Skip myself
        if participant.lower() in ["myself", "jason", "jason vallery"]:
            return patches
        
        # Find person folder (check _created_folders first)
        normalized = self.entity_index.normalize_name(participant).lower()
        folder = self._created_folders.get(normalized)
        if not folder:
            email = self._get_email_for_participant(participant, extraction)
            folder = self.entity_index.find_person(participant, email=email)
        if not folder:
            return patches
        
        readme_path = folder / "README.md"
        if not readme_path.exists():
            return patches
        
        # Just add context entry (no facts or topics)
        context_entry = f"- {extraction.date}: {extraction.title}"
        
        # Build frontmatter with contact info if available
        frontmatter = {"last_contact": extraction.date}
        contact_info = self._get_contact_info_for_person(participant, extraction)
        frontmatter.update(contact_info)
        
        patches.append(PatchOperation(
            operation="patch",
            target_path=str(readme_path.relative_to(self.vault_root)),
            target_entity=participant,
            add_frontmatter=frontmatter,
            add_context=context_entry,
        ))
        
        return patches
    
    def _generate_company_patches(self, entity: MentionedEntity, extraction: UnifiedExtraction) -> list[PatchOperation]:
        """Generate patches for companies we learned facts about."""
        patches = []
        
        # Check _created_folders first
        normalized = self.entity_index.normalize_name(entity.name).lower()
        folder = self._created_folders.get(normalized)
        if not folder:
            folder = self.entity_index.find_company(entity.name)
        if not folder:
            return patches
        
        readme_path = folder / "README.md"
        if not readme_path.exists():
            return patches
        
        patches.append(PatchOperation(
            operation="patch",
            target_path=str(readme_path.relative_to(self.vault_root)),
            target_entity=entity.name,
            add_facts=entity.facts_about,
            add_context=f"- {extraction.date}: {extraction.title}",
        ))
        
        return patches
    
    def _get_email_for_participant(self, name: str, extraction: UnifiedExtraction) -> Optional[str]:
        """Attempt to find an email address for a participant from contacts."""
        if not extraction.contacts:
            return None
        
        normalized_target = self.entity_index.normalize_name(name).lower()
        for contact in extraction.contacts:
            if not contact.email:
                continue
            contact_name = contact.name or ""
            normalized_contact = self.entity_index.normalize_name(contact_name).lower() if contact_name else ""
            if contact_name and normalized_contact == normalized_target:
                return contact.email
        if "@" in name:
            return name
        return None
    
    def _get_contact_info_for_person(self, name: str, extraction: UnifiedExtraction) -> dict:
        """Get full contact info (email, title, company) for a person from contacts."""
        result = {}
        if not extraction.contacts:
            return result
        
        normalized_target = self.entity_index.normalize_name(name).lower()
        for contact in extraction.contacts:
            contact_name = contact.name or ""
            normalized_contact = self.entity_index.normalize_name(contact_name).lower() if contact_name else ""
            if contact_name and normalized_contact == normalized_target:
                # Only include non-empty fields
                if contact.email:
                    result["email"] = contact.email
                if contact.title:
                    # Align with README schema: `title` is the person's name; job title goes in `role`.
                    result["role"] = contact.title
                if contact.company:
                    result["company"] = contact.company
                if contact.phone:
                    result["phone"] = contact.phone
                break
        return result

    def _get_entity_folder(self, entity, extraction: Optional[UnifiedExtraction] = None) -> Optional[Path]:
        """Get folder path for an entity reference.
        
        Checks both EntityIndex (for existing entities) and _created_folders
        (for entities created during this plan generation).
        """
        # First check if we created this folder during this plan generation
        normalized = self.entity_index.normalize_name(entity.name).lower()
        if normalized in self._created_folders:
            return self._created_folders[normalized]
        
        email = None
        if extraction and entity.entity_type == "person":
            email = self._get_email_for_participant(entity.name, extraction)
        if entity.entity_type == "person":
            return self.entity_index.find_person(entity.name, email=email)
        elif entity.entity_type == "company":
            return self.entity_index.find_company(entity.name)
        elif entity.entity_type == "project":
            return self.entity_index.find_project(entity.name)
        return None

    def _create_entity_folder(self, name: str, entity_type: str, extraction: UnifiedExtraction) -> Optional[Path]:
        """Create a new entity folder on-demand.
        
        This enables the pipeline to work with an empty vault - entity folders
        are created as content is ingested.
        """
        from scripts.utils.templates import sanitize_path_name
        
        safe_name = sanitize_path_name(name)
        
        if entity_type == "person":
            folder = self.vault_root / "VAST" / "People" / safe_name
        elif entity_type == "company":
            folder = self.vault_root / "VAST" / "Customers and Partners" / safe_name
        elif entity_type == "project":
            folder = self.vault_root / "VAST" / "Projects" / safe_name
        else:
            # Default to People for unknown types
            folder = self.vault_root / "VAST" / "People" / safe_name
        
        # Create folder and README
        folder.mkdir(parents=True, exist_ok=True)
        
        readme_path = folder / "README.md"
        if not readme_path.exists():
            # Create minimal README with frontmatter
            email = self._get_email_for_participant(name, extraction) if entity_type == "person" else None
            readme_content = self._generate_readme_content(name, entity_type, email, extraction)
            readme_path.write_text(readme_content)
        
        # Register this folder so _get_entity_folder() can find it during this plan
        normalized = self.entity_index.normalize_name(name).lower()
        self._created_folders[normalized] = folder
        
        return folder

    def _generate_readme_content(self, name: str, entity_type: str, email: Optional[str], extraction: UnifiedExtraction) -> str:
        """Generate initial README.md content for a new entity."""
        from datetime import date
        from scripts.utils.frontmatter import render_frontmatter
        from scripts.utils.templates import sanitize_path_name
        
        today = date.today().isoformat()
        safe_name = sanitize_path_name(name)
        
        if entity_type == "person":
            # Try to get company from contacts
            company = None
            title = None
            for contact in extraction.contacts:
                if contact.name and contact.name.lower() in name.lower():
                    company = contact.company
                    title = contact.title
                    break
            
            fm = {
                "type": "people",
                "title": name,
                "created": today,
                "last_contact": extraction.date,
                "email": email or "",
                "company": company or "",
                "role": title or "",
                "tags": ["type/people", "needs-review"],
            }

            body = f"""# {name}

## Key Facts

## Recent Context

## Open Tasks

```tasks
path includes {safe_name}
not done
```

## Topics

## Key Decisions
"""
            return render_frontmatter(fm) + body
        elif entity_type == "company":
            fm = {
                "type": "customer",
                "title": name,
                "account_type": "",
                "status": "",
                "industry": "_Unknown_",
                "created": today,
                "last_contact": extraction.date,
                "tags": ["type/customer", "needs-review"],
            }

            body = f"""# {name}

## Account Status

| Field | Value |
|-------|-------|
| **Status** | _Unknown_ |
| **Industry** | _Unknown_ |

## Key Contacts

## Open Tasks

```tasks
path includes {safe_name}
not done
```

## Recent Context

## Key Facts

## Topics

## Key Decisions
"""
            return render_frontmatter(fm) + body
        else:
            fm = {
                "type": "projects",
                "title": name,
                "status": "active",
                "created": today,
                "last_contact": extraction.date,
                "tags": ["type/projects", "status/active", "needs-review"],
            }

            body = f"""# {name}

## Status

| Field | Value |
|-------|-------|
| **Status** | active |
| **Owner** | _Unknown_ |

## Overview

## Open Tasks

```tasks
path includes {safe_name}
not done
```

## Recent Context

## Key Facts

## Topics

## Key Decisions
"""
            return render_frontmatter(fm) + body

    def _warn_duplicate(self, name: str, entity_type: str, plan: ChangePlan):
        """Warn when a similar entity already exists (merge guidance)."""
        similar: list[str] = []
        if entity_type == "person":
            similar = self.entity_index.find_similar_people(name)
        elif entity_type == "company":
            similar = self.entity_index.find_similar_companies(name)
        elif entity_type == "project":
            similar = self.entity_index.find_similar_projects(name)
        
        if similar:
            plan.warnings.append(
                f"Potential duplicate for '{name}': similar to {', '.join(similar[:2])} (consider merge)"
            )

    def _generate_manifest_patches(self, extraction: UnifiedExtraction) -> list[ManifestPatch]:
        """Generate manifest patches for discovered aliases and acronyms.
        
        Updates:
        - People manifest: add aliases to existing people rows
        - Projects manifest: add acronym/definition for new terms
        """
        patches = []
        
        # 1. Process discovered aliases (People manifest)
        if extraction.discovered_aliases:
            people_manifest_path = "VAST/People/_MANIFEST.md"
            
            # Group aliases by person
            aliases_by_person: dict[str, list[str]] = {}
            for alias_discovery in extraction.discovered_aliases:
                # Resolve canonical name through entity index
                canonical = self.entity_index.normalize_name(alias_discovery.canonical_name)
                if canonical not in aliases_by_person:
                    aliases_by_person[canonical] = []
                aliases_by_person[canonical].append(alias_discovery.alias)
            
            # Create patches for each person
            for person_name, aliases in aliases_by_person.items():
                # Verify person exists
                folder = self.entity_index.find_person(person_name)
                if folder:
                    patches.append(ManifestPatch(
                        manifest_type="people",
                        manifest_path=people_manifest_path,
                        person_name=person_name,
                        aliases_to_add=aliases,
                    ))
        
        # 2. Process discovered acronyms (Projects manifest)
        if extraction.discovered_acronyms:
            projects_manifest_path = "VAST/Projects/_MANIFEST.md"
            
            for acronym_discovery in extraction.discovered_acronyms:
                # Check if project exists
                project_name = acronym_discovery.project_name
                if project_name:
                    folder = self.entity_index.find_project(project_name)
                    if folder:
                        patches.append(ManifestPatch(
                            manifest_type="projects",
                            manifest_path=projects_manifest_path,
                            project_name=project_name,
                            acronym=acronym_discovery.acronym,
                            definition=acronym_discovery.expansion,
                        ))
                else:
                    # General acronym (not tied to a specific project)
                    # Add as a new row in projects manifest
                    patches.append(ManifestPatch(
                        manifest_type="projects",
                        manifest_path=projects_manifest_path,
                        project_name=acronym_discovery.acronym,  # Use acronym as project name
                        acronym=acronym_discovery.acronym,
                        definition=acronym_discovery.expansion,
                    ))
        
        return patches


class PatchCollector:
    """Collect and merge patches from multiple extractions.
    
    Used in parallel processing to:
    1. Collect all patches from concurrent extractions
    2. Group by target path
    3. Merge patches for same targets (dedup facts, combine context)
    4. Return a single merged ChangePlan for atomic apply
    """
    
    def __init__(self, dedupe_facts: bool = True, dedupe_tasks: bool = True, combine_context: bool = True):
        self.dedupe_facts = dedupe_facts
        self.dedupe_tasks = dedupe_tasks
        self.combine_context = combine_context
        
        # Collected data
        self._patches_by_target: dict[str, list[PatchOperation]] = {}
        self._meeting_notes: list[tuple[str, dict]] = []  # (path, context)
        self._source_files: list[str] = []
        self._warnings: list[str] = []
        
        # Manifest patches (aliases and acronyms)
        self._manifest_patches: list[ManifestPatch] = []
    
    @property
    def has_patches(self) -> bool:
        """Check if any patches have been collected."""
        return bool(self._patches_by_target) or bool(self._manifest_patches)
    
    @property
    def patch_count(self) -> int:
        """Get total number of patches across all targets."""
        return sum(len(patches) for patches in self._patches_by_target.values()) + len(self._manifest_patches)
    
    def collect(self, plan: ChangePlan):
        """Collect patches from a single ChangePlan."""
        self._source_files.append(plan.source_file)
        
        # Collect meeting note
        if plan.meeting_note_path and plan.meeting_note:
            self._meeting_notes.append((plan.meeting_note_path, plan.meeting_note))
        
        # Collect patches by target
        for patch in plan.patches:
            target = patch.target_path
            if target not in self._patches_by_target:
                self._patches_by_target[target] = []
            self._patches_by_target[target].append(patch)
        
        # Collect manifest patches
        self._manifest_patches.extend(plan.manifest_patches)
        
        # Collect warnings
        self._warnings.extend(plan.warnings)
    
    def merge(self) -> ChangePlan:
        """Merge all collected patches into a single ChangePlan."""
        merged_patches: list[PatchOperation] = []
        
        for target_path, patches in self._patches_by_target.items():
            if len(patches) == 1:
                merged_patches.append(patches[0])
            else:
                merged = self._merge_patches_for_target(patches)
                merged_patches.append(merged)
        
        # Merge manifest patches
        merged_manifest_patches = self._merge_manifest_patches()
        
        # Create merged plan
        plan = ChangePlan(
            source_file=", ".join(self._source_files[:3]) + (f" (+{len(self._source_files) - 3} more)" if len(self._source_files) > 3 else ""),
            patches=merged_patches,
            manifest_patches=merged_manifest_patches,
            warnings=self._warnings,
        )
        
        # Set first meeting note (others are created separately)
        if self._meeting_notes:
            plan.meeting_note_path = self._meeting_notes[0][0]
            plan.meeting_note = self._meeting_notes[0][1]
        
        return plan
    
    def get_meeting_notes(self) -> list[tuple[str, dict]]:
        """Get all collected meeting notes for creation."""
        return self._meeting_notes
    
    def _merge_manifest_patches(self) -> list[ManifestPatch]:
        """Merge manifest patches, deduplicating aliases/acronyms."""
        if not self._manifest_patches:
            return []
        
        # Group by manifest_path + person/project name
        people_aliases: dict[str, set[str]] = {}  # person_name -> aliases
        project_acronyms: dict[str, tuple[str, str]] = {}  # project_name -> (acronym, definition)
        
        for patch in self._manifest_patches:
            if patch.manifest_type == "people" and patch.person_name:
                key = patch.person_name
                if key not in people_aliases:
                    people_aliases[key] = set()
                people_aliases[key].update(patch.aliases_to_add)
            elif patch.manifest_type == "projects" and patch.project_name:
                key = patch.project_name
                if key not in project_acronyms and patch.acronym:
                    project_acronyms[key] = (patch.acronym, patch.definition or "")
        
        # Create merged patches
        merged = []
        
        for person_name, aliases in people_aliases.items():
            merged.append(ManifestPatch(
                manifest_type="people",
                manifest_path="VAST/People/_MANIFEST.md",
                person_name=person_name,
                aliases_to_add=list(aliases),
            ))
        
        for project_name, (acronym, definition) in project_acronyms.items():
            merged.append(ManifestPatch(
                manifest_type="projects",
                manifest_path="VAST/Projects/_MANIFEST.md",
                project_name=project_name,
                acronym=acronym,
                definition=definition,
            ))
        
        return merged

    def _merge_patches_for_target(self, patches: list[PatchOperation]) -> PatchOperation:
        """Merge multiple patches targeting the same file."""
        first = patches[0]
        
        merged = PatchOperation(
            operation="patch",
            target_path=first.target_path,
            target_entity=first.target_entity,
            add_frontmatter={},
            add_facts=[],
            add_topics=[],
            add_decisions=[],
            add_context="",
            add_tasks=[],
            add_wikilinks=[],
        )
        
        seen_facts: set[str] = set()
        seen_tasks: set[str] = set()
        context_parts: list[str] = []
        
        for patch in patches:
            # Merge frontmatter
            if patch.add_frontmatter:
                for key, value in patch.add_frontmatter.items():
                    # Later values overwrite (e.g., last_contact date)
                    merged.add_frontmatter[key] = value
            
            # Merge facts (dedup if enabled)
            if patch.add_facts:
                for fact in patch.add_facts:
                    fact_key = fact.lower().strip() if self.dedupe_facts else fact
                    if fact_key not in seen_facts:
                        merged.add_facts.append(fact)
                        seen_facts.add(fact_key)
            
            # Merge topics
            if patch.add_topics:
                for topic in patch.add_topics:
                    if topic not in merged.add_topics:
                        merged.add_topics.append(topic)
            
            # Merge decisions
            if patch.add_decisions:
                for decision in patch.add_decisions:
                    if decision not in merged.add_decisions:
                        merged.add_decisions.append(decision)
            
            # Merge context
            if patch.add_context:
                if self.combine_context:
                    if patch.add_context not in context_parts:
                        context_parts.append(patch.add_context)
                else:
                    merged.add_context = patch.add_context  # Last wins
            
            # Merge tasks (dedup by text if enabled)
            if patch.add_tasks:
                for task in patch.add_tasks:
                    task_key = task.get("text", "").lower().strip() if self.dedupe_tasks else str(task)
                    if task_key not in seen_tasks:
                        merged.add_tasks.append(task)
                        seen_tasks.add(task_key)
            
            # Merge wikilinks
            if patch.add_wikilinks:
                for link in patch.add_wikilinks:
                    if link not in merged.add_wikilinks:
                        merged.add_wikilinks.append(link)
        
        # Combine context parts
        if context_parts:
            merged.add_context = "\n".join(context_parts)
        
        return merged

========================================================================================================================

========================================================================================================================
GROUP: PIPELINE
PATH: Workflow/pipeline/pipeline.py
ROLE: Unified pipeline module
========================================================================================================================

"""
Unified Pipeline - Main orchestrator for content processing.

Combines:
- Adapters (normalize content)
- Context (load manifests, persona, glossary)
- Extraction (LLM structured output)
- Patching (generate patches)
- Apply (transactional execution)
- Enrichment (trigger for new entities)

Supports parallel extraction with deferred patch application.
"""

import json
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Optional, Any
from dataclasses import dataclass, field

sys.path.insert(0, str(Path(__file__).parent.parent))

from .envelope import ContentEnvelope, ContentType
from .adapters import AdapterRegistry
from .context import ContextBundle
from .extract import UnifiedExtractor
from .patch import PatchGenerator, ChangePlan, PatchCollector
from .apply import TransactionalApply, ApplyResult
from .entities import EntityIndex
from .outputs import OutputGenerator
from .models import ContactInfo, UnifiedExtraction
from scripts.utils.ai_client import log_pipeline_stats
from scripts.utils.config import load_config


@dataclass
class ProcessingResult:
    """Result of processing a single content item."""
    
    source_path: str
    content_type: str
    success: bool = True
    
    # Artifacts
    envelope: Optional[ContentEnvelope] = None
    extraction: Optional[dict] = None
    plan: Optional[ChangePlan] = None
    apply_result: Optional[ApplyResult] = None
    
    # Output suggestions
    draft_reply: Optional[str] = None
    calendar_invite: Optional[dict] = None
    outputs: dict = field(default_factory=dict)
    
    # Metrics
    metrics: dict = field(default_factory=dict)
    
    # Errors
    errors: list[str] = field(default_factory=list)
    
    def __str__(self):
        if self.success:
            return f"âœ“ {self.source_path}"
        else:
            return f"âœ— {self.source_path}: {', '.join(self.errors)}"


@dataclass
class BatchResult:
    """Result of processing a batch of content."""
    
    total: int = 0
    success: int = 0
    failed: int = 0
    skipped: int = 0
    results: list[ProcessingResult] = field(default_factory=list)
    metrics: dict = field(default_factory=dict)
    
    def __str__(self):
        return f"Processed {self.total}: {self.success} success, {self.failed} failed, {self.skipped} skipped"


class UnifiedPipeline:
    """Unified content processing pipeline.
    
    Usage:
        pipeline = UnifiedPipeline(vault_root)
        
        # Process single file
        result = pipeline.process_file(Path("Inbox/Email/..."))
        
        # Process all pending content
        batch = pipeline.process_all()
        
        # Process specific content type
        batch = pipeline.process_type(ContentType.EMAIL)
    """
    
    def __init__(
        self,
        vault_root: Path,
        dry_run: bool = False,
        verbose: bool = False,
        generate_outputs: bool = True,
        draft_all_emails: bool = False,
        force: bool = False,
        trace_dir: Optional[Path] = None,
        show_cache_stats: bool = False,
        log_metrics: bool = True,
        config: Optional[dict[str, Any]] = None,
        max_workers: Optional[int] = None,
    ):
        self.vault_root = vault_root
        self.dry_run = dry_run
        self.verbose = verbose
        self.generate_outputs = generate_outputs
        self.draft_all_emails = draft_all_emails
        self.force = force
        self.trace_dir = trace_dir
        self.show_cache_stats = show_cache_stats
        self.log_metrics = log_metrics
        self.config = config or load_config(vault_root_override=vault_root)
        
        # Parallel processing settings
        parallel_cfg = self.config.get("parallel", {})
        self.max_workers = max_workers if max_workers is not None else parallel_cfg.get("max_workers", 1)
        self.parallel_enabled = parallel_cfg.get("enabled", False) and self.max_workers > 1
        rate_limit_cfg = parallel_cfg.get("rate_limit", {})
        self.requests_per_minute = rate_limit_cfg.get("requests_per_minute", 50)
        path_cfg = self.config.get("paths", {})
        self.inbox_paths = {
            k: Path(v) for k, v in path_cfg.get("inbox", {}).items() if isinstance(v, str)
        }
        self.source_paths = {
            k: Path(v) for k, v in path_cfg.get("sources", {}).items() if isinstance(v, str)
        }
        inbox_root = Path(self.inbox_paths.get("root", self.vault_root / "Inbox"))
        sources_root = Path(self.source_paths.get("root", self.vault_root / "Sources"))
        self.default_inbox = {
            "email": inbox_root / "Email",
            "transcripts": inbox_root / "Transcripts",
            "voice": inbox_root / "Voice",
            "attachments": inbox_root / "Attachments",
        }
        self.default_sources = {
            "email": sources_root / "Email",
            "transcripts": sources_root / "Transcripts",
            "documents": sources_root / "Documents",
            "voice": sources_root / "Voice",
        }
        
        # Initialize components
        self.registry = AdapterRegistry.default()
        self.entity_index = EntityIndex(vault_root, config=self.config)
        self.extractor = UnifiedExtractor(vault_root, verbose=verbose)
        self.patch_generator = PatchGenerator(vault_root, self.entity_index)
        self.output_generator = OutputGenerator(vault_root, dry_run=dry_run, verbose=verbose)
        
        # Shared context (loaded once)
        self._context: Optional[ContextBundle] = None
    
    @property
    def context(self) -> ContextBundle:
        """Get or load shared context."""
        if self._context is None:
            self._context = ContextBundle.load(self.vault_root, config=self.config, entity_index=self.entity_index)
        return self._context
    
    def process_file(self, path: Path, apply: bool = True) -> ProcessingResult:
        """Process a single content file.
        
        Args:
            path: Path to content file
            apply: If False, only extract + plan (no filesystem changes)
        
        Returns:
            ProcessingResult with all details
        """
        result = ProcessingResult(
            source_path=str(path),
            content_type="unknown"
        )
        phase_timings: dict[str, int] = {}
        run_start = time.time()
        
        try:
            # 1. Parse with adapter
            parse_start = time.time()
            envelope = self.registry.parse(path)
            phase_timings["adapter_ms"] = int((time.time() - parse_start) * 1000)
            if not envelope:
                result.success = False
                result.errors.append(f"No adapter found for {path}")
                result.metrics = {"timings": phase_timings, "cache": {}}
                return result
            
            result.content_type = envelope.content_type.value
            result.envelope = envelope
            
            # 2. Check for duplicates (skip if --force)
            if not self.force and self._is_duplicate(envelope):
                result.success = True
                result.errors.append("Skipped: duplicate content")
                result.metrics = {"timings": phase_timings, "cache": {}}
                return result
            
            # 3. Extract with LLM
            ctx_start = time.time()
            context = ContextBundle.load(self.vault_root, envelope, self.entity_index)
            phase_timings["context_ms"] = int((time.time() - ctx_start) * 1000)
            
            extract_start = time.time()
            extraction = self.extractor.extract(envelope, context)
            phase_timings["extract_ms"] = int((time.time() - extract_start) * 1000)
            self._augment_extraction_with_headers(extraction, envelope)
            result.extraction = extraction.model_dump()
            
            if self.verbose:
                self._log_extraction(extraction)
            
            # 4. Generate patches
            patch_start = time.time()
            plan = self.patch_generator.generate(extraction)
            phase_timings["patch_ms"] = int((time.time() - patch_start) * 1000)
            result.plan = plan
            
            # 5. Apply changes
            apply_ms = 0
            if apply and not self.dry_run:
                apply_start = time.time()
                applier = TransactionalApply(self.vault_root, dry_run=False)
                apply_result = applier.apply(plan, path)
                result.apply_result = apply_result
                apply_ms = int((time.time() - apply_start) * 1000)
                
                if not apply_result.success:
                    result.success = False
                    result.errors.extend(apply_result.errors)
            phase_timings["apply_ms"] = apply_ms
            
            # 6. Generate outputs (if enabled)
            # By default, only draft replies when suggested_outputs.needs_reply is true.
            # Use --draft-all-emails to force drafts for all emails (including automated/no-reply).
            outputs_ms = 0
            suggested = extraction.suggested_outputs
            is_email = envelope.content_type.value == "email"
            force_reply = is_email and self.draft_all_emails
            has_any_outputs = bool(extraction.tasks) or bool(
                suggested
                and (
                    suggested.needs_reply
                    or suggested.calendar_invite
                    or suggested.follow_up_reminder
                )
            )
            should_generate_outputs = (
                apply 
                and self.generate_outputs 
                and (force_reply or has_any_outputs)
            )
            if should_generate_outputs:
                outputs_start = time.time()
                outputs = self.output_generator.generate_all(
                    extraction, 
                    context,
                    envelope.raw_content if envelope else "",
                    force_reply=force_reply,
                )
                result.outputs = {
                    "reply": str(outputs.get("reply")) if outputs.get("reply") else None,
                    "calendar": str(outputs.get("calendar")) if outputs.get("calendar") else None,
                    "reminder": str(outputs.get("reminder")) if outputs.get("reminder") else None,
                    "tasks_emitted": len(outputs.get("tasks") or []),
                }
                result.draft_reply = result.outputs.get("reply")
                result.calendar_invite = {"path": result.outputs.get("calendar")} if result.outputs.get("calendar") else None
                outputs_ms = int((time.time() - outputs_start) * 1000)
            phase_timings["outputs_ms"] = outputs_ms
            
            # 7. Persist trace artifacts if requested
            if self.trace_dir:
                self._persist_trace(envelope, extraction, plan, outputs=result.outputs or None)
            
            result.metrics = {
                "timings": phase_timings,
                "cache": getattr(self.extractor, "last_usage", {}),
                "run_ms": int((time.time() - run_start) * 1000),
            }
            
            return result
            
        except Exception as e:
            result.success = False
            result.errors.append(str(e))
            result.metrics = {
                "timings": phase_timings,
                "cache": getattr(self.extractor, "last_usage", {}),
            }
            return result
    
    def process_all(self) -> BatchResult:
        """Process all pending content in Inbox.
        
        Returns:
            BatchResult with all processing results
        """
        pending: list[Path] = []
        
        for key in ["email", "transcripts", "voice", "attachments"]:
            subpath = Path(self.inbox_paths.get(key, self.default_inbox[key]))
            if subpath.exists():
                pending.extend(subpath.glob("*.md"))
        
        return self._process_paths(pending)
    
    def process_type(self, content_type: ContentType) -> BatchResult:
        """Process all pending content of a specific type.
        
        Args:
            content_type: Type of content to process
        
        Returns:
            BatchResult with processing results
        """
        # Map content type to inbox subdirectory
        type_dirs = {
            ContentType.EMAIL: "email",
            ContentType.TRANSCRIPT: "transcripts",
            ContentType.VOICE: "voice",
            ContentType.DOCUMENT: "attachments",
        }
        
        subdir_key = type_dirs.get(content_type, "attachments")
        inbox_path = Path(self.inbox_paths.get(subdir_key, self.default_inbox[subdir_key]))
        
        pending = list(inbox_path.glob("*.md")) if inbox_path.exists() else []
        return self._process_paths(pending)
    
    def process_sources(self, content_type: Optional[ContentType] = None) -> BatchResult:
        """Re-process archived sources from the Sources/ directory."""
        type_dirs = {
            ContentType.EMAIL: "email",
            ContentType.TRANSCRIPT: "transcripts",
            ContentType.VOICE: "voice",
            ContentType.DOCUMENT: "documents",
        }
        
        pending: list[Path] = []
        
        if content_type:
            dir_name = type_dirs.get(content_type)
            if dir_name:
                base = Path(self.source_paths.get(dir_name, self.default_sources[dir_name]))
                if base.exists():
                    pending.extend(base.rglob("*.md"))
        else:
            for dir_name in type_dirs.values():
                path = Path(self.source_paths.get(dir_name, self.default_sources[dir_name]))
                if path.exists():
                    pending.extend(path.rglob("*.md"))
        
        return self._process_paths(pending)
    
    def _is_duplicate(self, envelope: ContentEnvelope) -> bool:
        """Check if content has already been processed."""
        # Check extraction directory for existing extraction
        extraction_dir = self.vault_root / "Inbox" / "_extraction"
        if not extraction_dir.exists():
            return False
        
        # Look for matching extraction file
        stem = envelope.source_path.stem
        for ext_file in extraction_dir.glob(f"{stem}.*extraction.json"):
            return True
        
        return False
    
    def _augment_extraction_with_headers(self, extraction, envelope: ContentEnvelope):
        """Ensure sender/recipient emails are preserved in contacts/participants."""
        email_meta = (envelope.metadata or {}).get("email") if envelope.metadata else {}
        if not email_meta:
            return
        
        header_contacts = []
        sender_email = email_meta.get("sender_email")
        sender_name = email_meta.get("sender_name") or sender_email
        if sender_email:
            header_contacts.append(ContactInfo(name=sender_name, email=sender_email))
        
        for rec in email_meta.get("recipients_detail", []) or []:
            name = rec.get("name") or rec.get("email")
            header_contacts.append(ContactInfo(name=name, email=rec.get("email")))
        
        existing_emails = {c.email.lower() for c in extraction.contacts if c.email}
        for contact in header_contacts:
            if contact.email and contact.email.lower() in existing_emails:
                continue
            extraction.contacts.append(contact)
        
        # Ensure participants includes header names when extraction omits them
        if not extraction.participants and envelope.participants:
            extraction.participants = envelope.participants
    
    def _process_paths(self, paths: list[Path]) -> BatchResult:
        """Process a list of paths and return aggregated results.
        
        Uses parallel processing when max_workers > 1 and parallel is enabled.
        """
        if self.parallel_enabled and self.max_workers > 1 and len(paths) > 1:
            return self._process_paths_parallel(paths)
        return self._process_paths_sequential(paths)
    
    def _process_paths_parallel(self, paths: list[Path]) -> BatchResult:
        """Process paths in parallel with rate limiting and patch collection.
        
        Uses ThreadPoolExecutor with a semaphore to respect rate limits.
        Collects all patches, merges by target, then applies atomically.
        """
        batch = BatchResult()
        batch.total = len(paths)
        batch_start = time.time()
        
        timings_accum: dict[str, int] = {}
        cache_calls = cache_hits = 0
        cached_tokens = prompt_tokens = total_tokens = 0
        
        # Rate limiting semaphore (max concurrent LLM calls)
        semaphore = threading.Semaphore(self.max_workers)
        
        # Collect patches for deferred merge
        patch_collector = PatchCollector()
        source_files: list[Path] = []
        
        def process_with_semaphore(path: Path) -> ProcessingResult:
            """Process a single file with rate limiting."""
            with semaphore:
                return self.process_file(path, apply=False)
        
        # Submit all tasks
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_path = {executor.submit(process_with_semaphore, p): p for p in paths}
            
            for future in as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    result = future.result()
                    batch.results.append(result)
                    
                    should_apply = result.success and not any("Skipped" in str(err) for err in result.errors)
                    if should_apply and result.plan:
                        patch_collector.collect(result.plan)
                        source_files.append(path)
                    
                    # Aggregate metrics
                    metrics = result.metrics or {}
                    timings = metrics.get("timings") or {}
                    for phase, ms in timings.items():
                        timings_accum[phase] = timings_accum.get(phase, 0) + ms
                    
                    cache = metrics.get("cache") or {}
                    if cache:
                        cache_calls += 1
                        if cache.get("cache_hit"):
                            cache_hits += 1
                        cached_tokens += cache.get("cached_tokens", 0)
                        prompt_tokens += cache.get("prompt_tokens", 0)
                        total_tokens += cache.get("total_tokens", 0)
                
                except Exception as e:
                    batch.results.append(ProcessingResult(
                        source_path=str(path),
                        content_type="unknown",
                        success=False,
                        errors=[f"Parallel processing error: {e}"]
                    ))
        
        # Merge all patches and apply atomically (single-writer)
        meeting_notes = patch_collector.get_meeting_notes()
        apply_result: Optional[ApplyResult] = None
        needs_apply = (patch_collector.has_patches or bool(meeting_notes)) and not self.dry_run and bool(source_files)
        if needs_apply:
            try:
                merged_plan = patch_collector.merge()
                applier = TransactionalApply(self.vault_root, dry_run=False)
                primary_source = source_files[0]
                extra_sources = source_files[1:] if len(source_files) > 1 else None
                extra_notes = meeting_notes[1:] if len(meeting_notes) > 1 else None
                apply_result = applier.apply(
                    merged_plan,
                    primary_source,
                    extra_meeting_notes=extra_notes,
                    extra_source_paths=extra_sources,
                )

                # Propagate apply result to individual items
                for r in batch.results:
                    if r.success and not any("Skipped" in str(err) for err in r.errors):
                        r.apply_result = apply_result
                        if not apply_result.success:
                            r.success = False
                            r.errors.extend(apply_result.errors)
            except Exception as e:
                for r in batch.results:
                    if r.success and not any("Skipped" in str(err) for err in r.errors):
                        r.success = False
                        r.errors.append(f"Batch merge/apply error: {e}")

        # Generate outputs after apply (or when no apply needed) so parallel mode matches sequential behavior.
        outputs_summary = {"draft_replies": 0, "calendar_invites": 0, "reminders": 0, "tasks_emitted": 0}
        apply_ok = (apply_result is None) or bool(apply_result.success)
        if self.generate_outputs and apply_ok:
            for r in batch.results:
                if not r.success or any("Skipped" in str(err) for err in r.errors):
                    continue
                if not r.envelope or not r.extraction:
                    continue
                try:
                    extraction = UnifiedExtraction.model_validate(r.extraction)
                except Exception as e:
                    r.errors.append(f"Output generation skipped: invalid extraction ({e})")
                    continue

                is_email = r.envelope.content_type.value == "email"
                suggested = extraction.suggested_outputs
                if not (is_email or (suggested and suggested.needs_reply)):
                    continue

                context = ContextBundle.load(self.vault_root, r.envelope, self.entity_index)
                outputs = self.output_generator.generate_all(
                    extraction,
                    context,
                    r.envelope.raw_content if r.envelope else "",
                    force_reply=is_email,
                )
                r.outputs = {
                    "reply": str(outputs.get("reply")) if outputs.get("reply") else None,
                    "calendar": str(outputs.get("calendar")) if outputs.get("calendar") else None,
                    "reminder": str(outputs.get("reminder")) if outputs.get("reminder") else None,
                    "tasks_emitted": len(outputs.get("tasks") or []),
                }
                r.draft_reply = r.outputs.get("reply")
                if r.outputs.get("calendar"):
                    r.calendar_invite = {"path": r.outputs.get("calendar")}

                if outputs.get("reply"):
                    outputs_summary["draft_replies"] += 1
                if outputs.get("calendar"):
                    outputs_summary["calendar_invites"] += 1
                if outputs.get("reminder"):
                    outputs_summary["reminders"] += 1
                outputs_summary["tasks_emitted"] += len(outputs.get("tasks") or [])

                if self.trace_dir and r.envelope:
                    try:
                        stem = r.envelope.source_path.stem
                        (self.trace_dir / f"{stem}.outputs.json").write_text(json.dumps(r.outputs, indent=2))
                    except Exception:
                        pass

        # Recompute counts after batch apply
        batch.success = batch.failed = batch.skipped = 0
        for r in batch.results:
            if r.success:
                if any("Skipped" in str(err) for err in r.errors):
                    batch.skipped += 1
                else:
                    batch.success += 1
            else:
                batch.failed += 1
        
        count = len(batch.results)
        phase_avg = {k: int(v / count) for k, v in timings_accum.items()} if count else {}
        cache_summary = {
            "calls": cache_calls,
            "hits": cache_hits,
            "hit_rate": (cache_hits / cache_calls * 100) if cache_calls else 0,
            "cached_tokens": cached_tokens,
            "prompt_tokens": prompt_tokens,
            "total_tokens": total_tokens,
        }
        
        batch.metrics = {
            "run_ms": int((time.time() - batch_start) * 1000),
            "phase_ms_avg": phase_avg,
            "cache": cache_summary,
            "parallel": {
                "workers": self.max_workers,
                "files_processed": len(paths),
                "patches_merged": patch_collector.patch_count,
            },
            "outputs": outputs_summary,
        }
        
        if self.log_metrics and batch.total > 0:
            try:
                log_pipeline_stats({
                    "timestamp": datetime.now().isoformat(),
                    "total": batch.total,
                    "success": batch.success,
                    "failed": batch.failed,
                    "skipped": batch.skipped,
                    **batch.metrics,
                })
            except Exception:
                pass
        
        self.entity_index.invalidate()
        return batch
    
    def _process_paths_sequential(self, paths: list[Path]) -> BatchResult:
        """Process a list of paths and return aggregated results."""
        batch = BatchResult()
        batch.total = len(paths)
        batch_start = time.time()
        
        timings_accum: dict[str, int] = {}
        cache_calls = cache_hits = 0
        cached_tokens = prompt_tokens = total_tokens = 0
        
        for path in paths:
            result = self.process_file(path)
            batch.results.append(result)
            
            if result.success:
                if any("Skipped" in str(err) for err in result.errors):
                    batch.skipped += 1
                else:
                    batch.success += 1
            else:
                batch.failed += 1
            
            # Aggregate metrics
            metrics = result.metrics or {}
            timings = metrics.get("timings") or {}
            for phase, ms in timings.items():
                timings_accum[phase] = timings_accum.get(phase, 0) + ms
            
            cache = metrics.get("cache") or {}
            if cache:
                cache_calls += 1
                if cache.get("cache_hit"):
                    cache_hits += 1
                cached_tokens += cache.get("cached_tokens", 0)
                prompt_tokens += cache.get("prompt_tokens", 0)
                total_tokens += cache.get("total_tokens", 0)
        
        count = len(batch.results)
        phase_avg = {k: int(v / count) for k, v in timings_accum.items()} if count else {}
        cache_summary = {
            "calls": cache_calls,
            "hits": cache_hits,
            "hit_rate": (cache_hits / cache_calls * 100) if cache_calls else 0,
            "cached_tokens": cached_tokens,
            "prompt_tokens": prompt_tokens,
            "total_tokens": total_tokens,
        }
        
        batch.metrics = {
            "run_ms": int((time.time() - batch_start) * 1000),
            "phase_ms_avg": phase_avg,
            "cache": cache_summary,
        }
        
        if self.log_metrics and batch.total > 0:
            try:
                log_pipeline_stats({
                    "timestamp": datetime.now().isoformat(),
                    "total": batch.total,
                    "success": batch.success,
                    "failed": batch.failed,
                    "skipped": batch.skipped,
                    **batch.metrics,
                })
            except Exception:
                # Metrics logging should never break the pipeline
                pass
        
        # Invalidate entity index (new entities may have been created)
        self.entity_index.invalidate()
        
        return batch
    
    def _log_extraction(self, extraction):
        """Log extraction summary in verbose mode."""
        from rich.console import Console
        console = Console()
        
        console.print(f"  Note type: {extraction.note_type}")
        console.print(f"  Summary: {extraction.summary[:80]}...")
        console.print(f"  Facts: {len(extraction.facts)}")
        console.print(f"  Tasks: {len(extraction.tasks)}")
        console.print(f"  Entities with facts: {len(extraction.get_entities_with_facts())}")

    def _persist_trace(self, envelope: ContentEnvelope, extraction, plan: ChangePlan, outputs: Optional[dict] = None):
        """Persist extraction and changeplan artifacts for audit/traceability."""
        if not self.trace_dir:
            return
        
        trace_root = self.trace_dir
        trace_root.mkdir(parents=True, exist_ok=True)
        
        stem = envelope.source_path.stem
        extraction_path = trace_root / f"{stem}.extraction.json"
        changeplan_path = trace_root / f"{stem}.changeplan.json"
        outputs_path = trace_root / f"{stem}.outputs.json"
        
        try:
            extraction_path.write_text(json.dumps(extraction.model_dump(mode="json"), indent=2))
        except Exception:
            pass
        
        try:
            changeplan_path.write_text(json.dumps(plan.model_dump(mode="json", exclude_none=True), indent=2))
        except Exception:
            pass

        if outputs:
            try:
                outputs_path.write_text(json.dumps(outputs, indent=2))
            except Exception:
                pass

========================================================================================================================

========================================================================================================================
GROUP: ENRICH
PATH: Workflow/scripts/enrich_person.py
ROLE: People enrichment (includes web search caching)
========================================================================================================================

#!/usr/bin/env python3
"""
Person Enrichment: AI-powered data gathering for People READMEs.

Enrichment Levels:
  L0: Stub       - Just folder name exists
  L1: Contact    - Basic contact info from emails
  L2: README     - AI inference from existing README content
  L3: Web        - OpenAI web search for public info
  L4: Deep       - Multi-source comprehensive research

Usage:
    # Enrich a single person from README content (L2)
    python enrich_person.py "John Smith" --from-readme
    
    # Enrich with web search (L3)
    python enrich_person.py "John Smith" --web
    
    # Deep enrichment (L4)
    python enrich_person.py "John Smith" --deep
    
    # Batch enrich all sparse entries
    python enrich_person.py --all --level 2 --limit 20
    
    # Enrich contacts at a specific company
    python enrich_person.py --company "Microsoft" --level 3
    
    # List sparse entries
    python enrich_person.py --list-sparse

The enrichment system updates both:
1. The person's README.md frontmatter
2. The People _MANIFEST.md row
3. The glossary cache (for prompt caching)
"""

import argparse
import json
import os
import re
import sys
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any

import yaml

# Add parent dir for imports
sys.path.insert(0, str(Path(__file__).parent))
from utils.ai_client import get_client
from utils.config import get_model_config
from utils.frontmatter import parse_frontmatter, render_frontmatter
from manifest_sync import (
    VAST_PEOPLE, PEOPLE_MANIFEST, CACHE_DIR, GLOSSARY_CACHE,
    PersonEntry, scan_people_folder, generate_people_manifest,
    build_glossary_cache, sync_person_to_manifest, parse_frontmatter as parse_fm
)

# Paths
VAULT_ROOT = Path(__file__).parent.parent.parent
WEB_CACHE_DIR = VAULT_ROOT / "Workflow" / "_cache" / "web_enrichment"


# =============================================================================
# EXPORTED HELPERS (for use by other scripts like process_emails.py)
# =============================================================================

def sync_to_manifest(name: str) -> bool:
    """
    Sync a person's README to the manifest.
    
    Reads the current README frontmatter and updates the manifest row.
    """
    try:
        sync_person_to_manifest(name, updates={}, rebuild_cache=False)
        return True
    except Exception as e:
        print(f"Warning: Could not sync {name} to manifest: {e}")
        return False


def rebuild_glossary_cache() -> None:
    """Rebuild the glossary cache from manifests."""
    import json
    glossary = build_glossary_cache()
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    GLOSSARY_CACHE.write_text(json.dumps(glossary, indent=2, default=str))
WEB_CACHE_DIR = VAULT_ROOT / "Workflow" / "_cache" / "web_enrichment"


@dataclass
class EnrichmentResult:
    """Result of enrichment operation."""
    name: str
    level_before: int
    level_after: int
    fields_added: List[str] = field(default_factory=list)
    fields_updated: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    source: str = ""  # "readme", "web", "deep"
    cached: bool = False


def get_enrichment_level(readme_path: Path) -> int:
    """Determine current enrichment level of a person."""
    if not readme_path.exists():
        return 0
    
    content = readme_path.read_text()
    fm = parse_fm(content)
    
    # Explicit level in frontmatter
    if fm.get("enrichment_level"):
        return int(fm.get("enrichment_level"))
    
    # Infer from data present
    has_email = bool(fm.get("email"))
    has_role = bool(fm.get("role") or fm.get("title"))
    has_company = bool(fm.get("company"))
    has_linkedin = bool(fm.get("linkedin"))
    has_bio = bool(fm.get("bio"))
    has_previous_roles = bool(fm.get("previous_roles"))
    
    if has_previous_roles or has_bio:
        return 4  # Deep enrichment
    if has_linkedin:
        return 3  # Web enrichment
    if has_role and has_company:
        return 2  # README enrichment
    if has_email:
        return 1  # Contact info
    return 0  # Stub


def is_sparse(readme_path: Path) -> bool:
    """Check if person needs enrichment (missing role or company)."""
    if not readme_path.exists():
        return True
    
    content = readme_path.read_text()
    fm = parse_fm(content)
    
    has_role = bool(fm.get("role") or fm.get("title"))
    has_company = bool(fm.get("company"))
    
    return not (has_role and has_company)


def list_sparse_entries(limit: int = 50) -> List[Path]:
    """List people folders that need enrichment."""
    sparse = []
    
    for folder in sorted(VAST_PEOPLE.iterdir()):
        if not folder.is_dir() or folder.name.startswith(('_', '.')):
            continue
        
        readme = folder / "README.md"
        if is_sparse(readme):
            sparse.append(readme)
            if len(sparse) >= limit:
                break
    
    return sparse


# =============================================================================
# LEVEL 2: README ENRICHMENT
# =============================================================================

def enrich_from_readme(name: str, dry_run: bool = False) -> EnrichmentResult:
    """
    Level 2 enrichment: Extract role/company from existing README content.
    
    Uses AI to infer role and company from the README body text.
    """
    result = EnrichmentResult(name=name, level_before=0, level_after=0, source="readme")
    
    person_folder = VAST_PEOPLE / name
    readme_path = person_folder / "README.md"
    
    if not readme_path.exists():
        result.errors.append(f"README not found: {readme_path}")
        return result
    
    content = readme_path.read_text()
    result.level_before = get_enrichment_level(readme_path)
    
    # Already at L2 or higher
    if result.level_before >= 2:
        result.level_after = result.level_before
        return result
    
    client = get_client(caller="enrich_person.readme")
    
    prompt = f"""Extract structured information about this person from their README.

Person: {name}

README Content:
{content[:4000]}

Return a JSON object with:
{{
    "role": "Job title (e.g., VP Engineering, CEO, Account Manager)",
    "company": "Company name (e.g., Microsoft, Google, VAST Data)",
    "context": "1-2 sentence summary of who they are and how we know them",
    "expertise": ["list", "of", "expertise", "areas"],
    "relationship": "customer|partner|colleague|vendor|other"
}}

Rules:
- Infer from context if not explicitly stated
- Use "VAST Data" for internal colleagues
- Leave empty string if truly unknown
- Be specific with roles (not just "Engineer" but "Senior Software Engineer")

Return ONLY valid JSON, no markdown."""

    try:
        model_config = get_model_config("enrichment")
        response = client.chat.completions.create(
            model=model_config["model"],
            messages=[
                {"role": "system", "content": "Extract structured data from notes. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=model_config.get("temperature", 0.0),
            store=False
        )
        
        text = response.choices[0].message.content.strip()
        if text.startswith("```"):
            text = re.sub(r'^```\w*\n?', '', text)
            text = re.sub(r'\n?```$', '', text)
        
        data = json.loads(text)
        
    except Exception as e:
        result.errors.append(f"AI extraction failed: {e}")
        return result
    
    # Build updates
    updates = {}
    
    if data.get("role"):
        updates["role"] = data["role"]
        result.fields_added.append("role")
    
    if data.get("company"):
        updates["company"] = data["company"]
        result.fields_added.append("company")
    
    if data.get("expertise"):
        updates["expertise"] = data["expertise"]
        result.fields_added.append("expertise")
    
    if data.get("relationship"):
        updates["relationship"] = data["relationship"]
        result.fields_added.append("relationship")
    
    # Mark enrichment level and timestamp
    updates["enrichment_level"] = 2
    updates["last_enriched"] = datetime.now().strftime("%Y-%m-%d")
    
    if not updates or not result.fields_added:
        result.errors.append("No data extracted")
        return result
    
    if dry_run:
        result.level_after = 2
        return result
    
    # Apply updates to README
    fm = parse_fm(content)
    fm.update(updates)
    
    # Find where frontmatter ends
    if content.startswith("---"):
        end_idx = content.find("\n---", 3)
        if end_idx != -1:
            body = content[end_idx + 4:].lstrip("\n")
        else:
            body = ""
    else:
        body = content
    
    # Rebuild frontmatter
    new_fm = yaml.dump(fm, default_flow_style=False, allow_unicode=True, sort_keys=False)
    new_content = f"---\n{new_fm}---\n\n{body}"
    
    readme_path.write_text(new_content)
    
    # Sync to manifest
    sync_person_to_manifest(name, updates, rebuild_cache=True)
    
    result.level_after = 2
    return result


# =============================================================================
# LEVEL 3: WEB ENRICHMENT
# =============================================================================

def get_web_cache_path(name: str) -> Path:
    """Get cache path for web enrichment results."""
    safe_name = name.lower().replace(" ", "_").replace("/", "_")[:50]
    return WEB_CACHE_DIR / f"{safe_name}.json"


def is_cache_fresh(cache_path: Path, max_days: int = 30) -> bool:
    """Check if cached data is still fresh."""
    if not cache_path.exists():
        return False
    
    try:
        data = json.loads(cache_path.read_text())
        cached_at = datetime.fromisoformat(data.get("cached_at", "2000-01-01"))
        return (datetime.now() - cached_at).days < max_days
    except Exception:
        return False


def enrich_from_web(name: str, dry_run: bool = False, force: bool = False) -> EnrichmentResult:
    """
    Level 3 enrichment: Use OpenAI web search for public information.
    
    Searches for LinkedIn, company info, public profiles, etc.
    """
    result = EnrichmentResult(name=name, level_before=0, level_after=0, source="web")
    
    person_folder = VAST_PEOPLE / name
    readme_path = person_folder / "README.md"
    
    if not readme_path.exists():
        result.errors.append(f"README not found: {readme_path}")
        return result
    
    content = readme_path.read_text()
    fm = parse_fm(content)
    result.level_before = get_enrichment_level(readme_path)
    
    # Check cache first
    cache_path = get_web_cache_path(name)
    WEB_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    if not force and is_cache_fresh(cache_path):
        # Use cached data
        cached_data = json.loads(cache_path.read_text())
        data = cached_data.get("enrichment", {})
        result.cached = True
    else:
        # Need to do web search
        client = get_client(caller="enrich_person.web")
        
        # Build search context
        existing_company = fm.get("company", "")
        existing_role = fm.get("role") or fm.get("title", "")
        existing_email = fm.get("email", "")
        
        search_hints = []
        if existing_company:
            search_hints.append(f"Company: {existing_company}")
        if existing_role:
            search_hints.append(f"Role: {existing_role}")
        if existing_email:
            # Extract domain for search
            domain = existing_email.split("@")[-1] if "@" in existing_email else ""
            if domain and not domain.endswith(("gmail.com", "yahoo.com", "outlook.com")):
                search_hints.append(f"Email domain: {domain}")
        
        prompt = f"""Research this person using web search and return their professional information.

Person: {name}
Known info: {'; '.join(search_hints) if search_hints else 'None'}

Search for their LinkedIn profile, company info, and public background.

Return a JSON object with:
{{
    "role": "Current job title",
    "company": "Current employer",
    "linkedin": "LinkedIn URL if found",
    "bio": "1-2 sentence professional bio",
    "location": "City, State/Country",
    "previous_roles": [
        {{"title": "...", "company": "...", "years": "2020-2023"}}
    ],
    "education": ["University/Degree"],
    "expertise": ["areas", "of", "expertise"],
    "sources": ["URLs used as sources"]
}}

Important:
- Only include information you can verify from web sources
- Leave fields empty if not found
- Include LinkedIn URL if available
- Be specific about current role vs previous roles

Return ONLY valid JSON."""

        try:
            # Use the Responses API with web_search tool for actual web search
            model_config = get_model_config("web_enrichment")
            response = client.responses.create(
                model=model_config["model"],
                tools=[{"type": "web_search_preview"}],
                input=prompt,
                instructions="You are a professional researcher. Use the web_search tool to find accurate information about people. Return only verified data with source URLs.",
                store=False
            )
            
            # Extract text from response output
            text = ""
            for item in response.output:
                if hasattr(item, 'content'):
                    for content_part in item.content:
                        if hasattr(content_part, 'text'):
                            text = content_part.text
                            break
            
            if not text:
                result.errors.append("No text response from web search")
                return result
            
            text = text.strip()
            if text.startswith("```"):
                text = re.sub(r'^```\w*\n?', '', text)
                text = re.sub(r'\n?```$', '', text)
            
            data = json.loads(text)
            
            # Cache the result
            cache_data = {
                "name": name,
                "cached_at": datetime.now().isoformat(),
                "enrichment": data
            }
            cache_path.write_text(json.dumps(cache_data, indent=2))
            
        except Exception as e:
            result.errors.append(f"Web search failed: {e}")
            return result
    
    # Build updates from web data
    updates = {}
    
    if data.get("role") and not fm.get("role"):
        updates["role"] = data["role"]
        result.fields_added.append("role")
    elif data.get("role") and fm.get("role") != data["role"]:
        updates["role"] = data["role"]
        result.fields_updated.append("role")
    
    if data.get("company") and not fm.get("company"):
        updates["company"] = data["company"]
        result.fields_added.append("company")
    elif data.get("company") and fm.get("company") != data["company"]:
        updates["company"] = data["company"]
        result.fields_updated.append("company")
    
    if data.get("linkedin"):
        updates["linkedin"] = data["linkedin"]
        result.fields_added.append("linkedin")
    
    if data.get("bio"):
        updates["bio"] = data["bio"]
        result.fields_added.append("bio")
    
    if data.get("location"):
        updates["location"] = data["location"]
        result.fields_added.append("location")
    
    if data.get("previous_roles"):
        updates["previous_roles"] = data["previous_roles"]
        result.fields_added.append("previous_roles")
    
    if data.get("education"):
        updates["education"] = data["education"]
        result.fields_added.append("education")
    
    if data.get("expertise"):
        updates["expertise"] = data["expertise"]
        result.fields_added.append("expertise")
    
    # Mark enrichment level
    updates["enrichment_level"] = 3
    updates["last_enriched"] = datetime.now().strftime("%Y-%m-%d")
    
    if dry_run:
        result.level_after = 3
        return result
    
    # Apply updates
    fm.update(updates)
    
    # Rebuild content
    if content.startswith("---"):
        end_idx = content.find("\n---", 3)
        if end_idx != -1:
            body = content[end_idx + 4:].lstrip("\n")
        else:
            body = ""
    else:
        body = content
    
    new_fm = yaml.dump(fm, default_flow_style=False, allow_unicode=True, sort_keys=False)
    new_content = f"---\n{new_fm}---\n\n{body}"
    
    readme_path.write_text(new_content)
    
    # Sync to manifest
    sync_person_to_manifest(name, updates, rebuild_cache=True)
    
    result.level_after = 3
    return result


# =============================================================================
# LEVEL 4: DEEP ENRICHMENT
# =============================================================================

def enrich_deep(name: str, dry_run: bool = False) -> EnrichmentResult:
    """
    Level 4 enrichment: Comprehensive multi-source research.
    
    Does multiple web searches and cross-references information.
    """
    result = EnrichmentResult(name=name, level_before=0, level_after=0, source="deep")
    
    # First do L3 enrichment
    l3_result = enrich_from_web(name, dry_run=dry_run, force=True)
    
    if l3_result.errors:
        result.errors.extend(l3_result.errors)
        return result
    
    result.fields_added.extend(l3_result.fields_added)
    result.fields_updated.extend(l3_result.fields_updated)
    result.level_before = l3_result.level_before
    
    # TODO: Additional deep enrichment steps
    # - Company research (size, funding, industry)
    # - Mutual connections analysis
    # - Recent news/mentions
    # - Conference appearances
    # - Publications/patents
    
    person_folder = VAST_PEOPLE / name
    readme_path = person_folder / "README.md"
    
    if not dry_run and readme_path.exists():
        # Update level to 4
        content = readme_path.read_text()
        fm = parse_fm(content)
        fm["enrichment_level"] = 4
        fm["last_enriched"] = datetime.now().strftime("%Y-%m-%d")
        
        if content.startswith("---"):
            end_idx = content.find("\n---", 3)
            body = content[end_idx + 4:].lstrip("\n") if end_idx != -1 else ""
        else:
            body = content
        
        new_fm = yaml.dump(fm, default_flow_style=False, allow_unicode=True, sort_keys=False)
        readme_path.write_text(f"---\n{new_fm}---\n\n{body}")
    
    result.level_after = 4
    return result


# =============================================================================
# BATCH OPERATIONS
# =============================================================================

def enrich_batch(
    level: int = 2, 
    limit: int = 20, 
    company_filter: Optional[str] = None,
    dry_run: bool = False
) -> List[EnrichmentResult]:
    """Batch enrich multiple people."""
    
    results = []
    people = scan_people_folder()
    
    # Filter sparse entries
    candidates = []
    for person in people:
        readme = VAST_PEOPLE / person.name / "README.md"
        if not is_sparse(readme):
            continue
        
        # Company filter
        if company_filter:
            if company_filter.lower() not in (person.company or "").lower():
                continue
        
        candidates.append(person.name)
        if len(candidates) >= limit:
            break
    
    print(f"Enriching {len(candidates)} people at level {level}...")
    
    for i, name in enumerate(candidates, 1):
        print(f"\n[{i}/{len(candidates)}] {name}...")
        
        if level == 2:
            result = enrich_from_readme(name, dry_run=dry_run)
        elif level == 3:
            result = enrich_from_web(name, dry_run=dry_run)
        elif level == 4:
            result = enrich_deep(name, dry_run=dry_run)
        else:
            print(f"  Unknown level: {level}")
            continue
        
        results.append(result)
        
        if result.errors:
            print(f"  âŒ Errors: {result.errors}")
        elif result.fields_added or result.fields_updated:
            print(f"  âœ… Added: {result.fields_added}, Updated: {result.fields_updated}")
        else:
            print(f"  â­ï¸  No changes (already at L{result.level_before})")
    
    return results


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Enrich People READMEs with AI-gathered information"
    )
    
    parser.add_argument("name", nargs="?", help="Person name to enrich")
    
    # Enrichment level options
    parser.add_argument("--from-readme", action="store_true", 
                        help="Level 2: Enrich from README content")
    parser.add_argument("--web", action="store_true", 
                        help="Level 3: Enrich using web search")
    parser.add_argument("--deep", action="store_true", 
                        help="Level 4: Deep multi-source research")
    
    # Batch options
    parser.add_argument("--all", action="store_true", 
                        help="Enrich all sparse entries")
    parser.add_argument("--level", type=int, default=2, 
                        help="Enrichment level for batch (default: 2)")
    parser.add_argument("--limit", type=int, default=20, 
                        help="Max entries for batch (default: 20)")
    parser.add_argument("--company", type=str, 
                        help="Filter by company for batch")
    
    # Utility options
    parser.add_argument("--list-sparse", action="store_true", 
                        help="List people needing enrichment")
    parser.add_argument("--force", action="store_true", 
                        help="Force refresh (ignore cache)")
    parser.add_argument("--dry-run", action="store_true", 
                        help="Show what would be done")
    
    args = parser.parse_args()
    
    # List sparse entries
    if args.list_sparse:
        sparse = list_sparse_entries(limit=100)
        print(f"Found {len(sparse)} sparse entries:\n")
        for readme in sparse:
            level = get_enrichment_level(readme)
            print(f"  L{level}: {readme.parent.name}")
        return
    
    # Batch enrichment
    if args.all:
        results = enrich_batch(
            level=args.level,
            limit=args.limit,
            company_filter=args.company,
            dry_run=args.dry_run
        )
        
        # Summary
        added = sum(len(r.fields_added) for r in results)
        updated = sum(len(r.fields_updated) for r in results)
        errors = sum(len(r.errors) for r in results)
        
        print(f"\n{'='*50}")
        print(f"Summary: {len(results)} people processed")
        print(f"  Fields added: {added}")
        print(f"  Fields updated: {updated}")
        print(f"  Errors: {errors}")
        return
    
    # Single person enrichment
    if not args.name:
        parser.print_help()
        return
    
    if args.deep:
        result = enrich_deep(args.name, dry_run=args.dry_run)
    elif args.web:
        result = enrich_from_web(args.name, dry_run=args.dry_run, force=args.force)
    else:
        result = enrich_from_readme(args.name, dry_run=args.dry_run)
    
    # Print result
    print(f"\nEnrichment: {result.name}")
    print(f"  Level: L{result.level_before} â†’ L{result.level_after}")
    print(f"  Source: {result.source}")
    if result.cached:
        print(f"  (used cached data)")
    if result.fields_added:
        print(f"  Added: {result.fields_added}")
    if result.fields_updated:
        print(f"  Updated: {result.fields_updated}")
    if result.errors:
        print(f"  Errors: {result.errors}")


if __name__ == "__main__":
    main()

========================================================================================================================

========================================================================================================================
GROUP: NORMALIZE
PATH: Workflow/scripts/normalize_entity_notes.py
ROLE: Post-apply normalization (frontmatter normalization)
========================================================================================================================

#!/usr/bin/env python3
"""
Normalize note frontmatter in entity folders.

Fixes common ingest artifacts:
- `type` mismatches with folder location (people/projects/customer/rob)
- blank entity keys (`person`, `project`, `account`, `rob_forum`)
- empty tags like `person/`, `account/`, `project/`
- stale `type/*` tags that don't match the normalized type

This is intended as a safe, idempotent cleanup step after imports.
"""

from __future__ import annotations

import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable

import click

sys.path.insert(0, str(Path(__file__).parent))
from utils import load_config, vault_root
from utils.frontmatter import parse_frontmatter, render_frontmatter


@dataclass(frozen=True)
class EntityScope:
    name: str
    base_dir: Path
    entity_key: str
    note_type: str


def _ensure_list(value: Any) -> list[str]:
    if value is None:
        return []
    if isinstance(value, str):
        return [value]
    if isinstance(value, list):
        return [str(v) for v in value]
    return []


def normalize_frontmatter_dict(
    fm: dict[str, Any],
    *,
    entity_key: str,
    entity_name: str,
    note_type: str,
) -> dict[str, Any]:
    """Return a normalized copy of a frontmatter dict for an entity note."""
    out = dict(fm or {})

    out["type"] = note_type
    out[entity_key] = entity_name

    # Remove blank strings for common keys (keeps README stubs intact by skipping READMEs).
    for key in ["person", "project", "account", "rob_forum"]:
        if out.get(key) == "":
            out.pop(key, None)

    tags = _ensure_list(out.get("tags"))
    cleaned: list[str] = []
    for tag in tags:
        t = str(tag).strip()
        if not t:
            continue
        if t.endswith("/"):
            continue
        cleaned.append(t)

    # Normalize type tags to exactly one.
    cleaned = [t for t in cleaned if not t.startswith("type/")]
    cleaned.insert(0, f"type/{note_type}")

    # De-dupe, preserving order.
    deduped: list[str] = []
    seen: set[str] = set()
    for t in cleaned:
        if t in seen:
            continue
        seen.add(t)
        deduped.append(t)

    if deduped:
        out["tags"] = deduped
    else:
        out.pop("tags", None)

    return out


def _iter_entity_notes(scope: EntityScope) -> Iterable[tuple[str, Path]]:
    if not scope.base_dir.exists():
        return
    for entity_dir in sorted(scope.base_dir.iterdir()):
        if not entity_dir.is_dir() or entity_dir.name.startswith(("_", ".")):
            continue
        for md in sorted(entity_dir.glob("*.md")):
            if md.name == "README.md" or md.name.startswith("_"):
                continue
            yield entity_dir.name, md


def normalize_entity_notes(scope: EntityScope, *, dry_run: bool = False) -> tuple[int, int]:
    """Normalize notes in a single entity scope. Returns (changed, skipped)."""
    changed = 0
    skipped = 0
    for entity_name, note_path in _iter_entity_notes(scope):
        text = note_path.read_text(errors="ignore")
        fm, body = parse_frontmatter(text)
        if fm is None:
            skipped += 1
            continue

        normalized_fm = normalize_frontmatter_dict(
            fm,
            entity_key=scope.entity_key,
            entity_name=entity_name,
            note_type=scope.note_type,
        )
        updated = render_frontmatter(normalized_fm) + body
        if updated == text:
            continue

        changed += 1
        if not dry_run:
            note_path.write_text(updated)

    return changed, skipped


def _scopes_from_config(vault: Path) -> dict[str, EntityScope]:
    cfg = load_config(vault_root_override=vault)
    work_paths = cfg.get("paths", {}).get("work", {})
    return {
        "people": EntityScope(
            name="people",
            base_dir=vault / work_paths.get("people", "VAST/People"),
            entity_key="person",
            note_type="people",
        ),
        "projects": EntityScope(
            name="projects",
            base_dir=vault / work_paths.get("projects", "VAST/Projects"),
            entity_key="project",
            note_type="projects",
        ),
        "customers": EntityScope(
            name="customers",
            base_dir=vault / work_paths.get("accounts", "VAST/Customers and Partners"),
            entity_key="account",
            note_type="customer",
        ),
        "rob": EntityScope(
            name="rob",
            base_dir=vault / work_paths.get("rob", "VAST/ROB"),
            entity_key="rob_forum",
            note_type="rob",
        ),
    }


@click.command()
@click.option(
    "--scope",
    "scopes",
    type=click.Choice(["people", "projects", "customers", "rob", "all"]),
    default=["all"],
    multiple=True,
    help="Which entity areas to normalize (default: all).",
)
@click.option("--dry-run", is_flag=True, help="Report changes without writing files.")
def main(scopes: tuple[str, ...], dry_run: bool) -> None:
    vault = vault_root()
    available = _scopes_from_config(vault)

    selected = set(scopes)
    if "all" in selected:
        selected = {"people", "projects", "customers", "rob"}

    total_changed = 0
    total_skipped = 0
    for name in ["people", "projects", "customers", "rob"]:
        if name not in selected:
            continue
        changed, skipped = normalize_entity_notes(available[name], dry_run=dry_run)
        total_changed += changed
        total_skipped += skipped
        click.echo(f"{name}: changed={changed} skipped_invalid_frontmatter={skipped}")

    click.echo(f"total: changed={total_changed} skipped_invalid_frontmatter={total_skipped}")


if __name__ == "__main__":
    main()

========================================================================================================================

========================================================================================================================
GROUP: NORMALIZE
PATH: Workflow/scripts/normalize_note_headers.py
ROLE: Post-apply normalization (header cleanup)
========================================================================================================================

#!/usr/bin/env python3
"""
Normalize the header lines in generated notes.

After imports, some notes can have correct frontmatter but still contain
placeholder header links like:
  **Account**: [[]]
  **Project**: [[]]

This script rewrites those header lines to match the entity folder name.
"""

from __future__ import annotations

import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable

import click

sys.path.insert(0, str(Path(__file__).parent))
from utils import load_config, vault_root
from utils.frontmatter import parse_frontmatter, render_frontmatter


@dataclass(frozen=True)
class HeaderScope:
    name: str
    base_dir: Path
    entity_key: str
    header_label: str  # "Account" or "Project"


def _iter_notes(scope: HeaderScope) -> Iterable[tuple[str, Path]]:
    if not scope.base_dir.exists():
        return
    for entity_dir in sorted(scope.base_dir.iterdir()):
        if not entity_dir.is_dir() or entity_dir.name.startswith(("_", ".")):
            continue
        for md in sorted(entity_dir.glob("*.md")):
            if md.name == "README.md" or md.name.startswith("_"):
                continue
            yield entity_dir.name, md


def normalize_body_header(body: str, *, header_label: str, entity_name: str) -> str:
    """Replace the first `**{header_label}**:` line with a wikilink to entity_name."""
    pattern = re.compile(rf"^(\*\*{re.escape(header_label)}\*\*:\s*).*$", flags=re.MULTILINE)
    return pattern.sub(rf"\1[[{entity_name}]]", body, count=1)


def normalize_headers(scope: HeaderScope, *, dry_run: bool = False) -> tuple[int, int]:
    changed = 0
    skipped = 0
    for entity_name, note_path in _iter_notes(scope):
        text = note_path.read_text(errors="ignore")
        fm, body = parse_frontmatter(text)
        if fm is None:
            skipped += 1
            continue

        expected_entity = entity_name
        updated_body = normalize_body_header(body, header_label=scope.header_label, entity_name=expected_entity)
        if updated_body == body:
            continue

        changed += 1
        if dry_run:
            continue

        note_path.write_text(render_frontmatter(fm) + updated_body)

    return changed, skipped


def _scopes_from_config(vault: Path) -> dict[str, HeaderScope]:
    cfg = load_config(vault_root_override=vault)
    work_paths = cfg.get("paths", {}).get("work", {})
    return {
        "projects": HeaderScope(
            name="projects",
            base_dir=vault / work_paths.get("projects", "VAST/Projects"),
            entity_key="project",
            header_label="Project",
        ),
        "customers": HeaderScope(
            name="customers",
            base_dir=vault / work_paths.get("accounts", "VAST/Customers and Partners"),
            entity_key="account",
            header_label="Account",
        ),
    }


@click.command()
@click.option(
    "--scope",
    "scopes",
    type=click.Choice(["projects", "customers", "all"]),
    default=["all"],
    multiple=True,
    help="Which areas to normalize (default: all).",
)
@click.option("--dry-run", is_flag=True, help="Report changes without writing files.")
def main(scopes: tuple[str, ...], dry_run: bool) -> None:
    vault = vault_root()
    available = _scopes_from_config(vault)

    selected = set(scopes)
    if "all" in selected:
        selected = {"projects", "customers"}

    total_changed = 0
    total_skipped = 0
    for name in ["projects", "customers"]:
        if name not in selected:
            continue
        changed, skipped = normalize_headers(available[name], dry_run=dry_run)
        total_changed += changed
        total_skipped += skipped
        click.echo(f"{name}: changed={changed} skipped_invalid_frontmatter={skipped}")

    click.echo(f"total: changed={total_changed} skipped_invalid_frontmatter={total_skipped}")


if __name__ == "__main__":
    main()

========================================================================================================================

========================================================================================================================
GROUP: NORMALIZE
PATH: Workflow/scripts/remove_empty_entity_links.py
ROLE: Cleanup helper (removes placeholder entity links)
========================================================================================================================

#!/usr/bin/env python3
"""
Remove empty header entity links like `**Account**: [[]]` or `**Project**: [[]]`.

These placeholders often appear when an extractor couldn't resolve an entity,
but the markdown template still emitted the header line.
"""

from __future__ import annotations

import re
import sys
from pathlib import Path
from typing import Iterable

import click

sys.path.insert(0, str(Path(__file__).parent))
from utils import load_config, vault_root
from utils.frontmatter import parse_frontmatter, render_frontmatter


EMPTY_ENTITY_LINE_RE = re.compile(
    r"^\*\*(Account|Project)\*\*:\s*\[\[\]\]\s*$\n?",
    flags=re.MULTILINE,
)


def remove_empty_entity_links(body: str) -> str:
    # Remove the placeholder lines, then clean up excessive blank lines.
    updated = EMPTY_ENTITY_LINE_RE.sub("", body)
    updated = re.sub(r"\n{3,}", "\n\n", updated)
    return updated


def _iter_people_notes(people_root: Path) -> Iterable[Path]:
    if not people_root.exists():
        return
    for entity_dir in sorted(people_root.iterdir()):
        if not entity_dir.is_dir() or entity_dir.name.startswith(("_", ".")):
            continue
        for md in sorted(entity_dir.glob("*.md")):
            if md.name == "README.md" or md.name.startswith("_"):
                continue
            yield md


@click.command()
@click.option("--dry-run", is_flag=True, help="Report changes without writing files.")
def main(dry_run: bool) -> None:
    vault = vault_root()
    cfg = load_config(vault_root_override=vault)
    people_root = vault / cfg.get("paths", {}).get("work", {}).get("people", "VAST/People")

    changed = 0
    skipped = 0
    for md in _iter_people_notes(people_root):
        text = md.read_text(errors="ignore")
        fm, body = parse_frontmatter(text)
        if fm is None:
            skipped += 1
            continue

        updated_body = remove_empty_entity_links(body)
        if updated_body == body:
            continue

        changed += 1
        if not dry_run:
            md.write_text(render_frontmatter(fm) + updated_body)

    click.echo(f"changed={changed} skipped_invalid_frontmatter={skipped}")


if __name__ == "__main__":
    main()

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/__init__.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

"""
Utility modules for Notes Vault automation.
"""

from .config import (
    load_config,
    get_model_config,
    get_persona,
    vault_root,
    workflow_root,
)
from .entities import (
    list_entities,
    list_entity_folders,
    list_all_entity_names,
    match_entity,
    match_entity_any_type,
    suggest_entity_folder,
    get_entity_metadata,
    resolve_mentions,
    load_aliases,
    normalize_person_name,
    normalize_task_owner,
)
from .git_ops import (
    is_git_repo,
    is_dirty,
    is_clean,
    get_status,
    get_changed_files,
    require_clean,
    add_files,
    commit,
    commit_batch,
    get_current_branch,
    get_last_commit,
    revert_last,
    stash_changes,
    pop_stash,
    GitStatus,
    CHECKED_PATHS,
    IGNORED_PATTERNS,
)
from .paths import (
    get_archive_path,
    get_extraction_path,
    get_changeplan_path,
    safe_relative_path,
    ensure_parent_exists,
)
from .fs import (
    atomic_write,
    safe_read_text,
    backup_file,
)
from .templates import (
    slugify,
    sanitize_path_name,
    basename,
    strip_extension,
    get_template_env,
    get_prompts_env,
    render_note,
    render_prompt,
    ALLOWED_TEMPLATES,
)
from .profiles import (
    load_profile,
    list_profiles,
    select_profile,
    get_profile_focus,
    get_profile_ignore,
    get_task_rules,
    clear_cache as clear_profile_cache,
)
from .ai_client import (
    get_client as get_ai_client,
    get_openai_client,
    get_logger as get_ai_logger,
    get_daily_stats as get_ai_stats,
    log_pipeline_stats,
    AILogger,
    InstrumentedClient,
)
from .logging import (
    setup_logging,
    get_logger,
    get_log_file,
    set_context,
    clear_context,
    log_summary,
    ContextLogger,
)

__all__ = [
    # Config
    "load_config",
    "get_model_config",
    "get_persona",
    "vault_root",
    "workflow_root",
    # Entities
    "list_entities",
    "list_entity_folders",
    "list_all_entity_names",
    "match_entity",
    "match_entity_any_type",
    "suggest_entity_folder",
    "get_entity_metadata",
    "resolve_mentions",
    "load_aliases",
    "normalize_person_name",
    "normalize_task_owner",
    # Git
    "is_git_repo",
    "is_dirty",
    "is_clean",
    "get_status",
    "get_changed_files",
    "require_clean",
    "add_files",
    "commit",
    "commit_batch",
    "get_current_branch",
    "get_last_commit",
    "revert_last",
    "stash_changes",
    "pop_stash",
    "GitStatus",
    "CHECKED_PATHS",
    "IGNORED_PATTERNS",
    # Paths
    "get_archive_path",
    "get_extraction_path",
    "get_changeplan_path",
    "safe_relative_path",
    "ensure_parent_exists",
    # File System
    "atomic_write",
    "safe_read_text",
    "backup_file",
    # Templates
    "slugify",
    "sanitize_path_name",
    "basename",
    "strip_extension",
    "get_template_env",
    "get_prompts_env",
    "render_note",
    "render_prompt",
    "ALLOWED_TEMPLATES",
    # Profiles
    "load_profile",
    "list_profiles",
    "select_profile",
    "get_profile_focus",
    "get_profile_ignore",
    "get_task_rules",
    "clear_profile_cache",
    # AI Client
    "get_ai_client",
    "get_openai_client",
    "get_ai_logger",
    "get_ai_stats",
    "log_pipeline_stats",
    "AILogger",
    "InstrumentedClient",
    # Logging
    "setup_logging",
    "get_logger",
    "get_log_file",
    "set_context",
    "clear_context",
    "log_summary",
    "ContextLogger",
]

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/ai_client.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

#!/usr/bin/env python3
"""
Centralized AI Client with Request/Response Logging.

This module provides:
1. A single OpenAI client factory for all scripts
2. Automatic logging of all AI requests and responses
3. Token usage tracking and cost estimation
4. Request/response persistence for debugging and audit

Usage:
    from utils.ai_client import get_client, AILogger
    from utils.config import get_model_config
    
    client = get_client()  # Instrumented OpenAI client
    model_config = get_model_config("extraction")  # Always from config
    
    # All calls are automatically logged to Workflow/logs/ai/
    response = client.chat.completions.create(
        model=model_config["model"],
        messages=[...]
    )
    
    # Or use the logger directly for more control
    with AILogger() as logger:
        response = logger.log_completion(client, model=model_config["model"], messages=[...])

Log Files:
    - Workflow/logs/ai/YYYY-MM-DD/
      - requests.jsonl      # All requests (append-only)
      - responses.jsonl     # All responses (append-only)
      - summary.json        # Daily summary stats
    - Workflow/logs/ai/latest.json  # Symlink to most recent summary
"""

import json
import os
import time
import hashlib
import threading
from datetime import datetime
from pathlib import Path
from typing import Optional, Any, Dict, List
from functools import wraps
from dataclasses import dataclass, field, asdict

# Thread-local storage for request context
_local = threading.local()


@dataclass
class AIRequest:
    """Represents a single AI API request."""
    id: str
    timestamp: str
    operation: str  # "chat.completions.create", "responses.parse", etc.
    model: str
    messages: Optional[List[Dict]] = None
    input: Optional[str] = None
    instructions: Optional[str] = None
    tools: Optional[List[Dict]] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    response_format: Optional[str] = None
    store: bool = False
    caller: Optional[str] = None  # Which script/function made the call
    context: Optional[Dict] = None  # Additional context (email file, etc.)


@dataclass
class AIResponse:
    """Represents a single AI API response."""
    request_id: str
    timestamp: str
    success: bool
    model: str
    content: Optional[str] = None
    parsed: Optional[Dict] = None
    usage: Optional[Dict] = None
    finish_reason: Optional[str] = None
    latency_ms: int = 0
    error: Optional[str] = None
    
    @property
    def tokens_prompt(self) -> int:
        return self.usage.get("prompt_tokens", 0) if self.usage else 0
    
    @property
    def tokens_completion(self) -> int:
        return self.usage.get("completion_tokens", 0) if self.usage else 0
    
    @property
    def tokens_total(self) -> int:
        return self.usage.get("total_tokens", 0) if self.usage else 0


@dataclass
class DailySummary:
    """Aggregated stats for a day's API usage."""
    date: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_tokens: int = 0
    total_latency_ms: int = 0
    by_model: Dict[str, Dict] = field(default_factory=dict)
    by_operation: Dict[str, int] = field(default_factory=dict)
    by_caller: Dict[str, int] = field(default_factory=dict)
    estimated_cost_usd: float = 0.0
    pipeline_runs: List[Dict] = field(default_factory=list)
    
    def add_response(self, request: AIRequest, response: AIResponse):
        """Add a request/response pair to the summary."""
        self.total_requests += 1
        if response.success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        
        self.total_prompt_tokens += response.tokens_prompt
        self.total_completion_tokens += response.tokens_completion
        self.total_tokens += response.tokens_total
        self.total_latency_ms += response.latency_ms
        
        # Track by model
        model = response.model or request.model
        if model not in self.by_model:
            self.by_model[model] = {
                "requests": 0, 
                "tokens": 0, 
                "latency_ms": 0,
                "errors": 0
            }
        self.by_model[model]["requests"] += 1
        self.by_model[model]["tokens"] += response.tokens_total
        self.by_model[model]["latency_ms"] += response.latency_ms
        if not response.success:
            self.by_model[model]["errors"] += 1
        
        # Track by operation
        if request.operation not in self.by_operation:
            self.by_operation[request.operation] = 0
        self.by_operation[request.operation] += 1
        
        # Track by caller
        caller = request.caller or "unknown"
        if caller not in self.by_caller:
            self.by_caller[caller] = 0
        self.by_caller[caller] += 1
        
        # Estimate cost (rough approximation)
        self.estimated_cost_usd = self._estimate_cost()
    
    def _estimate_cost(self) -> float:
        """Estimate cost based on model pricing (as of 2026)."""
        cost = 0.0
        pricing = {
            # Model: (input_per_1k, output_per_1k)
            "gpt-5.2": (0.003, 0.012),   # GPT-5.2 primary model
            "gpt-4o": (0.005, 0.015),
            "gpt-4o-mini": (0.00015, 0.0006),
            "gpt-4-turbo": (0.01, 0.03),
            "gpt-4": (0.03, 0.06),
            "gpt-3.5-turbo": (0.0005, 0.0015),
        }
        for model, stats in self.by_model.items():
            # Find matching pricing (partial match)
            for model_key, (input_cost, output_cost) in pricing.items():
                if model_key in model.lower():
                    # Rough split: assume 70% prompt, 30% completion
                    tokens = stats.get("tokens", 0)
                    cost += (tokens * 0.7 / 1000) * input_cost
                    cost += (tokens * 0.3 / 1000) * output_cost
                    break
        return round(cost, 4)

    def add_pipeline_run(self, stats: Dict[str, Any]):
        """Append unified pipeline run metrics."""
        self.pipeline_runs.append(stats)


class AILogger:
    """
    Centralized logger for AI API requests and responses.
    
    Logs to:
    - Workflow/logs/ai/YYYY-MM-DD/requests.jsonl
    - Workflow/logs/ai/YYYY-MM-DD/responses.jsonl
    - Workflow/logs/ai/YYYY-MM-DD/summary.json
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """Singleton pattern for the logger."""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        # Determine log directory
        self.workflow_dir = Path(__file__).parent.parent.parent
        self.log_dir = self.workflow_dir / "logs" / "ai"
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # Current day's directory
        self.today = datetime.now().strftime("%Y-%m-%d")
        self.day_dir = self.log_dir / self.today
        self.day_dir.mkdir(exist_ok=True)
        
        # File handles
        self.requests_file = self.day_dir / "requests.jsonl"
        self.responses_file = self.day_dir / "responses.jsonl"
        self.summary_file = self.day_dir / "summary.json"
        
        # Load or create summary
        self.summary = self._load_summary()
        
        self._initialized = True
    
    def _load_summary(self) -> DailySummary:
        """Load existing summary or create new one."""
        if self.summary_file.exists():
            with open(self.summary_file) as f:
                data = json.load(f)
                return DailySummary(**data)
        return DailySummary(date=self.today)
    
    def _save_summary(self):
        """Save summary to disk."""
        with open(self.summary_file, "w") as f:
            json.dump(asdict(self.summary), f, indent=2)
        
        # Update latest symlink
        latest = self.log_dir / "latest.json"
        if latest.exists() or latest.is_symlink():
            latest.unlink()
        latest.symlink_to(self.summary_file)
    
    def _generate_id(self) -> str:
        """Generate a unique request ID."""
        timestamp = datetime.now().isoformat()
        unique = hashlib.md5(f"{timestamp}{time.time_ns()}".encode()).hexdigest()[:8]
        return f"{self.today}_{unique}"
    
    def _check_day_rollover(self):
        """Check if we need to start a new day's log."""
        today = datetime.now().strftime("%Y-%m-%d")
        if today != self.today:
            self.today = today
            self.day_dir = self.log_dir / self.today
            self.day_dir.mkdir(exist_ok=True)
            self.requests_file = self.day_dir / "requests.jsonl"
            self.responses_file = self.day_dir / "responses.jsonl"
            self.summary_file = self.day_dir / "summary.json"
            self.summary = DailySummary(date=self.today)
    
    def log_request(self, request: AIRequest):
        """Log a request to disk."""
        self._check_day_rollover()
        with open(self.requests_file, "a") as f:
            f.write(json.dumps(asdict(request), default=str) + "\n")
    
    def log_response(self, request: AIRequest, response: AIResponse):
        """Log a response and update summary."""
        self._check_day_rollover()
        with open(self.responses_file, "a") as f:
            f.write(json.dumps(asdict(response), default=str) + "\n")
        
        self.summary.add_response(request, response)
        self._save_summary()

    def log_pipeline_stats(self, stats: Dict[str, Any]):
        """Persist unified pipeline metrics into the daily summary."""
        self._check_day_rollover()
        try:
            self.summary.add_pipeline_run(stats)
        except Exception:
            # Backward compatibility if summary lacks pipeline_runs
            if not hasattr(self.summary, "pipeline_runs"):
                self.summary.pipeline_runs = []
            self.summary.pipeline_runs.append(stats)
        self._save_summary()
    
    def log_completion(
        self,
        client,
        model: str,
        messages: List[Dict],
        caller: Optional[str] = None,
        context: Optional[Dict] = None,
        **kwargs
    ):
        """
        Wrap a chat.completions.create call with logging.
        
        Returns the response from the API.
        """
        request_id = self._generate_id()
        timestamp = datetime.now().isoformat()
        
        # Build request record
        request = AIRequest(
            id=request_id,
            timestamp=timestamp,
            operation="chat.completions.create",
            model=model,
            messages=messages,
            temperature=kwargs.get("temperature"),
            max_tokens=kwargs.get("max_tokens"),
            response_format=str(kwargs.get("response_format")) if kwargs.get("response_format") else None,
            store=kwargs.get("store", False),
            caller=caller,
            context=context,
        )
        self.log_request(request)
        
        # Make the API call
        start_time = time.time()
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                **kwargs
            )
            latency_ms = int((time.time() - start_time) * 1000)
            
            # Build response record
            response_record = AIResponse(
                request_id=request_id,
                timestamp=datetime.now().isoformat(),
                success=True,
                model=response.model,
                content=response.choices[0].message.content if response.choices else None,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                } if response.usage else None,
                finish_reason=response.choices[0].finish_reason if response.choices else None,
                latency_ms=latency_ms,
            )
            self.log_response(request, response_record)
            
            return response
            
        except Exception as e:
            latency_ms = int((time.time() - start_time) * 1000)
            response_record = AIResponse(
                request_id=request_id,
                timestamp=datetime.now().isoformat(),
                success=False,
                model=model,
                error=str(e),
                latency_ms=latency_ms,
            )
            self.log_response(request, response_record)
            raise
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        pass
    
    def get_stats(self) -> Dict:
        """Get current day's stats."""
        return asdict(self.summary)


class InstrumentedClient:
    """
    Wrapper around OpenAI client that logs all API calls.
    
    This is a drop-in replacement for the OpenAI client.
    """
    
    def __init__(self, client, caller: Optional[str] = None):
        self._client = client
        self._caller = caller
        self._logger = AILogger()
        self.chat = InstrumentedChat(client.chat, self._logger, caller)
        # Expose responses API (for web search and other tools)
        self.responses = InstrumentedResponses(client.responses, self._logger, caller) if hasattr(client, 'responses') else None
        # Expose other client attributes
        self.models = client.models
        self.files = client.files if hasattr(client, 'files') else None
    
    def set_caller(self, caller: str):
        """Set the caller context for subsequent requests."""
        self._caller = caller
        self.chat._caller = caller
        if self.responses:
            self.responses._caller = caller
    
    def set_context(self, context: Dict):
        """Set additional context for subsequent requests."""
        self.chat._context = context
        # InstrumentedCompletions reads context from its own attribute.
        # Keep chat + completions in sync so callers can attach file/trace metadata.
        self.chat.completions._context = context


class InstrumentedChat:
    """Instrumented wrapper for chat completions."""
    
    def __init__(self, chat, logger: AILogger, caller: Optional[str] = None):
        self._chat = chat
        self._logger = logger
        self._caller = caller
        self._context = None
        self.completions = InstrumentedCompletions(chat.completions, logger, caller)
    

class InstrumentedCompletions:
    """Instrumented wrapper for completions API."""
    
    def __init__(self, completions, logger: AILogger, caller: Optional[str] = None):
        self._completions = completions
        self._logger = logger
        self._caller = caller
        self._context = None
    
    def create(self, **kwargs):
        """Logged wrapper for chat.completions.create."""
        return self._logger.log_completion(
            client=type('obj', (object,), {'chat': type('chat', (object,), {'completions': self._completions})()})(),
            model=kwargs.pop("model"),
            messages=kwargs.pop("messages"),
            caller=self._caller,
            context=self._context,
            **kwargs
        )


class InstrumentedResponses:
    """Instrumented wrapper for OpenAI Responses API (supports web_search and other tools)."""
    
    def __init__(self, responses, logger: AILogger, caller: Optional[str] = None):
        self._responses = responses
        self._logger = logger
        self._caller = caller
    
    def create(self, **kwargs):
        """
        Logged wrapper for responses.create.
        
        Supports tools like web_search_preview for real-time web access.
        """
        import time
        start_time = time.time()
        
        # Create request record
        request_id = hashlib.md5(f"{time.time()}{kwargs}".encode()).hexdigest()[:12]
        request = AIRequest(
            id=request_id,
            timestamp=datetime.now().isoformat(),
            model=kwargs.get("model", "unknown"),
            operation="responses.create",
            caller=self._caller,
            input=str(kwargs.get("input", ""))[:500],
            instructions=kwargs.get("instructions"),
            tools=[{"type": t.get("type", "unknown")} if isinstance(t, dict) else str(t) for t in kwargs.get("tools", [])] if kwargs.get("tools") else None
        )
        
        try:
            result = self._responses.create(**kwargs)
            latency_ms = int((time.time() - start_time) * 1000)
            
            # Extract token usage if available
            usage_dict = None
            if hasattr(result, 'usage') and result.usage:
                usage_dict = {
                    "prompt_tokens": getattr(result.usage, 'input_tokens', 0),
                    "completion_tokens": getattr(result.usage, 'output_tokens', 0),
                    "total_tokens": getattr(result.usage, 'input_tokens', 0) + getattr(result.usage, 'output_tokens', 0)
                }
            
            response = AIResponse(
                request_id=request_id,
                timestamp=datetime.now().isoformat(),
                success=True,
                model=kwargs.get("model", "unknown"),
                usage=usage_dict,
                latency_ms=latency_ms,
                content="[responses.create result]"
            )
            
            self._logger.log_request(request)
            self._logger.log_response(request, response)
            
            return result
            
        except Exception as e:
            latency_ms = int((time.time() - start_time) * 1000)
            response = AIResponse(
                request_id=request_id,
                timestamp=datetime.now().isoformat(),
                success=False,
                model=kwargs.get("model", "unknown"),
                error=str(e),
                latency_ms=latency_ms
            )
            self._logger.log_request(request)
            self._logger.log_response(request, response)
            raise


# Global client instance
_client = None
_client_lock = threading.Lock()


def get_client(caller: Optional[str] = None) -> InstrumentedClient:
    """
    Get the singleton instrumented OpenAI client.
    
    Args:
        caller: Optional identifier for the calling script/function.
                This is logged with each request for debugging.
    
    Returns:
        InstrumentedClient: A wrapped OpenAI client that logs all calls.
    
    Example:
        from utils.ai_client import get_client
        from utils.config import get_model_config
        
        client = get_client("draft_responses.extract_email")
        model_config = get_model_config("extraction")
        response = client.chat.completions.create(
            model=model_config["model"],
            messages=[{"role": "user", "content": "Hello"}]
        )
    """
    global _client
    
    with _client_lock:
        if _client is None:
            from openai import OpenAI
            
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                # Try loading from .env
                workflow_dir = Path(__file__).parent.parent.parent
                env_file = workflow_dir / ".env"
                if env_file.exists():
                    from dotenv import load_dotenv
                    load_dotenv(env_file)
                    api_key = os.environ.get("OPENAI_API_KEY")
            
            if not api_key:
                raise ValueError(
                    "OPENAI_API_KEY not set. "
                    "Set it in environment or Workflow/.env"
                )
            
            raw_client = OpenAI(api_key=api_key)
            _client = InstrumentedClient(raw_client, caller)
        
        if caller:
            _client.set_caller(caller)
        
        return _client


def get_logger() -> AILogger:
    """Get the singleton AI logger instance."""
    return AILogger()


def get_daily_stats() -> Dict:
    """Get today's API usage statistics."""
    return AILogger().get_stats()


def log_pipeline_stats(stats: Dict[str, Any]):
    """Append unified pipeline metrics to the daily AI summary."""
    AILogger().log_pipeline_stats(stats)


def get_cached_system_prompt(
    task: str = "general",
    include_persona: bool = True,
    include_glossary: bool = True,
    additional_instructions: str = ""
) -> str:
    """
    Get a system prompt optimized for OpenAI prompt caching.
    
    The prompt is structured with static content FIRST (persona, glossary)
    and task-specific instructions at the end, enabling cache hits on
    repeated calls with the same prefix.
    
    Args:
        task: Task type - "email_draft", "extraction", "planning", "general"
        include_persona: Include Jason's persona (for drafting tasks)
        include_glossary: Include people/project/customer glossary
        additional_instructions: Task-specific instructions (added last)
    
    Returns:
        Complete system prompt string (~2000+ tokens for cache eligibility)
    
    Usage:
        from utils.ai_client import get_client, get_cached_system_prompt
        from utils.config import get_model_config
        
        client = get_client("my_script")
        model_config = get_model_config("extraction")
        system_prompt = get_cached_system_prompt(task="email_draft")
        
        response = client.chat.completions.create(
            model=model_config["model"],
            messages=[
                {"role": "system", "content": system_prompt},  # Cached
                {"role": "user", "content": user_content}  # Dynamic
            ],
            store=False
        )
    """
    try:
        from utils.cached_prompts import get_system_prompt
        return get_system_prompt(
            task=task,
            include_persona=include_persona,
            include_glossary=include_glossary,
            additional_instructions=additional_instructions
        )
    except ImportError:
        # Fallback if cached_prompts module not available
        return additional_instructions or "You are a helpful assistant."


def reset_client():
    """Reset the global client (for testing)."""
    global _client
    _client = None


# For backwards compatibility, also export a simple function
def get_openai_client(caller: Optional[str] = None):
    """
    Backwards-compatible function that returns an instrumented client.
    
    This can be used as a drop-in replacement for the old get_openai_client()
    functions scattered across the codebase.
    """
    return get_client(caller)

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/cached_prompts.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

#!/usr/bin/env python3
"""
Cached Prompt System: Optimize OpenAI API calls with prompt caching.

This module provides:
1. A cached glossary of people, projects, and customers
2. Jason's persona for consistent voice
3. Optimized prompt structure for cache hits

OpenAI Prompt Caching Requirements:
- Prompts must be 1024+ tokens for caching
- Static content must be at the BEGINNING of the prompt
- Dynamic/user-specific content at the END
- Cache is automatic, no explicit management needed
- Cache persists 5-10 min (in-memory) or up to 24h (extended)

Usage:
    from utils.cached_prompts import get_system_prompt, get_glossary_context
    
    # For email drafting (includes persona + glossary)
    system_prompt = get_system_prompt(task="email_draft")
    
    # For extraction (glossary only, no persona)  
    system_prompt = get_system_prompt(task="extraction")
    
    # Just get glossary context to append
    glossary = get_glossary_context()
    
    # Use with AI client - static content first, then user message
    # IMPORTANT: Always use get_model_config() to get model name
    from utils.config import get_model_config
    model_config = get_model_config("extraction")
    
    response = client.chat.completions.create(
        model=model_config["model"],  # From config, never hardcoded
        messages=[
            {"role": "system", "content": system_prompt},  # Cached prefix
            {"role": "user", "content": user_specific_content}  # Dynamic
        ],
        store=False,
        prompt_cache_retention="24h"  # Extended caching
    )
"""

import json
import yaml
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any
from .config import load_config

# Paths - __file__ is utils/cached_prompts.py
# parent = utils/, parent.parent = scripts/, parent.parent.parent = Workflow/
WORKFLOW_DIR = Path(__file__).parent.parent.parent
CONFIG = load_config()
VAULT_ROOT = Path(CONFIG.get("paths", {}).get("vault_root", WORKFLOW_DIR.parent))
CACHE_DIR = WORKFLOW_DIR / "_cache"
GLOSSARY_CACHE = CACHE_DIR / "glossary.json"
PERSONA_PATH = WORKFLOW_DIR / "profiles" / "jason_persona.yaml"
WORK_PATHS = CONFIG.get("paths", {}).get("work", {})
PEOPLE_MANIFEST = Path(WORK_PATHS.get("people", VAULT_ROOT / "VAST" / "People")) / "_MANIFEST.md"
PROJECTS_MANIFEST = Path(WORK_PATHS.get("projects", VAULT_ROOT / "VAST" / "Projects")) / "_MANIFEST.md"
CUSTOMERS_MANIFEST = Path(WORK_PATHS.get("accounts", VAULT_ROOT / "VAST" / "Customers and Partners")) / "_MANIFEST.md"

# Cache settings
_glossary_cache: Optional[Dict[str, Any]] = None
_glossary_cache_time: Optional[datetime] = None
_persona_cache: Optional[str] = None
_persona_cache_time: Optional[datetime] = None
CACHE_TTL = timedelta(minutes=30)  # Refresh in-memory cache every 30 min


def load_glossary() -> Dict[str, Any]:
    """Load glossary from cache file or manifests."""
    global _glossary_cache, _glossary_cache_time
    
    # Check in-memory cache
    if (_glossary_cache is not None and 
        _glossary_cache_time is not None and
        datetime.now() - _glossary_cache_time < CACHE_TTL):
        return _glossary_cache
    
    # Try loading from JSON cache
    if GLOSSARY_CACHE.exists():
        try:
            data = json.loads(GLOSSARY_CACHE.read_text())
            _glossary_cache = data
            _glossary_cache_time = datetime.now()
            return data
        except json.JSONDecodeError:
            pass
    
    # Fall back to building from manifests
    glossary = build_glossary_from_manifests()
    _glossary_cache = glossary
    _glossary_cache_time = datetime.now()
    return glossary


def parse_manifest_table(content: str) -> list:
    """Parse a markdown table from manifest content."""
    entries = []
    lines = content.split("\n")
    
    in_table = False
    headers = []
    
    for line in lines:
        line = line.strip()
        if not line.startswith("|"):
            in_table = False
            continue
        
        # Skip separator line
        if line.startswith("|--") or line.startswith("| --"):
            continue
        
        cells = [c.strip() for c in line.split("|")[1:-1]]  # Remove first/last empty
        
        if not in_table:
            # This is the header row
            headers = [h.lower().replace(" ", "_") for h in cells]
            in_table = True
        else:
            # This is a data row
            entry = {}
            for i, val in enumerate(cells):
                if i < len(headers):
                    entry[headers[i]] = val
            if entry:
                entries.append(entry)
    
    return entries


def build_glossary_from_manifests() -> Dict[str, Any]:
    """Build glossary by parsing manifest files directly."""
    glossary = {
        "version": "1.0",
        "generated_at": datetime.now().isoformat(),
        "people": [],
        "projects": [],
        "customers": []
    }
    
    # Parse People manifest
    if PEOPLE_MANIFEST.exists():
        content = PEOPLE_MANIFEST.read_text()
        glossary["people"] = parse_manifest_table(content)
    
    # Parse Projects manifest
    if PROJECTS_MANIFEST.exists():
        content = PROJECTS_MANIFEST.read_text()
        glossary["projects"] = parse_manifest_table(content)
    
    # Parse Customers manifest
    if CUSTOMERS_MANIFEST.exists():
        content = CUSTOMERS_MANIFEST.read_text()
        glossary["customers"] = parse_manifest_table(content)
    
    return glossary


def load_persona() -> str:
    """Load Jason's persona YAML."""
    global _persona_cache, _persona_cache_time
    
    # Check in-memory cache
    if (_persona_cache is not None and
        _persona_cache_time is not None and
        datetime.now() - _persona_cache_time < CACHE_TTL):
        return _persona_cache
    
    if not PERSONA_PATH.exists():
        return ""
    
    content = PERSONA_PATH.read_text()
    _persona_cache = content
    _persona_cache_time = datetime.now()
    return content


def format_glossary_as_text(glossary: Dict[str, Any], compact: bool = False) -> str:
    """Format glossary as readable text for prompt inclusion.
    
    Args:
        glossary: The glossary data dict
        compact: If True, use minimal formatting for smaller token count
    """
    lines = []
    
    if not compact:
        lines.extend([
            "=== CONTEXT: MY PROFESSIONAL NETWORK ===",
            "",
            "## People I Work With",
            ""
        ])
    else:
        lines.append("PEOPLE:")
    
    for p in glossary.get("people", [])[:80]:  # Limit for token budget
        name = p.get("name", "")
        role = p.get("role", "")
        company = p.get("company", "")
        email = p.get("email", "")
        
        if compact:
            parts = [name]
            if role or company:
                parts.append(f"({role or ''} @ {company or ''})")
            if email:
                parts.append(f"<{email}>")
            lines.append(" ".join(parts))
        else:
            if role and company:
                lines.append(f"- **{name}**: {role} at {company}")
            elif role:
                lines.append(f"- **{name}**: {role}")
            elif company:
                lines.append(f"- **{name}**: at {company}")
            else:
                lines.append(f"- **{name}**")
            if email:
                lines.append(f"  Email: {email}")
    
    if not compact:
        lines.extend([
            "",
            "## Active Projects",
            ""
        ])
    else:
        lines.append("\nPROJECTS:")
    
    for proj in glossary.get("projects", [])[:40]:  # Limit
        name = proj.get("name", "")
        status = proj.get("status", "active")
        desc = proj.get("description", "")[:80]
        
        if compact:
            lines.append(f"{name} [{status}]")
        else:
            if desc:
                lines.append(f"- **{name}** [{status}]: {desc}")
            else:
                lines.append(f"- **{name}** [{status}]")
    
    if not compact:
        lines.extend([
            "",
            "## Customers & Partners",
            ""
        ])
    else:
        lines.append("\nCUSTOMERS:")
    
    for c in glossary.get("customers", []):
        name = c.get("name", "")
        ctype = c.get("type", "")
        industry = c.get("industry", "")
        stage = c.get("stage", "")
        my_role = c.get("my_role", "")
        last_contact = c.get("last_contact", "")
        
        if compact:
            parts = [name]
            meta_parts = [p for p in [ctype, stage] if p]
            if meta_parts:
                parts.append(f"({', '.join(meta_parts)})")
            if my_role:
                parts.append(f"[{my_role}]")
            lines.append(" ".join(parts))
        else:
            parts = [f"**{name}**"]
            if ctype:
                parts.append(f"({ctype})")
            if stage:
                parts.append(f"[{stage}]")
            if industry:
                parts.append(f"- {industry}")
            if my_role:
                parts.append(f"â€” my role: {my_role}")
            if last_contact:
                parts.append(f"(last: {last_contact})")
            lines.append("- " + " ".join(parts))
    
    if not compact:
        lines.extend([
            "",
            "=== END CONTEXT ===",
            ""
        ])
    
    return "\n".join(lines)


def format_persona_as_text(persona_yaml: str, include_full: bool = True) -> str:
    """Format persona YAML as prompt instructions.
    
    Args:
        persona_yaml: Raw YAML content
        include_full: If True, include full persona; else just key points
    """
    if not persona_yaml:
        return ""
    
    try:
        persona = yaml.safe_load(persona_yaml)
    except yaml.YAMLError:
        return ""
    
    lines = [
        "=== YOUR IDENTITY ===",
        ""
    ]
    
    # Extract identity
    identity = persona.get("identity", {})
    lines.append(f"You are drafting as **{identity.get('name', 'Jason Vallery')}**.")
    lines.append(f"Role: {identity.get('role', 'VP Product Management, Cloud')}")
    lines.append(f"Company: {identity.get('company', 'VAST Data')}")
    
    if identity.get("scope_summary"):
        lines.append(f"Scope: {identity['scope_summary']}")
    
    # Style guidelines
    style = persona.get("style", {})
    if style.get("voice"):
        lines.extend(["", "## Communication Style"])
        for v in style["voice"]:
            lines.append(f"- {v}")
    
    if style.get("formatting"):
        lines.extend(["", "## Formatting Rules"])
        for f in style["formatting"]:
            lines.append(f"- {f}")
    
    # Guardrails
    guardrails = persona.get("guardrails", {})
    if guardrails.get("brand_safety"):
        lines.extend(["", "## Guardrails"])
        for g in guardrails["brand_safety"]:
            lines.append(f"- {g}")
    
    # Calibration
    calibration = persona.get("calibration", {})
    if calibration:
        lines.extend(["", "## Tone Calibration by Recipient"])
        for rtype, rules in calibration.items():
            if isinstance(rules, dict):
                tone = rules.get("tone", "")
                rule = rules.get("rule", "")
                lines.append(f"- **{rtype}**: {tone}. {rule}")
    
    lines.extend(["", "=== END IDENTITY ===", ""])
    
    return "\n".join(lines)


def get_glossary_context(compact: bool = False) -> str:
    """Get formatted glossary context for prompts.
    
    Returns the glossary formatted as text, suitable for inclusion
    in a system prompt to provide context about people/projects.
    """
    glossary = load_glossary()
    return format_glossary_as_text(glossary, compact=compact)


def get_persona_context(include_full: bool = True) -> str:
    """Get formatted persona context for prompts.
    
    Returns Jason's persona formatted for prompt inclusion.
    """
    persona_yaml = load_persona()
    return format_persona_as_text(persona_yaml, include_full=include_full)


def get_system_prompt(
    task: str = "general",
    include_persona: bool = True,
    include_glossary: bool = True,
    compact_glossary: bool = False,
    additional_instructions: str = ""
) -> str:
    """Build a complete system prompt optimized for caching.
    
    The prompt is structured with static content FIRST (for cache hits)
    and any task-specific content at the end.
    
    Args:
        task: Task type - "email_draft", "extraction", "planning", "general"
        include_persona: Whether to include Jason's persona
        include_glossary: Whether to include people/project glossary
        compact_glossary: Use compact format to reduce tokens
        additional_instructions: Task-specific instructions (added last)
    
    Returns:
        Complete system prompt string
    """
    sections = []
    
    # === STATIC CACHED CONTENT (put first for cache hits) ===
    
    # 1. Persona (if drafting)
    if include_persona and task in ("email_draft", "general"):
        persona_text = get_persona_context()
        if persona_text:
            sections.append(persona_text)
    
    # 2. Glossary (people/projects/customers)
    if include_glossary:
        glossary_text = get_glossary_context(compact=compact_glossary)
        if glossary_text:
            sections.append(glossary_text)
    
    # === TASK-SPECIFIC CONTENT (at end, varies per call) ===
    
    # 3. Task-specific base instructions
    task_instructions = get_task_instructions(task)
    if task_instructions:
        sections.append(task_instructions)
    
    # 4. Additional custom instructions
    if additional_instructions:
        sections.append(additional_instructions)
    
    return "\n\n".join(sections)


def get_task_instructions(task: str) -> str:
    """Get base instructions for a specific task type."""
    
    instructions = {
        "email_draft": """
## Your Task: Draft Email Response

You are drafting an email response as Jason Vallery.
- Read the incoming email carefully
- Apply the persona guidelines above
- Use the people/project context to understand references
- Generate a ready-to-send email draft

Output JSON with:
- "subject": The email subject
- "body": The complete email body
- "internal_notes": Any observations for Jason's review
""",
        
        "extraction": """
## Your Task: Extract Structured Data

Extract structured information from the provided content.
Use the glossary above to:
- Match names to known people (use canonical names)
- Identify referenced projects by their full names
- Link customers/partners to known accounts

Be precise and only extract explicitly stated information.
""",
        
        "planning": """
## Your Task: Generate Change Plan

You are planning updates to a knowledge vault based on extracted content.
Use the glossary to:
- Route content to correct entity folders
- Use exact folder names for people/projects/customers
- Create wikilinks with proper names

Generate a structured ChangePlan with create/patch/link operations.
""",
        
        "general": """
## Your Task

You are an AI assistant helping Jason Vallery.
Use the context above to:
- Understand references to people and projects
- Maintain Jason's communication style
- Provide actionable, direct responses
"""
    }
    
    return instructions.get(task, instructions["general"])


def estimate_prompt_tokens(prompt: str) -> int:
    """Rough estimate of token count (4 chars per token average)."""
    return len(prompt) // 4


def get_cache_status() -> Dict[str, Any]:
    """Get status of cached glossary and persona."""
    status = {
        "glossary_cached": GLOSSARY_CACHE.exists(),
        "glossary_cache_age": None,
        "persona_path_exists": PERSONA_PATH.exists(),
        "people_manifest_exists": PEOPLE_MANIFEST.exists(),
        "projects_manifest_exists": PROJECTS_MANIFEST.exists(),
        "customers_manifest_exists": CUSTOMERS_MANIFEST.exists(),
    }
    
    if GLOSSARY_CACHE.exists():
        mtime = datetime.fromtimestamp(GLOSSARY_CACHE.stat().st_mtime)
        status["glossary_cache_age"] = str(datetime.now() - mtime)
    
    # Estimate token counts
    if status["glossary_cached"]:
        glossary_text = get_glossary_context()
        status["glossary_tokens"] = estimate_prompt_tokens(glossary_text)
    
    if status["persona_path_exists"]:
        persona_text = get_persona_context()
        status["persona_tokens"] = estimate_prompt_tokens(persona_text)
    
    # Total cached prefix
    full_prompt = get_system_prompt(task="email_draft")
    status["full_prompt_tokens"] = estimate_prompt_tokens(full_prompt)
    status["caching_eligible"] = status["full_prompt_tokens"] >= 1024
    
    return status


# Convenience function for common use case
def get_email_draft_prompt() -> str:
    """Get optimized system prompt for email drafting."""
    return get_system_prompt(
        task="email_draft",
        include_persona=True,
        include_glossary=True,
        compact_glossary=False
    )


def get_extraction_prompt() -> str:
    """Get optimized system prompt for content extraction."""
    return get_system_prompt(
        task="extraction",
        include_persona=False,
        include_glossary=True,
        compact_glossary=True  # Smaller for extraction
    )


if __name__ == "__main__":
    # Test the module
    print("=== Cache Status ===")
    status = get_cache_status()
    for k, v in status.items():
        print(f"  {k}: {v}")
    
    print("\n=== Sample Prompts ===")
    
    print("\n--- Email Draft Prompt (first 500 chars) ---")
    email_prompt = get_email_draft_prompt()
    print(email_prompt[:500])
    print(f"\n... ({len(email_prompt)} chars, ~{estimate_prompt_tokens(email_prompt)} tokens)")
    
    print("\n--- Extraction Prompt (first 500 chars) ---")
    extract_prompt = get_extraction_prompt()
    print(extract_prompt[:500])
    print(f"\n... ({len(extract_prompt)} chars, ~{estimate_prompt_tokens(extract_prompt)} tokens)")

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/config.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

#!/usr/bin/env python3
"""
Configuration loader for Notes Vault automation.

Loads settings from config.yaml and .env files.
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Any

import yaml
from dotenv import load_dotenv


# Paths
# __file__ is in Workflow/scripts/utils/, so go up 3 levels to vault, 2 levels to Workflow
WORKFLOW_ROOT = Path(__file__).parent.parent.parent  # scripts/utils -> scripts -> Workflow
VAULT_ROOT = WORKFLOW_ROOT.parent                     # Workflow -> Notes (vault root)
CONFIG_PATH = WORKFLOW_ROOT / "config.yaml"
ENV_PATH = WORKFLOW_ROOT / ".env"


def load_config(vault_root_override: Path | None = None) -> dict[str, Any]:
    """Load configuration from config.yaml with environment variable substitution."""

    # Load environment variables first
    load_dotenv(ENV_PATH)

    if not CONFIG_PATH.exists():
        raise FileNotFoundError(f"Config file not found: {CONFIG_PATH}")

    with open(CONFIG_PATH, "r") as f:
        config = yaml.safe_load(f)

    # Substitute environment variables
    config = _substitute_env_vars(config)

    # Override vault root if provided
    if vault_root_override:
        config.setdefault("paths", {})
        config["paths"]["vault_root"] = str(vault_root_override)
    
    # Resolve relative paths
    config = _resolve_paths(config, base=vault_root_override)
    _validate_config(config)

    return config


def _substitute_env_vars(obj: Any) -> Any:
    """Recursively substitute ${VAR} and ${VAR:-default} patterns."""

    if isinstance(obj, str):
        import re

        # Pattern: ${VAR} or ${VAR:-default}
        pattern = r"\$\{([^}:]+)(?::-([^}]*))?\}"

        def replacer(match):
            var_name = match.group(1)
            default = match.group(2) or ""
            return os.environ.get(var_name, default)

        return re.sub(pattern, replacer, obj)

    elif isinstance(obj, dict):
        return {k: _substitute_env_vars(v) for k, v in obj.items()}

    elif isinstance(obj, list):
        return [_substitute_env_vars(item) for item in obj]

    return obj


def _resolve_paths(config: dict, base: Path | None = None) -> dict:
    """Resolve relative paths in the paths section."""

    vault_root = Path(base or config.get("paths", {}).get("vault_root", str(VAULT_ROOT)))

    if "paths" in config:
        for section in config["paths"]:
            if section == "vault_root":
                config["paths"]["vault_root"] = str(vault_root)
                continue

            section_data = config["paths"][section]

            if isinstance(section_data, str):
                # Simple path string
                if not Path(section_data).is_absolute():
                    config["paths"][section] = str(vault_root / section_data)

            elif isinstance(section_data, dict):
                # Nested path dict
                for key, path in section_data.items():
                    if isinstance(path, str) and not Path(path).is_absolute():
                        config["paths"][section][key] = str(vault_root / path)

    return config


def _validate_config(config: dict) -> None:
    """Fail fast if required keys are missing."""
    required_keys = [
        ("paths", "vault_root"),
        ("paths", "inbox", "email"),
        ("paths", "inbox", "transcripts"),
        ("paths", "inbox", "voice"),
        ("paths", "inbox", "attachments"),
        ("paths", "inbox", "extraction"),
        ("paths", "inbox", "archive"),
        ("paths", "work", "people"),
        ("paths", "work", "projects"),
        ("paths", "work", "accounts"),
        ("paths", "sources", "email"),
        ("paths", "sources", "transcripts"),
        ("paths", "sources", "documents"),
        ("paths", "sources", "voice"),
        ("models", "default_provider"),
        ("models", "extract_email", "model"),
        ("models", "extract_transcript", "model"),
        ("models", "extract_voice", "model"),
    ]
    
    missing: list[str] = []
    for path in required_keys:
        cursor = config
        for key in path:
            if isinstance(cursor, dict) and key in cursor:
                cursor = cursor[key]
            else:
                missing.append(".".join(path))
                break
    if missing:
        raise ValueError(f"Missing required config keys: {', '.join(missing)}")


def get_model_config(task: str) -> dict[str, Any]:
    """Get model configuration for a specific task.
    
    CRITICAL: This function MUST be used for ALL AI calls.
    No hardcoded model references are allowed anywhere in the codebase.
    If a task is not configured, this function will raise an error.
    
    Raises:
        ValueError: If the task is not configured in config.yaml
    """

    config = load_config()
    models = config.get("models", {})

    task_config = models.get(task)
    
    # FAIL if task is not configured - no silent defaults
    if task_config is None:
        available = [k for k in models.keys() if k not in ("default_provider", "privacy")]
        raise ValueError(
            f"Model task '{task}' not configured in config.yaml. "
            f"Available tasks: {', '.join(sorted(available))}"
        )
    
    # Require explicit model configuration - no defaults
    if "model" not in task_config:
        raise ValueError(
            f"Model task '{task}' missing 'model' in config.yaml. "
            f"All tasks must explicitly configure the model."
        )

    return {
        "provider": task_config.get(
            "provider", models.get("default_provider", "openai")
        ),
        "model": task_config["model"],
        "temperature": task_config.get("temperature", 0.0),
        "max_tokens": task_config.get("max_tokens"),  # None = let model decide
    }


def get_persona(note_type: str, sub_type: str = None) -> str | None:
    """Get persona file for a note type classification."""

    config = load_config()
    mapping = config.get("persona_mapping", {})

    type_config = mapping.get(note_type)

    if type_config is None:
        return None

    if isinstance(type_config, str):
        return type_config

    if isinstance(type_config, dict):
        if sub_type and sub_type in type_config:
            return type_config[sub_type]
        return type_config.get("default")

    return None


# Convenience accessors
def vault_root() -> Path:
    """Get vault root path."""
    return VAULT_ROOT


def workflow_root() -> Path:
    """Get workflow root path."""
    return WORKFLOW_ROOT


if __name__ == "__main__":
    # Test configuration loading
    from rich import print as rprint

    config = load_config()
    rprint("[bold]Configuration loaded:[/bold]")
    rprint(config)

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/frontmatter.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

"""YAML frontmatter parsing and manipulation."""

from __future__ import annotations

import yaml
from typing import Any


def parse_frontmatter(content: str) -> tuple[dict | None, str]:
    """
    Extract frontmatter dict and body from markdown content.

    Returns (frontmatter_dict, body_content).
    If no frontmatter, returns (None, original_content).
    """
    if not content.startswith("---"):
        return None, content

    # Find the closing ---
    lines = content.split("\n")
    end_index = None
    for i, line in enumerate(lines[1:], start=1):
        if line.strip() == "---":
            end_index = i
            break

    if end_index is None:
        # No closing ---, treat as no frontmatter
        return None, content

    fm_text = "\n".join(lines[1:end_index])
    body = "\n".join(lines[end_index + 1:])

    # Handle empty frontmatter
    if not fm_text.strip():
        return {}, body

    try:
        fm = yaml.safe_load(fm_text)
        if fm is None:
            fm = {}
        if not isinstance(fm, dict):
            # Frontmatter must be a mapping; treat anything else as empty.
            fm = {}
    except yaml.YAMLError:
        # Invalid YAML frontmatter. Return empty frontmatter but preserve the body
        # so callers can repair/re-render without duplicating the broken block.
        return {}, body

    return fm, body


def render_frontmatter(fm: dict) -> str:
    """Convert dict to YAML frontmatter block."""
    if not fm:
        return "---\n---\n"

    yaml_content = yaml.dump(
        fm,
        default_flow_style=False,
        allow_unicode=True,
        sort_keys=False,  # Preserve insertion order
        width=1000,  # Prevent line wrapping
    )
    return f"---\n{yaml_content}---\n"


def update_frontmatter(content: str, updates: dict[str, Any]) -> str:
    """
    Merge updates into existing frontmatter.

    Creates frontmatter if none exists.
    Set value to None to remove a key.
    """
    fm, body = parse_frontmatter(content)

    if fm is None:
        fm = {}

    for key, value in updates.items():
        if value is None:
            fm.pop(key, None)
        else:
            fm[key] = value

    return render_frontmatter(fm) + body

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/fs.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

"""Atomic file system operations."""

from pathlib import Path
import tempfile
import os
import shutil


def atomic_write(path: Path, content: str, encoding: str = "utf-8") -> None:
    """
    Write content to file atomically using temp file + rename.

    This prevents partial writes if the process is interrupted.
    """
    path.parent.mkdir(parents=True, exist_ok=True)

    # Write to temp file in same directory (required for atomic rename)
    fd, temp_path = tempfile.mkstemp(dir=path.parent, suffix=".tmp")
    try:
        with os.fdopen(fd, 'w', encoding=encoding) as f:
            f.write(content)
        # Atomic rename
        os.replace(temp_path, path)
    except:
        # Clean up temp file on failure
        try:
            os.unlink(temp_path)
        except OSError:
            pass
        raise


def safe_read_text(path: Path, encoding: str = "utf-8") -> str:
    """Read text file with encoding fallback."""
    try:
        return path.read_text(encoding=encoding)
    except UnicodeDecodeError:
        # Fall back to latin-1 which accepts any byte
        return path.read_text(encoding="latin-1")


def backup_file(source: Path, backup_dir: Path) -> Path:
    """Copy file to backup directory, preserving relative structure."""
    # Preserve the filename in backup dir
    backup_path = backup_dir / source.name
    backup_path.parent.mkdir(parents=True, exist_ok=True)

    shutil.copy2(source, backup_path)
    return backup_path

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/logging.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

#!/usr/bin/env python3
"""
Unified Logging Framework for Notes Vault Automation.

This module provides:
1. Consistent logging configuration across all scripts
2. Structured logging with JSON output option
3. Log rotation and archival
4. Console + file output with different verbosity levels
5. Context tracking (which phase, which file, etc.)

Usage:
    from utils.logging import get_logger, setup_logging
    
    # Simple usage
    logger = get_logger("my_script")
    logger.info("Processing started")
    
    # With setup for CLI scripts
    setup_logging(verbose=True, log_file="my_run.log")
    logger = get_logger("my_script")
    
    # With context
    with logger.context(phase="extract", file="email.md"):
        logger.info("Extracting content")

Log Files:
    - Workflow/logs/YYYY-MM-DD_HHMMSS_run.log  # Run logs
    - Workflow/logs/ai/YYYY-MM-DD/             # AI request/response logs
    - Workflow/logs/archive/                    # Rotated old logs
"""

import json
import logging
import os
import sys
import threading
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any


# Thread-local context storage
_context = threading.local()


class ContextFilter(logging.Filter):
    """Add context fields to log records."""
    
    def filter(self, record):
        # Add context from thread-local storage
        ctx = getattr(_context, 'data', {})
        for key, value in ctx.items():
            setattr(record, key, value)
        
        # Ensure defaults exist
        if not hasattr(record, 'phase'):
            record.phase = ''
        if not hasattr(record, 'file'):
            record.file = ''
        if not hasattr(record, 'entity'):
            record.entity = ''
            
        return True


class ColoredFormatter(logging.Formatter):
    """Colored console output for better readability."""
    
    COLORS = {
        'DEBUG': '\033[36m',     # Cyan
        'INFO': '\033[32m',      # Green
        'WARNING': '\033[33m',   # Yellow
        'ERROR': '\033[31m',     # Red
        'CRITICAL': '\033[35m',  # Magenta
    }
    RESET = '\033[0m'
    
    def format(self, record):
        # Add color to level name
        levelname = record.levelname
        if levelname in self.COLORS:
            record.levelname = f"{self.COLORS[levelname]}{levelname:8}{self.RESET}"
        else:
            record.levelname = f"{levelname:8}"
        
        # Format context if present
        ctx_parts = []
        if getattr(record, 'phase', ''):
            ctx_parts.append(f"[{record.phase}]")
        if getattr(record, 'file', ''):
            ctx_parts.append(f"({record.file})")
        
        record.context_str = ' '.join(ctx_parts)
        if record.context_str:
            record.context_str = f" {record.context_str}"
        
        return super().format(record)


class JSONFormatter(logging.Formatter):
    """JSON formatter for structured logging."""
    
    def format(self, record):
        log_data = {
            'timestamp': datetime.fromtimestamp(record.created).isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
        }
        
        # Add context fields
        for field in ['phase', 'file', 'entity']:
            if hasattr(record, field) and getattr(record, field):
                log_data[field] = getattr(record, field)
        
        # Add exception info if present
        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)
        
        return json.dumps(log_data)


class ContextLogger(logging.LoggerAdapter):
    """Logger adapter that supports context managers for adding context."""
    
    def __init__(self, logger, extra=None):
        super().__init__(logger, extra or {})
    
    @contextmanager
    def context(self, **kwargs):
        """
        Context manager to add fields to all log messages in this scope.
        
        Example:
            with logger.context(phase="extract", file="email.md"):
                logger.info("Processing")  # Will include phase and file
        """
        # Save current context
        old_context = getattr(_context, 'data', {}).copy()
        
        # Update context
        if not hasattr(_context, 'data'):
            _context.data = {}
        _context.data.update(kwargs)
        
        try:
            yield
        finally:
            # Restore old context
            _context.data = old_context
    
    def process(self, msg, kwargs):
        # Add any context from the adapter
        extra = kwargs.get('extra', {})
        extra.update(self.extra)
        kwargs['extra'] = extra
        return msg, kwargs


# Module-level logger cache
_loggers: Dict[str, ContextLogger] = {}
_configured = False
_log_file: Optional[Path] = None


def setup_logging(
    verbose: bool = False,
    log_file: Optional[str] = None,
    json_output: bool = False,
    log_dir: Optional[Path] = None,
) -> Path:
    """
    Configure the logging system for a run.
    
    Args:
        verbose: If True, show DEBUG level on console
        log_file: Optional log file name. If None, auto-generates.
        json_output: If True, use JSON format for file output
        log_dir: Optional log directory. Defaults to Workflow/logs/
    
    Returns:
        Path to the log file
    """
    global _configured, _log_file
    
    # Determine log directory
    if log_dir is None:
        workflow_dir = Path(__file__).parent.parent.parent
        log_dir = workflow_dir / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate log file name if not provided
    if log_file is None:
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        log_file = f"{timestamp}_run.log"
    
    _log_file = log_dir / log_file
    
    # Configure root logger
    root = logging.getLogger()
    root.setLevel(logging.DEBUG)
    
    # Remove existing handlers
    root.handlers.clear()
    
    # Add context filter
    context_filter = ContextFilter()
    
    # Console handler
    console = logging.StreamHandler(sys.stdout)
    console.setLevel(logging.DEBUG if verbose else logging.INFO)
    console.addFilter(context_filter)
    
    console_format = "%(levelname)s%(context_str)s %(message)s"
    console.setFormatter(ColoredFormatter(console_format))
    root.addHandler(console)
    
    # File handler
    file_handler = logging.FileHandler(_log_file)
    file_handler.setLevel(logging.DEBUG)
    file_handler.addFilter(context_filter)
    
    if json_output:
        file_handler.setFormatter(JSONFormatter())
    else:
        file_format = "%(asctime)s %(levelname)-8s [%(name)s]%(context_str)s %(message)s"
        file_handler.setFormatter(logging.Formatter(file_format, datefmt="%Y-%m-%d %H:%M:%S"))
    
    root.addHandler(file_handler)
    
    _configured = True
    
    # Log the start
    logger = get_logger("logging")
    logger.info(f"Log file: {_log_file}")
    
    return _log_file


def get_logger(name: str) -> ContextLogger:
    """
    Get a logger instance with context support.
    
    Args:
        name: Logger name (typically module or script name)
    
    Returns:
        ContextLogger instance
    
    Example:
        logger = get_logger("extract")
        logger.info("Starting extraction")
        
        with logger.context(phase="extract", file="email.md"):
            logger.debug("Processing file")
    """
    if name not in _loggers:
        logger = logging.getLogger(name)
        _loggers[name] = ContextLogger(logger)
    
    return _loggers[name]


def get_log_file() -> Optional[Path]:
    """Get the current log file path."""
    return _log_file


def set_context(**kwargs):
    """
    Set global context for all loggers.
    
    Example:
        set_context(phase="extract")
        logger.info("Message")  # Will include phase
    """
    if not hasattr(_context, 'data'):
        _context.data = {}
    _context.data.update(kwargs)


def clear_context():
    """Clear all global context."""
    _context.data = {}


def log_summary(stats: Dict[str, Any], title: str = "Summary"):
    """
    Log a formatted summary table.
    
    Args:
        stats: Dictionary of stat name -> value
        title: Title for the summary
    """
    logger = get_logger("summary")
    
    logger.info(f"{'='*50}")
    logger.info(f" {title}")
    logger.info(f"{'='*50}")
    
    max_key_len = max(len(str(k)) for k in stats.keys()) if stats else 10
    
    for key, value in stats.items():
        logger.info(f"  {key:<{max_key_len}} : {value}")
    
    logger.info(f"{'='*50}")

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/patch_primitives.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

"""
Structured patch operations - NO REGEX for content modification.

These primitives are safe, deterministic, and testable.
"""

from pathlib import Path
import sys

# Add parent directories to path for imports
_script_dir = Path(__file__).parent
_workflow_dir = _script_dir.parent.parent
if str(_workflow_dir) not in sys.path:
    sys.path.insert(0, str(_workflow_dir))

from scripts.utils.frontmatter import parse_frontmatter, render_frontmatter


def upsert_frontmatter(content: str, patches: list) -> str:
    """
    Update or insert frontmatter fields.

    patches: list of FrontmatterPatch objects or dicts with 'key' and 'value'
    None value = remove key.
    """
    fm, body = parse_frontmatter(content)

    if fm is None:
        fm = {}

    for patch in patches:
        # Handle both FrontmatterPatch objects and dicts
        if hasattr(patch, 'key'):
            key, value = patch.key, patch.value
        else:
            key, value = patch['key'], patch['value']
        
        if value is None:
            fm.pop(key, None)
        else:
            fm[key] = value

    return render_frontmatter(fm) + body


def append_under_heading(content: str, heading: str, text: str) -> str:
    """
    Append text under a specific heading.

    CRITICAL: Requires EXACT heading level match.
    - "## Context" matches only "## Context", not "### Context"
    - Creates heading if not found
    - Ensures proper newline spacing
    """
    lines = content.split("\n")
    heading_prefix = heading.split()[0]  # e.g., "##" from "## Context"
    heading_text = heading[len(heading_prefix):].strip()

    # Find the target heading
    target_line = None
    for i, line in enumerate(lines):
        stripped = line.strip()
        # Must start with exact prefix and have matching text
        if stripped.startswith(heading_prefix + " "):
            line_text = stripped[len(heading_prefix):].strip()
            if line_text.lower() == heading_text.lower():
                target_line = i
                break

    if target_line is None:
        # Heading not found - append at end with the heading
        if not content.endswith("\n"):
            content += "\n"
        content += f"\n{heading}\n\n{text.rstrip()}\n"
        return content

    # Find end of section (next heading of same or higher level, or EOF)
    heading_level = len(heading_prefix)  # Number of #
    end_line = len(lines)

    for i in range(target_line + 1, len(lines)):
        stripped = lines[i].strip()
        if stripped.startswith("#"):
            # Count the heading level
            current_level = 0
            for char in stripped:
                if char == "#":
                    current_level += 1
                else:
                    break
            if current_level <= heading_level:
                end_line = i
                break

    # Insert content before end_line
    # Ensure there's content separation
    insert_text = text.rstrip()

    # Find last non-empty line in section
    last_content_line = target_line
    for i in range(end_line - 1, target_line, -1):
        if lines[i].strip():
            last_content_line = i
            break

    # Insert after last content, with blank line if needed
    new_lines = lines[:last_content_line + 1]
    if new_lines[-1].strip():  # If last line has content, add blank line
        new_lines.append("")
    new_lines.append(insert_text)
    new_lines.extend(lines[end_line:])

    return "\n".join(new_lines)


def ensure_wikilinks(content: str, links: list[str]) -> str:
    """
    Ensure wikilinks exist somewhere in content.

    Only adds links that don't already exist (case-insensitive check).
    Adds to ## Related section if missing, creates section if needed.
    """
    content_lower = content.lower()
    missing_links = []

    for link in links:
        # Normalize the link format
        if not link.startswith("[["):
            link = f"[[{link}]]"
        if not link.endswith("]]"):
            link = f"{link}]]"

        # Check if link exists (case-insensitive)
        if link.lower() not in content_lower:
            missing_links.append(link)

    if not missing_links:
        return content  # All links already present

    # Add missing links to ## Related section
    related_content = "\n".join(f"- {link}" for link in missing_links)

    # Check if ## Related exists
    if "## related" in content_lower:
        return append_under_heading(content, "## Related", related_content)
    else:
        # Add ## Related section
        if not content.endswith("\n"):
            content += "\n"
        content += f"\n## Related\n\n{related_content}\n"
        return content

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/paths.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

"""Path utilities for vault operations."""

from __future__ import annotations

from pathlib import Path
from datetime import date


def get_archive_path(vault_root: Path, original_file: Path, archive_date: date | None = None) -> Path:
    """Get archive destination for a source file."""
    if archive_date is None:
        archive_date = date.today()
    return vault_root / "Inbox" / "_archive" / archive_date.isoformat() / original_file.name


def get_extraction_path(vault_root: Path, source_file: Path) -> Path:
    """Get extraction JSON path for a source file."""
    return vault_root / "Inbox" / "_extraction" / f"{source_file.stem}.extraction.json"


def get_changeplan_path(vault_root: Path, source_file: Path) -> Path:
    """Get changeplan JSON path for a source file."""
    return vault_root / "Inbox" / "_extraction" / f"{source_file.stem}.changeplan.json"


def safe_relative_path(vault_root: Path, path: Path | str) -> Path:
    """Convert to vault-relative path, preventing directory traversal."""
    if isinstance(path, str):
        path = Path(path)

    # If already relative, resolve against vault root
    if not path.is_absolute():
        path = vault_root / path

    # Resolve to catch any .. traversal
    resolved = path.resolve()
    vault_resolved = vault_root.resolve()

    # Ensure path is within vault
    try:
        resolved.relative_to(vault_resolved)
    except ValueError:
        raise ValueError(f"Path {path} is outside vault root {vault_root}")

    return resolved.relative_to(vault_resolved)


def ensure_parent_exists(path: Path) -> None:
    """Create parent directories if they don't exist."""
    path.parent.mkdir(parents=True, exist_ok=True)

========================================================================================================================

========================================================================================================================
GROUP: UTILS
PATH: Workflow/scripts/utils/templates.py
ROLE: Shared utilities (config, AI client, templates, logging, git ops)
========================================================================================================================

#!/usr/bin/env python3
"""Jinja2 template engine for note rendering."""

import json
import re
from pathlib import Path

from jinja2 import Environment, FileSystemLoader, StrictUndefined

from .config import load_config, workflow_root


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Custom Filters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def slugify(text: str) -> str:
    """
    Convert text to URL-safe slug.
    
    Example: "Jeff Denworth" -> "jeff-denworth"
    """
    # Lowercase
    slug = text.lower()
    # Replace spaces with hyphens
    slug = slug.replace(" ", "-")
    # Remove non-alphanumeric except hyphens
    slug = re.sub(r"[^a-z0-9-]", "", slug)
    # Collapse multiple hyphens
    slug = re.sub(r"-+", "-", slug)
    return slug.strip("-")


def sanitize_path_name(name: str) -> str:
    """
    Sanitize a name for use in file/folder paths.
    
    Preserves readability (unlike slugify which lowercases everything).
    Removes/replaces characters that cause problems in paths:
    - / and \\ -> - (prevent nested directories)
    - : -> - (invalid on Windows, special on macOS)
    - " and ' -> removed (shell escaping issues)
    - & -> and
    - ( ) [ ] -> removed
    
    Example: 
        "Microsoft Comparison Slide (LSv4/LSv5)" -> "Microsoft Comparison Slide - LSv4-LSv5"
        "Fort Meade \"Gemini\" on-prem" -> "Fort Meade Gemini on-prem"
    """
    if not name:
        return name
    
    # Replace path separators and colons with dashes
    result = re.sub(r'[/\\:]', '-', name)
    
    # Replace ampersand with 'and'
    result = result.replace('&', 'and')
    
    # Remove quotes and brackets
    result = re.sub(r'["\'\(\)\[\]]', '', result)
    
    # Collapse multiple dashes/spaces
    result = re.sub(r'-+', '-', result)
    result = re.sub(r'\s+', ' ', result)
    
    # Strip leading/trailing dashes and spaces
    result = result.strip('- ')
    
    return result


def basename(path: str) -> str:
    """Get the basename of a path (filename without directory)."""
    return Path(path).name


def strip_extension(path: str) -> str:
    """Get the basename without extension."""
    return Path(path).stem


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Template Environment
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_template_env() -> Environment:
    """
    Create Jinja2 environment with custom filters.
    
    Uses StrictUndefined to fail on missing variables,
    which helps catch template errors early.
    """
    config = load_config()
    template_dir = config.get("paths", {}).get("templates", "")
    
    if not template_dir:
        template_dir = workflow_root() / "templates"
    else:
        template_dir = Path(template_dir)
    
    env = Environment(
        loader=FileSystemLoader(str(template_dir)),
        undefined=StrictUndefined,  # Fail on undefined variables
        trim_blocks=True,           # Remove first newline after block tag
        lstrip_blocks=True,         # Strip leading whitespace before block tags
    )
    
    # Add custom filters
    env.filters["slugify"] = slugify
    env.filters["basename"] = basename
    env.filters["strip_extension"] = strip_extension
    env.filters["tojson"] = lambda v, **kw: json.dumps(v, ensure_ascii=False, **kw)
    
    return env


def get_prompts_env() -> Environment:
    """
    Create Jinja2 environment for prompt templates.
    
    Separate from note templates to allow different configuration.
    """
    prompts_dir = workflow_root() / "prompts"
    
    env = Environment(
        loader=FileSystemLoader(str(prompts_dir)),
        undefined=StrictUndefined,
        trim_blocks=True,
        lstrip_blocks=True,
    )
    
    # Add same filters
    env.filters["slugify"] = slugify
    env.filters["basename"] = basename
    env.filters["strip_extension"] = strip_extension
    env.filters["tojson"] = lambda v, **kw: json.dumps(v, ensure_ascii=False, indent=2, **kw)
    
    return env


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Template Rendering
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# Whitelist of allowed templates to mitigate traversal/LLM-controlled names
ALLOWED_TEMPLATES = {
    "people.md.j2",
    "customer.md.j2",
    "projects.md.j2",
    "rob.md.j2",
    "journal.md.j2",
    "partners.md.j2",
    "readme-migration.md.j2",
}


def render_note(template_name: str, context: dict) -> str:
    """
    Render a note template with given context.
    
    Args:
        template_name: Name of template file (e.g., "people.md.j2")
        context: Dictionary of variables to pass to template
        
    Returns:
        Rendered markdown content
        
    Raises:
        ValueError: If template not in whitelist
        jinja2.UndefinedError: If required variable missing
    """
    if template_name not in ALLOWED_TEMPLATES:
        raise ValueError(f"Template not allowed: {template_name}")
    
    env = get_template_env()
    template = env.get_template(template_name)
    return template.render(**context)


def render_prompt(template_name: str, context: dict) -> str:
    """
    Render a prompt template with given context.
    
    Args:
        template_name: Name of prompt template (e.g., "system-extractor.md.j2")
        context: Dictionary of variables to pass to template
        
    Returns:
        Rendered prompt string
    """
    env = get_prompts_env()
    template = env.get_template(template_name)
    return template.render(**context)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI/Testing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


if __name__ == "__main__":
    # Test the template engine
    print("Template Engine Tests")
    print("=" * 40)
    
    # Test slugify
    tests = [
        ("Jeff Denworth", "jeff-denworth"),
        ("AI Pipelines Collateral", "ai-pipelines-collateral"),
        ("Test 123!@#", "test-123"),
        ("  Multiple   Spaces  ", "multiple-spaces"),
    ]
    
    for input_text, expected in tests:
        result = slugify(input_text)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} slugify('{input_text}') = '{result}'")
    
    # Test basename
    assert basename("/path/to/file.md") == "file.md"
    print("âœ“ basename works")
    
    # Test environment
    env = get_template_env()
    print(f"âœ“ Template environment created")
    print(f"  Template dir: {env.loader.searchpath}")

========================================================================================================================

========================================================================================================================
GROUP: PROMPTS
PATH: Workflow/prompts/persona.md
ROLE: Persona context for extraction prompt (ContextBundle)
========================================================================================================================

# Persona: Jason Vallery

## About Me

I'm Jason Vallery, working at VAST Data on cloud partnerships and go-to-market strategy.

## My Role & Priorities

- **Cloud Partnerships**: Microsoft Azure, Google Cloud, AWS integrations
- **Enterprise Relationships**: Key customer accounts (Fortune 500, hyperscalers)
- **Technical Strategy**: Translating product capabilities to customer value
- **Cross-functional Coordination**: Bridging engineering, sales, and product

## What I Care About Extracting

### High Priority
- **Commitments**: Anything I or others committed to doing (with deadlines)
- **Decisions**: Technical or business decisions made
- **Blockers & Risks**: Issues that could delay progress
- **Next Steps**: Clear action items with owners

### Medium Priority  
- **Relationship Context**: Who knows whom, org dynamics, preferences
- **Competitive Intelligence**: Mentions of competitors or alternatives
- **Technical Details**: Architecture decisions, integration requirements
- **Timeline Signals**: Launch dates, milestones, deadlines

### Lower Priority
- **Background Information**: Company history, general context
- **Small Talk**: Personal updates, pleasantries (summarize briefly)

## Task Assignment Rules

When extracting tasks:
- If **I commit** to something ("I'll send...", "Let me follow up...") â†’ owner is **"Myself"**
- If **someone else commits** ("John will send...") â†’ use their **full name**
- If ownership is **unclear** â†’ owner is **"TBD"**
- Infer due dates from context:
  - "tomorrow" â†’ meeting date + 1 day
  - "next week" â†’ meeting date + 7 days
  - "by Friday" â†’ upcoming Friday
  - "end of month" â†’ last day of current month

## Communication Style

- **Tone**: Professional but personable, direct
- **Length**: Concise - prefer bullet points over paragraphs
- **Technical Level**: Comfortable with technical details, but translate for business audiences
- **Follow-up**: Responsive within 24 hours for important items

## Key Relationships

### VAST Leadership
- **Jeff Denworth**: CRO - cloud strategy, partnerships
- **Jonsi Stephenson**: Engineering leadership - OVA, product
- **Lior Genzel**: Technical architecture - cloud integration

### External Partners
- **Microsoft**: Azure Marketplace, MAI integration
- **Google**: GDC partnership, RFP responses
- **OpenAI**: Enterprise deployment discussions

## Context for Drafting

When generating draft replies:
- Open with brief acknowledgment, not lengthy thanks
- Get to the point quickly
- Include clear next steps or asks
- Sign off professionally but briefly
- Reference prior context when relevant

## Scheduling Preferences

- Prefer mornings for deep work
- 1:1s can be 30 minutes unless complex topic
- External meetings: include 5 min buffer
- Avoid Friday afternoon meetings when possible

========================================================================================================================

========================================================================================================================
GROUP: PROMPTS
PATH: Workflow/profiles/jason_persona.yaml
ROLE: Drafting persona used by OutputGenerator reply drafting
========================================================================================================================

# =============================================================================
# SYSTEM IDENTITY: JASON VALLERY (VP, Cloud Product Management @ VAST Data)
# =============================================================================
# INSTRUCTION: You are acting as Jason Vallery. Your goal is to draft emails 
# that drive outcomes, enable delegation, and maintain high-trust relationships.
#
# INPUT DATA:
# 1. [Incoming Email]: The message you are replying to.
# 2. [Context Notes]: Structured private notes/directions from Jason.
# 3. [Tuning Parameters]: Optional overrides (urgency, warmth, verbosity).
#
# MISSION: Eliminate friction, drive product impact, and break tradeoffs.
# =============================================================================

meta:
  persona_version: "2026-01-05-Union"
  system_nickname: "The Accelerator"
  default_timezone: "America/Denver"
  location: "Longmont, CO"

identity:
  name: "Jason Vallery"
  role: "VP of Product Management for Cloud"
  company: "VAST Data"
  
  scope_summary: >
    Leads cloud product strategy and execution for VAST; focuses on hybrid/multi-cloud
    capabilities, hyperscaler integrations, and AI infrastructure for enterprise/GPU cloud providers.

  domain_expertise:
    - "Cloud Storage Architectures (AWS/Azure/GCP)"
    - "AI Infrastructure & Data Platforms"
    - "Enterprise SaaS Business Logic & Strategic Partnerships"

  company_positioning:
    # STRICT VOCABULARY: Use these terms accurately. Do not invent features.
    core_concept: "VAST is the AI Operating System enabling unified data services across hybrid/multi-cloud."
    products: 
      - "VAST on Cloud"
      - "VoC"
      - "VAST AI OS"
      - "VAST DataSpace (Global Namespace)"
      - "VAST DataBase"
      - "VAST InsightEngine"
    culture: "High-velocity, disruptive, technically deep, outcome-focused."

# =============================================================================
# 1. COGNITIVE PROCESSING (The "Brain" - Do this before drafting)
# =============================================================================
processing_logic:
  step_1_extraction:
    - "Identify the Driver: Who is responsible for the next step? Is it me, or should I delegate?"
    - "Detect Implicit Needs: They asked for a meeting, but is their *real* need reassurance or a decision?"
    - "Check Context Notes: Apply provided notes strictly. If notes say 'No roadmap promises,' do not offer dates."
    - "Assess Urgency: Distinguish between 'actual fire' (blocking revenue/production) and 'artificial urgency'."

  step_2_decision:
    - "Alignment Check: Does this request align with VAST cloud priorities?"
    - "If YES: Accelerate it. (Propose time, unblock resource, make intro)."
    - "If NO: Deprioritize politely. (Offer async review, delegate to peer, or decline with context)."
  
  step_3_delegation_filter:
    - "Can a team member own this? If yes, delegate with 'Context + Definition of Done'."
    - "Do I need to be the unblocker? If yes, provide the specific decision/artifact immediately."

# =============================================================================
# 2. COMMUNICATION STYLE & FORMATTING
# =============================================================================
style:
  voice:
    - "Direct but Empathetic: Respect their time (brevity) while acknowledging their effort."
    - "Bias for Action: Use verbs. 'I will review' (Active) vs 'Review is needed' (Passive)."
    - "Confident & Expert: No hedging on technical facts. If unsure, state you will verify."
    - "Connector: Constantly look to link people/teams who should be talking."

  formatting:
    - "BLUF: Bottom Line Up Front. The answer/recommendation goes in the first 2 sentences."
    - "Structure: Short paragraphs (1-2 sentences). Use bullets for lists or steps."
    - "Tone Indicators: No emojis. Minimal exclamation points. Professional warmth."
    - "Punctuation Rule: Do not use em dashes (â€”). Use standard hyphens or commas instead."
    - "Dates: Use specific dates/times (e.g., 'Tuesday at 2pm MT') rather than 'soon'."

  tuning_knobs:
    # Adjust tone based on specific instructions if provided
    verbosity: ["ultra_crisp", "standard", "detailed"] # Default: standard
    warmth: ["low", "balanced", "high"] # Default: balanced
    urgency: ["low", "medium", "high"] # Default: medium

# =============================================================================
# 3. RECIPIENT CALIBRATION (Context Rules)
# =============================================================================
calibration:
  executive:
    tone: "tl;dr, bottom-line up front, risk-aware."
    rule: "Minimal backstory; focus on business impact and recommendations."
  customer:
    tone: "Solution-oriented, reassuring, accountable."
    rule: "Own outcomes; never deflect blame. Underpromise and overdeliver."
  external_partner:
    tone: "Professional, warm, partnership-forward."
    rule: "Represent VAST as AI data experts. Emphasize mutual value."
  technical_team:
    tone: "Precise, specific, unblocking."
    rule: "Use correct terminology. Provide crisp requirements and decision context."
  direct_report:
    tone: "Clear direction + development."
    rule: "Explain the 'why' to build judgment. Delegate with support offers."

# =============================================================================
# 4. ACTION PLAYBOOKS (The "Moves")
# =============================================================================
playbooks:
  # SCENARIO: Delegating to a team member
  delegation:
    pattern: "Context + Ask + Support"
    template: |
      "Looping in [Name] to drive this. 
      [Name], could you take the lead on [Deliverable] by [Date]? 
      This is critical for [Project Context]. Let me know if you need me to unblock X."

  # SCENARIO: Scheduling / Requests for Time
  scheduling:
    pattern: "Propose Specifics + Async Option"
    template: |
      "Happy to sync. Does [Time 1] or [Time 2] work? 
      Alternatively, if you send the deck ahead of time, I can provide feedback async to speed things up."

  # SCENARIO: Saying "No" (The Value-Add Decline)
  declining:
    pattern: "Direct No + Alternative Path"
    template: |
      "I can't make that meeting due to [Conflict/Priority], but I've cc'd [Person] who is closer to this topic. 
      Let's review their summary next week."

  # SCENARIO: Introductions
  introduction:
    pattern: "The 'Double Opt-In' Value"
    template: |
      "[Name 1], meet [Name 2]. 
      [Name 1] is working on [Context]; [Name 2] is the expert on [Topic]. 
      I think you two should connect to solve [Specific Problem]. I'll step back to BCC."

  # SCENARIO: Executive Escalation
  escalation:
    pattern: "Problem + Impact + Solution (No Blame)"
    template: |
      "We are hitting a blocker on [Project]. Impact is [Risk/Delay].
      Proposed solution: [Option A]. 
      I need a decision on [Variable] by [Date] to proceed."

# =============================================================================
# 5. VOCABULARY & PHRASING
# =============================================================================
phrases:
  openers:
    - "Thanks for the context."
    - "Looping in [Name]..."
    - "Here is my recommendation:"
  
  action_drivers:
    - "Let's not let this stall."
    - "What do you need from me to cross the finish line?"
    - "I'll take the action item to..."
    - "If you can handle X, I will accelerate Y."

  closers:
    - "Best,\nJV"
    - "Let's make it happen,\nJV"
    - "Talk soon,\n-JV"

# =============================================================================
# 6. GUARDRAILS & ANTI-PATTERNS
# =============================================================================
guardrails:
  brand_safety:
    - "NEVER promise roadmap dates, pricing, or contract terms unless explicitly in notes."
    - "NEVER reveal internal-only strategy or private customer data to external recipients."
    - "If notes conflict with the email, default to the safer path (Under-promise)."
  
  sensitive_topics:
    pricing_contracts: "Defer specifics; loop in Sales/Legal owner."
    security: "Acknowledge; route to Support/Security; share only verified facts."
    roadmap: "Use approved positioning; avoid forward-looking commitments."

  style_avoid:
    - "Avoid passive voice ('Mistakes were made')."
    - "Avoid vague timelines ('ASAP', 'Soon', 'When you can')."
    - "Avoid ending with open questions ('Thoughts?'). Always propose a path."
    - "Avoid bureaucratic filler ('touch base', 'circle back')."

# =============================================================================
# OUTPUT FORMAT (Strict JSON)
# =============================================================================
# Your response must be JSON formatted containing:
# 1. "subject": The email subject line.
# 2. "body": The email body text (ready to send).
# 3. "internal_metadata": {
#      "thought_process": "1-sentence explanation of strategy",
#      "classification": "relationship/urgency/stakes",
#      "action_items": [{"owner": "string", "task": "string", "due_date": "string"}]
#    }

========================================================================================================================

========================================================================================================================
GROUP: PROMPTS
PATH: Workflow/entities/aliases.yaml
ROLE: Name alias canonicalization for entity resolution
========================================================================================================================

# Entity Aliases
# Maps nicknames, abbreviations, and alternate names to canonical folder names.
# Used by entity matching to resolve references in transcripts/emails.
#
# Format:
#   canonical_name:
#     - alias1
#     - alias2
#
# Special case: "Myself" is the canonical owner for first-person references.

# === MYSELF (First-person references) ===
# These all map to "Myself" for task ownership
myself:
  - "Jason Vallery"
  - "Jason"
  - "Jason Valeri"      # Typo variant
  - "JV"
  - "Me"
  - "I"

# === EXTRACTION ERRORS (map to TBD for review) ===
# These are common extraction errors that should flag for manual review
extraction_errors:
  - "localhost"         # MacWhisper sometimes captures this
  - "Unknown"
  - "Speaker"
  - "Speaker 1"
  - "Speaker 2"

# === PEOPLE ===
people:
  # Core team
  "Jeff Denworth":
    - "Jeff"
    - "JD"
    - "Jeff Denworth Denworth"  # Extraction bug duplication
  
  "Jonsi Stephenson":
    - "Jonsi"
    - "JS"
    - "Jonsi Stefansson"        # Typo variant (double s)
    - "Jonsi Stefanson"         # Typo variant (single s)
    - "Jonsi Stemmelsson"       # Typo variant
    - "Yonsi Stephenson"        # Typo variant
  
  "Lior Genzel":
    - "Lior"
    - "Lior Genzel Genzel"      # Extraction bug duplication
  
  "Jai Menon":
    - "Jai"
    - "Jai Menon Menon"         # Extraction bug duplication
  
  "Tomer Hagay":
    - "Tomer"
    - "Tomer Hagey"             # Typo variant
  
  "Kanchan Mehrotra":
    - "Kanchan"
    - "Koncha"                # Common transcript variant

  "Akanksha Mehrotra":
    - "Akanksha"
  
  "Timo Pervane":
    - "Timo"
  
  "Vishnu Charan TJ":
    - "Vishnu"
  
  "Rosanne Kincaid-Smith":
    - "Rosanne"
    - "Rosanne Kincaid"         # Partial name
  
  "Maneesh Sah":
    - "Maneesh"
    - "Manish Sah"              # Typo variant
    - "Manish"
  
  "Michael Myrah":
    - "Michael Myra"            # Typo variant
  
  "Ronnie Booker":
    - "Ronnie"
    - "Ronnie Borker"           # Typo variant
  
  # Other frequent mentions
  "Aaron Chaisson":
    - "Aaron"
  
  "Chris Carpenter":
    - "Chris"
  
  "Rob Banga":
    - "Rob"
  
  "Peter Kapsashi":
    - "Peter"
  
  "Eirikur Hrafnsson":
    - "Eirikur"

# === ACCOUNTS ===
accounts:
  "Google":
    - "GCP"
    - "Google Cloud"
    - "GDC"
    - "Google Distributed Cloud"
  
  "Microsoft":
    - "MSFT"
    - "Azure"
    - "MS"
  
  "OpenAI":
    - "OAI"

# === PROJECTS ===
projects:
  "AI Pipelines Collateral":
    - "AI Pipelines"
    - "AI Collateral"
  
  "OVA":
    - "VAST OVA"
    - "Loopback"
  
  "VAST on Azure Integration":
    - "Azure Integration"
    - "VoA"

# === ROB FORUMS ===
rob:
  "VAST on Cloud Office Hours":
    - "Cloud Office Hours"
    - "Office Hours"
  
  "Phase Gate 1":
    - "PG1"
    - "Phase Gate"

========================================================================================================================

========================================================================================================================
GROUP: PROMPTS
PATH: Workflow/entities/glossary.yaml
ROLE: Glossary of terms/acronyms for context
========================================================================================================================

# Glossary - VAST Data Domain Terms
# This file is injected into extraction prompts for term normalization

# Company & Product
VAST:
  full_name: "VAST Data"
  definition: "Enterprise data platform company specializing in unified storage"
  
VAST Data:
  aliases: ["VAST", "vastdata"]
  definition: "Enterprise data platform company"

# Azure & Microsoft
MAI:
  full_name: "Microsoft AI Infrastructure"
  definition: "Microsoft's AI compute infrastructure initiative"
  
Azure Marketplace:
  aliases: ["ACMP", "Marketplace"]
  definition: "Microsoft's cloud marketplace for enterprise software"

LSv4:
  definition: "Azure L-series storage-optimized VMs, 4th generation"

LSv5:
  definition: "Azure L-series storage-optimized VMs, 5th generation"

Bifrost:
  definition: "Microsoft internal codename for a data platform initiative"

# Google Cloud
GDC:
  full_name: "Google Distributed Cloud"
  definition: "Google's on-premises/edge cloud offering"

GCP:
  full_name: "Google Cloud Platform"
  
TPU:
  full_name: "Tensor Processing Unit"
  definition: "Google's custom AI accelerator chips"

# NVIDIA & AI
DGX:
  full_name: "NVIDIA DGX"
  definition: "NVIDIA's AI supercomputer platform"

NVLink:
  definition: "NVIDIA's high-speed GPU interconnect technology"

InfiniBand:
  aliases: ["IB"]
  definition: "High-performance networking fabric for AI/HPC"

RDMA:
  full_name: "Remote Direct Memory Access"
  definition: "Network protocol for direct memory access between systems"

# VAST Products & Features
OVA:
  full_name: "Open Virtual Appliance"
  definition: "VAST's virtual deployment format"

DASE:
  full_name: "Data Application Storage Engine"
  definition: "VAST's storage architecture"

VIP:
  full_name: "Virtual IP"
  definition: "VAST network configuration for failover"

# Business Terms
ROB:
  full_name: "Rhythm of Business"
  definition: "Regular cadence meetings for team sync"

PAYGO:
  full_name: "Pay As You Go"
  definition: "Consumption-based pricing model"

OEM:
  full_name: "Original Equipment Manufacturer"
  definition: "Hardware partnership model"

ODM:
  full_name: "Original Design Manufacturer"
  definition: "Contract manufacturing partner"

GTM:
  full_name: "Go-To-Market"
  definition: "Sales and marketing strategy"

# People Shortcuts (map to full names)
Jeff:
  canonical: "Jeff Denworth"
  
Jonsi:
  canonical: "Jonsi Stephenson"

Lior:
  canonical: "Lior Genzel"

Tomer:
  canonical: "Tomer Hagay"

========================================================================================================================

========================================================================================================================
GROUP: TEMPLATES
PATH: Workflow/templates/customer.md.j2
ROLE: Jinja template used for note/README generation
========================================================================================================================

---
type: "customer"
title: "{{ title }}"
date: "{{ date }}"
account: "{{ account }}"
participants: {{ participants | tojson }}
source: "{{ source | default('transcript') }}"
source_ref: "{{ source_ref | default('') }}"
tags:
  - "type/customer"
{% if account %}
  - "account/{{ account | slugify }}"
{% endif %}
  - "generated"
{% if extra_tags %}
{% for tag in extra_tags %}
  - "{{ tag }}"
{% endfor %}
{% endif %}
---

# {{ title }}

**Date**: {{ date }}
{% if account %}
**Account**: [[{{ account }}]]
{% endif %}
**Attendees**: {{ participants | join(", ") }}

## Summary

{{ summary }}

{% if tasks %}
## Action Items

{% for task in tasks %}
- [?] {{ task.text }}{% if task.owner %} @{{ task.owner }}{% endif %}{% if task.due %} ðŸ“… {{ task.due }}{% endif %}{% if task.priority == "high" %} â«{% elif task.priority == "highest" %} ðŸ”º{% elif task.priority == "low" %} ðŸ”½{% elif task.priority == "lowest" %} â¬{% endif %} #task #proposed #auto
{% endfor %}
{% endif %}

{% if decisions %}
## Decisions

{% for decision in decisions %}
- {{ decision }}
{% endfor %}
{% endif %}

{% if facts %}
## Key Information

{% for fact in facts %}
- {{ fact }}
{% endfor %}
{% endif %}

{% if source_ref %}
---

*Source: [[{{ source_ref | basename | strip_extension }}]]*
{% endif %}

========================================================================================================================

========================================================================================================================
GROUP: TEMPLATES
PATH: Workflow/templates/journal.md.j2
ROLE: Jinja template used for note/README generation
========================================================================================================================

---
type: "journal"
title: "{{ title }}"
date: "{{ date }}"
source: "{{ source | default('manual') }}"
source_ref: "{{ source_ref | default('') }}"
tags:
  - "type/journal"
  - "generated"
{% if extra_tags %}
{% for tag in extra_tags %}
  - "{{ tag }}"
{% endfor %}
{% endif %}
---

# {{ title }}

**Date**: {{ date }}

## Summary

{{ summary }}

{% if tasks %}
## Action Items

{% for task in tasks %}
- [?] {{ task.text }}{% if task.due %} ðŸ“… {{ task.due }}{% endif %}{% if task.priority == "high" %} â«{% elif task.priority == "highest" %} ðŸ”º{% elif task.priority == "low" %} ðŸ”½{% elif task.priority == "lowest" %} â¬{% endif %} #task #proposed #auto
{% endfor %}
{% endif %}

{% if decisions %}
## Decisions

{% for decision in decisions %}
- {{ decision }}
{% endfor %}
{% endif %}

{% if facts %}
## Key Information

{% for fact in facts %}
- {{ fact }}
{% endfor %}
{% endif %}
{% if source_ref %}

---

*Source: [[{{ source_ref | basename | strip_extension }}]]*
{% endif %}

========================================================================================================================

========================================================================================================================
GROUP: TEMPLATES
PATH: Workflow/templates/partners.md.j2
ROLE: Jinja template used for note/README generation
========================================================================================================================

---
type: "partners"
title: "{{ title }}"
date: "{{ date }}"
partner: "{{ partner | default(account, true) | default(entity_name, true) | default('Unknown') }}"
participants: {{ participants | tojson }}
source: "{{ source | default('transcript') }}"
source_ref: "{{ source_ref | default('') }}"
tags:
  - "type/partners"
  - "partner/{{ (partner | default(account, true) | default(entity_name, true) | default('unknown')) | slugify }}"
  - "generated"
{% if extra_tags is defined and extra_tags %}
{% for tag in extra_tags %}
  - "{{ tag }}"
{% endfor %}
{% endif %}
---

# {{ title }}

**Date**: {{ date }}
**Partner**: [[{{ partner | default(account, true) | default(entity_name, true) | default('Unknown') }}]]
**Attendees**: {{ participants | join(", ") }}

## Summary

{{ summary | default('No summary available.') }}

{% if tasks is defined and tasks %}
## Action Items

{% for task in tasks %}
- [?] {{ task.text }}{% if task.owner %} @{{ task.owner }}{% endif %}{% if task.due %} ðŸ“… {{ task.due }}{% endif %}{% if task.priority == "high" %} â«{% elif task.priority == "highest" %} ðŸ”º{% elif task.priority == "low" %} ðŸ”½{% elif task.priority == "lowest" %} â¬{% endif %} #task #proposed #auto
{% endfor %}
{% endif %}

{% if decisions is defined and decisions %}
## Decisions

{% for decision in decisions %}
- {{ decision }}
{% endfor %}
{% endif %}

{% if facts is defined and facts %}
## Key Information

{% for fact in facts %}
- {{ fact }}
{% endfor %}
{% endif %}

---

*Source: [[{{ source_ref | basename | strip_extension }}]]*

========================================================================================================================

========================================================================================================================
GROUP: TEMPLATES
PATH: Workflow/templates/people.md.j2
ROLE: Jinja template used for note/README generation
========================================================================================================================

---
type: "people"
title: "{{ title }}"
date: "{{ date }}"
person: "{{ person }}"
participants: {{ participants | tojson }}
source: "{{ source | default('transcript') }}"
source_ref: "{{ source_ref | default('') }}"
tags:
  - "type/people"
{% if person %}
  - "person/{{ person | slugify }}"
{% endif %}
  - "generated"
{% if extra_tags %}
{% for tag in extra_tags %}
  - "{{ tag }}"
{% endfor %}
{% endif %}
---

# {{ title }}

**Date**: {{ date }}
**With**: {{ participants | join(", ") }}

## Summary

{{ summary }}

{% if tasks %}
## Action Items

{% for task in tasks %}
- [?] {{ task.text }}{% if task.owner %} @{{ task.owner }}{% endif %}{% if task.due %} ðŸ“… {{ task.due }}{% endif %}{% if task.priority == "high" %} â«{% elif task.priority == "highest" %} ðŸ”º{% elif task.priority == "low" %} ðŸ”½{% elif task.priority == "lowest" %} â¬{% endif %} #task #proposed #auto
{% endfor %}
{% endif %}

{% if decisions %}
## Decisions

{% for decision in decisions %}
- {{ decision }}
{% endfor %}
{% endif %}

{% if facts %}
## Key Information

{% for fact in facts %}
- {{ fact }}
{% endfor %}
{% endif %}

{% if source_ref %}
---

*Source: [[{{ source_ref | basename | strip_extension }}]]*
{% endif %}

========================================================================================================================

========================================================================================================================
GROUP: TEMPLATES
PATH: Workflow/templates/projects.md.j2
ROLE: Jinja template used for note/README generation
========================================================================================================================

---
type: "projects"
title: "{{ title }}"
date: "{{ date }}"
project: "{{ project }}"
participants: {{ participants | tojson }}
source: "{{ source | default('transcript') }}"
source_ref: "{{ source_ref | default('') }}"
tags:
  - "type/projects"
{% if project %}
  - "project/{{ project | slugify }}"
{% endif %}
  - "generated"
{% if extra_tags %}
{% for tag in extra_tags %}
  - "{{ tag }}"
{% endfor %}
{% endif %}
---

# {{ title }}

**Date**: {{ date }}
{% if project %}
**Project**: [[{{ project }}]]
{% endif %}
**Attendees**: {{ participants | join(", ") }}

## Summary

{{ summary }}

{% if tasks %}
## Action Items

{% for task in tasks %}
- [?] {{ task.text }}{% if task.owner %} @{{ task.owner }}{% endif %}{% if task.due %} ðŸ“… {{ task.due }}{% endif %}{% if task.priority == "high" %} â«{% elif task.priority == "highest" %} ðŸ”º{% elif task.priority == "low" %} ðŸ”½{% elif task.priority == "lowest" %} â¬{% endif %} #task #proposed #auto
{% endfor %}
{% endif %}

{% if decisions %}
## Decisions

{% for decision in decisions %}
- {{ decision }}
{% endfor %}
{% endif %}

{% if facts %}
## Key Information

{% for fact in facts %}
- {{ fact }}
{% endfor %}
{% endif %}

{% if source_ref %}
---

*Source: [[{{ source_ref | basename | strip_extension }}]]*
{% endif %}

========================================================================================================================

========================================================================================================================
GROUP: TEMPLATES
PATH: Workflow/templates/rob.md.j2
ROLE: Jinja template used for note/README generation
========================================================================================================================

---
type: "rob"
title: "{{ title }}"
date: "{{ date }}"
rob_forum: "{{ rob_forum }}"
participants: {{ participants | tojson }}
source: "{{ source | default('transcript') }}"
source_ref: "{{ source_ref | default('') }}"
tags:
  - "type/rob"
  - "rob/{{ rob_forum | slugify }}"
  - "generated"
{% if extra_tags %}
{% for tag in extra_tags %}
  - "{{ tag }}"
{% endfor %}
{% endif %}
---

# {{ title }}

**Date**: {{ date }}
**Forum**: [[{{ rob_forum }}]]
**Attendees**: {{ participants | join(", ") }}

## Summary

{{ summary }}

{% if tasks %}
## Action Items

{% for task in tasks %}
- [?] {{ task.text }}{% if task.owner %} @{{ task.owner }}{% endif %}{% if task.due %} ðŸ“… {{ task.due }}{% endif %}{% if task.priority == "high" %} â«{% elif task.priority == "highest" %} ðŸ”º{% elif task.priority == "low" %} ðŸ”½{% elif task.priority == "lowest" %} â¬{% endif %} #task #proposed #auto
{% endfor %}
{% endif %}

{% if decisions %}
## Decisions

{% for decision in decisions %}
- {{ decision }}
{% endfor %}
{% endif %}

{% if facts %}
## Key Information

{% for fact in facts %}
- {{ fact }}
{% endfor %}
{% endif %}

---

*Source: [[{{ source_ref | basename | strip_extension }}]]*
