# NAS & NFS Configuration

← [Documentation Index](../index.md)

## Synology NAS Overview

The Synology DS920+ serves as centralized storage for backups, media, and shared data that doesn't require the high performance of local NVMe storage.

### Hardware Configuration
- **Model**: Synology DiskStation DS920+
- **CPU**: Intel Celeron J4125 (4C/4T, 2.0-2.7 GHz)
- **RAM**: 8 GB DDR4 (4 GB base + 4 GB upgrade)
- **Storage**: 4×8 TB WD Red Plus (WD80EFBX)
- **Network**: 10 GbE connection (via Netgear switch)
- **Expansion**: 1×eSATA port for DX517 expansion unit

### Storage Pool Configuration
- **RAID Type**: SHR-1 (Synology Hybrid RAID with 1-disk redundancy)
- **Raw Capacity**: 32 TB (4×8 TB drives)
- **Usable Capacity**: ~22 TB after RAID overhead and formatting
- **File System**: Btrfs with built-in snapshots and compression

## NFS Export Configuration

### Export Directory Structure
```
/volume1/
├── backup/           # Automated backup destination
├── containers/       # Shared container persistent data
├── media/           # Multimedia content library
├── home-infra/      # Infrastructure configs and docs
├── ai-models/       # Shared AI/ML model repository
├── datasets/        # Training data and datasets
└── archive/         # Long-term cold storage
```

### NFS Exports Table

| Export Path | Clients | Permissions | Options | Purpose |
|-------------|---------|-------------|---------|---------|
| `/volume1/backup` | 192.168.30.0/24 | rw | sync,no_subtree_check | Backup destination |
| `/volume1/containers` | 192.168.30.0/24 | rw | sync,no_subtree_check | Shared app data |
| `/volume1/media` | 192.168.30.0/24 | ro | sync,no_subtree_check | Read-only media |
| `/volume1/home-infra` | 192.168.30.0/24 | rw | sync,no_subtree_check | Config repository |
| `/volume1/ai-models` | 192.168.30.20-22 | rw | sync,no_subtree_check | GPU workers only |
| `/volume1/datasets` | 192.168.30.20-22 | rw | sync,no_subtree_check | GPU workers only |

### NFS Service Configuration
```bash
# /etc/exports on Synology (via DSM interface)
/volume1/backup 192.168.30.0/24(rw,sync,no_subtree_check,root_squash)
/volume1/containers 192.168.30.0/24(rw,sync,no_subtree_check,root_squash)
/volume1/media 192.168.30.0/24(ro,sync,no_subtree_check,root_squash)
/volume1/home-infra 192.168.30.0/24(rw,sync,no_subtree_check,root_squash)
/volume1/ai-models 192.168.30.20(rw,sync,no_subtree_check,root_squash) 192.168.30.21(rw,sync,no_subtree_check,root_squash) 192.168.30.22(rw,sync,no_subtree_check,root_squash)
/volume1/datasets 192.168.30.20(rw,sync,no_subtree_check,root_squash) 192.168.30.21(rw,sync,no_subtree_check,root_squash) 192.168.30.22(rw,sync,no_subtree_check,root_squash)
```

## Client Mount Configuration

### Automated Mounting via Ansible
All NFS mounts are configured through the `storage_nfs` role:

```yaml
# group_vars/all/storage.yml
nfs_mounts:
  - src: "obsidian.servers.vallery.net:/volume1/backup"
    dest: "/data/backup"
    opts: "nfs4,rw,hard,intr,timeo=600,retrans=2"
    state: mounted

  - src: "obsidian.servers.vallery.net:/volume1/containers"
    dest: "/data/containers"
    opts: "nfs4,rw,hard,intr,timeo=600,retrans=2"
    state: mounted

  - src: "obsidian.servers.vallery.net:/volume1/media"
    dest: "/data/media"
    opts: "nfs4,ro,hard,intr,timeo=600,retrans=2"
    state: mounted
```

### fstab Configuration
```bash
# /etc/fstab entries (auto-generated by Ansible)
obsidian.servers.vallery.net:/volume1/backup /data/backup nfs4 rw,hard,intr,timeo=600,retrans=2 0 0
obsidian.servers.vallery.net:/volume1/containers /data/containers nfs4 rw,hard,intr,timeo=600,retrans=2 0 0
obsidian.servers.vallery.net:/volume1/media /data/media nfs4 ro,hard,intr,timeo=600,retrans=2 0 0
```

### Mount Options Explained
- **nfs4**: Use NFSv4 protocol for better performance and security
- **hard**: Hard mount - retries indefinitely on failure
- **intr**: Allow interruption of NFS operations
- **timeo=600**: 60-second timeout for operations
- **retrans=2**: Retry failed operations twice before timeout

## Performance Optimization

### Network Configuration
The NAS uses bonded 1 Gb interfaces for increased throughput:

```bash
# Bond configuration in DSM
Bond Mode: 802.3ad (LACP)
Link Aggregation: Yes
MTU: 9000 (Jumbo Frames)
```

### Client-Side Optimization
```bash
# Increase NFS read/write buffer sizes
echo 'net.core.rmem_default = 262144' >> /etc/sysctl.conf
echo 'net.core.rmem_max = 16777216' >> /etc/sysctl.conf
echo 'net.core.wmem_default = 262144' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 16777216' >> /etc/sysctl.conf
```

### Performance Baseline
| Operation | Throughput | Latency | Notes |
|-----------|------------|---------|-------|
| Sequential Read | 220 MB/s | 2ms | Single client |
| Sequential Write | 180 MB/s | 5ms | Single client |
| Random Read | 45 MB/s | 8ms | 4K blocks |
| Random Write | 35 MB/s | 12ms | 4K blocks |
| Concurrent Access | 150 MB/s | 10ms | 4 clients |

## Backup Strategy

### Automated Backup Jobs
The NAS runs several automated backup jobs:

| Source | Destination | Schedule | Retention |
|--------|-------------|----------|-----------|
| All volumes | Internal snapshots | Hourly | 48 hours |
| All volumes | Internal snapshots | Daily | 30 days |
| All volumes | Internal snapshots | Weekly | 12 weeks |
| Critical data | External USB | Weekly | 12 weeks |
| Archive data | Cloud storage | Monthly | 12 months |

### Snapshot Configuration
```bash
# Btrfs snapshot schedule (via DSM)
Hourly: Keep 48 snapshots
Daily: Keep 30 snapshots
Weekly: Keep 12 snapshots
Monthly: Keep 12 snapshots
```

### External Backup
```bash
# USB external drive backup (via DSM Hyper Backup)
Target: USB external drive (8 TB)
Schedule: Weekly on Sunday 02:00
Encryption: AES-256
Compression: LZ4
Versioning: Keep 12 versions
```

## Security Configuration

### Access Control
- **User Authentication**: Integrated with local user database
- **NFS Security**: root_squash enabled on all exports
- **Network ACLs**: Access restricted to servers VLAN (192.168.30.0/24)
- **File Permissions**: Standard Unix permissions enforced

### Encryption
- **At Rest**: Shared folders can be encrypted with AES-256
- **In Transit**: NFSv4 with Kerberos authentication (future)
- **Backup**: External backups encrypted with AES-256

## Monitoring and Alerting

### SNMP Monitoring
The NAS exposes SNMP metrics for Prometheus collection:

```yaml
# Prometheus SNMP configuration
- job_name: 'synology-snmp'
  static_configs:
    - targets: ['192.168.30.30:161']
  metrics_path: /snmp
  params:
    module: [synology]
  relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: prometheus:9116
```

### Key Metrics
| Metric | Normal | Warning | Critical |
|--------|--------|---------|----------|
| Volume Usage | <80% | 85% | 90% |
| CPU Usage | <50% | 70% | 85% |
| Memory Usage | <70% | 80% | 90% |
| Temperature | <45°C | 50°C | 55°C |
| Network Throughput | Baseline | +200% | +500% |

### Health Notifications
```bash
# DSM notification settings
SMART: Email on drive errors
RAID: Email on degraded/failed arrays
Volume: Email at 80% and 90% capacity
UPS: Email on power events
Temperature: Email when >50°C
```

## Disaster Recovery

### Recovery Procedures

#### Single Drive Failure
1. DSM automatically marks drive as failed
2. Order replacement drive (WD Red Plus 8TB)
3. Hot-swap failed drive (no downtime)
4. DSM automatically rebuilds RAID array
5. Monitor rebuild progress (typically 8-12 hours)

#### Multiple Drive Failure
1. Power down NAS immediately
2. Send drives to data recovery service if needed
3. Restore from external backup to replacement drives
4. Rebuild NAS configuration from backup
5. Restore data from external backups

#### Complete NAS Failure
1. Order replacement DS920+ unit
2. Install recovered or replacement drives
3. Import existing volume configuration
4. Restore system configuration from backup
5. Verify all NFS exports and services

### Recovery Time Objectives
- **Single Drive Failure**: 0 downtime, 8-12 hour rebuild
- **NAS Replacement**: 24-48 hours (depends on drive recovery)
- **Data Restore**: 48-72 hours for full restore from backup

## Expansion Planning

### Current Utilization
- **Volume 1**: 14.2 TB used of 22 TB (65%)
- **Growth Rate**: ~800 GB/month
- **Projected Full**: ~10 months at current rate

### Expansion Options
1. **DX517 Expansion**: Add 5×8 TB drives (+30 TB usable)
2. **Drive Upgrade**: Replace 8 TB drives with 16 TB drives (+22 TB usable)
3. **Second NAS**: DS1621+ for performance-critical workloads

### Migration Strategy
When approaching 80% capacity:
1. Order expansion unit or larger drives
2. Schedule maintenance window
3. Migrate cold data to expansion storage
4. Verify backup integrity before changes
5. Monitor performance impact post-expansion

---

**Next**: [Version Matrix](../04_software-stack/versions.md) | **Related**: [Local Storage](storage-local.md)
